[{"title":"CauseRec 反事实生成的序列推荐","date":"2022-08-04T02:08:37.612Z","url":"/2022/08/04/CauseRec-%E5%8F%8D%E4%BA%8B%E5%AE%9E%E7%94%9F%E6%88%90%E7%9A%84%E5%BA%8F%E5%88%97%E6%8E%A8%E8%8D%90/","categories":[["undefined",""]],"content":"说些废话2022&#x2F;8&#x2F;4 在8&#x2F;2日 在写完core这篇文章的时候 发现 文章的一个思路非常像这篇文中的反事实生成这个部分 还有 我其实写完了sine的文章 但是 学校服务器不让用 就离谱 然后发现 这个东西 我以前度过 说实话 没有整理出来 我东西都忘光了 说实话 现在回头看 自己还是看的不怎么清晰 前言论文名：CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation 链接： 论文链接  代码链接  文章介绍链接：（知乎） 相关知识： 1. 相关背景由于记录的用户交互的噪声和稀疏性，仅对观测行为序列进行建模可能最终导致系统脆弱和不稳定。 1.1 核心思想CauseRec通过替换原始概念序列中的可有可无和不可或缺的概念，从反事实数据分布中有条件地采样用户概念序列。利用从合成用户序列获得的用户表示，CauseRec通过对比反事实和观察数据来执行对比用户表示学习。 对比学习设计了模型无关和非侵入性框架，帮助任何基线模型以端到端的方式学习更有效的用户表示。通过将原始用户表示与反事实正样本和反事实负样本进行对比，这种表示更加准确和稳健 反事实建议 专注于去噪用户表示学习，并考虑了回顾问题，即“如果我们干预观察到的行为序列，用户表示会是什么？”。从技术上讲，我们提出了几个基于识别不可或缺&#x2F;可有可无概念的反事实转换，并设计了几个对比目标，用于学习准确和稳健的用户表示 2. 实证分析图 这个实证 是根据作者经验得到 在推荐系统中，用户通常仅与有限数量的项目进行交互，而在大型实时系统中，项目库很容易达到1亿个。因此，仅对既稀疏又有噪声的观测行为序列进行建模可能最终导致不太令人满意的脆弱系统。 3. 问题描述序列推荐 图！ 从顺序推荐的观点来看，数据集可以表示为D&#x3D;{（xu，t，yu，t）}u&#x3D;1,2，…，N，t&#x3D;1,3，…，Tu，其中xu，t&#x3D;{yu，1:（t−1） }表示用户在第t个行为yu，t之前的历史行为，并按时间顺序排列，Tu表示用户u的行为数。顺序推荐的目标是预测下一个项目yu，t，给定历史行为xu，t，其可以表示为建模所有可能项目的概率 同样的 因为要偶然删除子序列 所以 输入被定成了（关键是 有时候还得提取子序列） (xu,t,yu,t ) 4. 方法（基本为论文方法部分）注意，文章提出的框架CauseRec的所有变体都是在基础模型上展开的。 4.1 整体架构模型的本质是 回答追溯问题 具体来说，我们首先确定历史行为序列中不可或缺的概念。一个不可或缺的概念表示一个行为序列的子集，可以共同表示用户兴趣的一个有意义的方面。可有可无的概念表示在表示感兴趣的方面不太重要的噪声子集 4.2 归纳偏置（类似于先验）图 通常情况下，我们不知道具体上帝函数的情况，但我们猜测它类似于一个比较具体的函数。这种基于先验知识对目标模型的判断就是归纳偏置（inductive bias）。归纳偏置所做的事情，是将无限可能的目标函数约束在一个有限的假设类别之中，这样，模型的学习才成为可能。 4.3 概念不可或缺&#x2F;可有可无的划分 Item-level Concepts 图 Interest-level Concepts 图 反事实变换 旨在通过替换原始用户序列的一部分概念来构建分布外out-of-distribution的用户序列。这里的用户序列可以是众所周知的item序列，也可以是兴趣level的概念序列。基于全局总览中描述的归纳偏差，建议以 rrep 的速率替换已识别的必不可少&#x2F;可有可无的概念，以分别构建反事实的negative&#x2F;positive用户序列。直接删除不可或缺&#x2F;可有可无的概念似乎也是可行的，但替换具有不影响整体序列长度和概念的相对位置的优点。具体来说，文章维护一个先进先出队列作为每个level的概念memory，将从当前mini-batch中提取的全量概念加入队列，并使用dequeue的概念作为替代。（这部分没说清楚可能要看代码） 用户embedding生成 图 目标函数 除了最大化似然概率的目标函数外，文章引入几个对比学习的目标函数来学习更精确的更鲁棒的用户表示。 反事实和真实序列之间的对比 一个健壮的用户表示应该对序列中可有可无的概念不那么敏感。 因此，从反事实序列中学习到的具有必不可少的概念转换的用户表示应该远离原始用户表示。 类似，准确的表示应该更多地信任不可或缺的概念。 因此，从具有可有可无的概念转换的反事实序列中学习的用户表示应该直观地更接近原始用户表示。因此，文章使用三元组边际损失来衡量样本之间的相对相似性： 图！ 不过这里想强调的是，在 测试&#x2F;服务阶段，生成 user embedding 只用到了原始行为序列，不需要计算 proposal scores 以及 反事实用户序列生成。强调这个原因在于，文章计算 proposal scores 需要用到 target item，而 target item 在测试和服务阶段是看不到的。具体来说，CauseRec-H 和 CauseRec-In 在测试阶段完全一样，而CauseRec-Item 和 Base Model 的差别也是在训练阶段。 5. 实验6. 总结7. 自己的想法"},{"title":"core 在一致表示空间内基于会话的简单有效的推荐","date":"2022-08-02T03:11:21.347Z","url":"/2022/08/02/core-%E5%9C%A8%E4%B8%80%E8%87%B4%E8%A1%A8%E7%A4%BA%E7%A9%BA%E9%97%B4%E5%86%85%E5%9F%BA%E4%BA%8E%E4%BC%9A%E8%AF%9D%E7%9A%84%E7%AE%80%E5%8D%95%E6%9C%89%E6%95%88%E7%9A%84%E6%8E%A8%E8%8D%90/","categories":[["undefined",""]],"content":"说些废话看到题目 目的解决我的两个疑问 什么是一直表示空间 简单有效 前言论文名：CORE: Simple and Effective Session-based Recommendationwithin Consistent Representation Space 链接： 论文链接  代码链接  文章介绍链接：（知乎）无 相关知识： 会话推荐 推荐系统 1. 相关背景非线性编码器学习的会话嵌入通常与项目嵌入不在同一表示空间中，这导致在推荐项目时出现不一致的预测问题 1.1 核心思想设计了一种表示一致性编码器，将输入项嵌入的线性组合作为会话嵌入，保证会话和项位于相同的表示空间 使用了一个稳健的距离测量 来使 相近的在一块 其他的偏离 1.2 挑战 考虑到会话和项目嵌入共享一致的表示空间，如何设计更合适的编码器，以便我们能够利用深度非线性神经网络的巨大能力 一旦表示空间统一，项目嵌入直接涉及分数计算和模型优化，如何测量嵌入之间的距离以避免项目嵌入的过度拟合 2. 实证分析 看 实证分析的右侧 匿名用户 点击a 最后 甚至会跑到item b 那里去（离大谱） 事实证明 一致空间 从某种程度来说 也增加了可解释性 3. 问题描述会话推荐 4. 方法（基本为论文方法部分） 输入 有三 会话 下一个item 其他的item 通过session对话 计算出 每一个session的权重 然后 加和 其训练过程 就是 让 我们 训练出来的一致表示下的item 与 next item 靠经 与 其他地 other item 拉远（这个好好看一下）我看过一篇类似的文章 4.1 DNN mean pooling平均池化 transformer通过transformer 来 进行权重的捕获4.2 用于解码的鲁棒距离测量 由于会话被编码为项目嵌入的线性组合，并通过测量嵌入空间中到项目的距离来解码，因此项目嵌入直接涉及嵌入之间的距离计算，导致过度拟合的高风险。因此，我们寻求一种稳健的方法来测量统一表示空间中的距离，以防止过度拟合。 说实话 有点看不太懂证明 论文名字：Improved Deep Metric Learning with Multi-class N-pair Loss Objective 看论文 貌似是一个优化方法 本文使用了三种方法来 避免过拟合 使用了一个可控的超参数 来替换 固定裕度2 dropout 很平常 受对比学习启发 使用了 cos 计算测量距离 24 where h′ denotes the item embeddings with dropout. 5. 实验作者做了两个消融实验来证明自己的效果好 将REC编码器（Representation-Consistent Encoding）替换为sasrec编码器 将RDM（Robust Distance Measuring for Decoding）替换成 传统点积 又做了一个添加模块的实验 真不错啊 ]() 作者 用图来表示空间一致性 超参数设计 6. 总结在本文中，我们提出了一种简单有效的一致表示空间中基于会话的推荐框架CORE，该框架在整个编码和解码过程中统一了表示空间，以克服不一致预测问题。与在项目嵌入上堆叠多个非线性层不同，我们建议只对项目嵌入应用加权和，将一致表示空间中的会话编码为项目。此外，我们从多个方面提出了稳健的距离测量技术，以防止所提出框架中项目嵌入的过度拟合。在五个公共数据集上的大量实验表明了所提出方法的有效性和效率，以及所提出的技术如何帮助现有方法。在未来的工作中，我们将考虑从理论和经验两方面研究所提出的表示一致性编码器的表达能力。此外，我们将探讨如何在拟议的框架中引入边特征和有用的归纳偏差 7. 自己的想法本文的方法简单且有效 对其十分感兴趣的是 一个先有鸡还是先有蛋的问题 即 先想到这个方法 然后跑个实验 给他取了一个漂亮的名字 然后讲了个故事。 还是说 一开始就想到了表示空间一致这个问题 然后 去跑了这个实验 发现有效。 这个真的是个问题。 sine 中 提取 拉取相近兴趣 拉远 不相关 兴趣 是不是可以思考一下 我之前 看到过一篇序列推荐 是 把序列不重要的item替换 然后 做对比学习(其中拉近拉远的操作)就跟这个差不多 我等会去看看 "},{"title":"图推荐系统综述","date":"2022-08-01T09:12:20.467Z","url":"/2022/08/01/%E5%9B%BE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"说些废话图推荐是目前的项目需要 最近一个小伙给我又发了一篇新的 文章 需要好好读一下 2022&#x2F;8&#x2F;1 因为下午不能游泳 我心情很不开心 前言论文名：Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions 链接： 论文链接  代码链接  文章介绍链接：（知乎） 目前没有 相关知识： 1. 相关背景 现有的方法 谱模型 空间模型 动机 高阶连通性 数据结构特征 增强的监督信号 挑战（） 图构造（ 适当地构造成图，节点表示元素，边表示关系 ） 嵌入传播&#x2F;聚合 模型优化（ 包括优化目标、损失函数、数据采样等 ） 计算效率及有效部署（ 由于GNN的嵌入传播操作引入了大量计算，图神经网络在推荐系统中的有效部署是另一个关键挑战 ） 2. 标准系统及我为什么需要图数据图！ 2.1 结构数据数据形式 从在线平台收集的数据有多种形式，包括用户项目交互（评级、点击、购买等）、用户档案（性别、年龄、收入等）、项目属性（品牌、类别、价格等）等 传统的推荐系统无法利用这些多形式的数据，通常只关注一个或几个特定的数据源，由于忽略了许多信息，这导致了次优性能。通过将所有数据表示为图上的节点和边，GNN提供了一种利用可用数据的统一方法。同时，GNN在学习表示方面表现出强大的能力，因此可以获得对用户、项目和其他特征的高质量嵌入，这对推荐性能至关重要 2.2 高阶连通性在传统方法中，由于训练数据主要是仅包含直接连接项的交互记录，因此只能隐式捕获协同过滤效果。换句话说，只考虑一阶连通性。缺少高阶连通性可能会在很大程度上损害推荐性能。相反，基于GNN的模型可以有效地捕捉高阶连通性。 2.3 监控信号（解决数据的稀疏问题）监督信号在收集的数据中通常是稀疏的，而基于GNN的模型可以在表示学习过程中利用半监督信号来缓解这一问题。以电子商务平台为例；与其他行为相比，目标行为purchase相当稀少。因此，仅使用目标行为的推荐系统可能会获得较差的性能。通过在图上编码半监督信号，基于GNN的模型可以有效地结合多种非目标行为，例如搜索和添加到购物车，这可以显著提高推荐性能 3. 挑战3.1 图构建数据输入为观察到的用户项交互数据，输出为缺失用户项交互的预测。因此，可以构造一个以用户&#x2F;项目为节点、交互为边的二部图。此外，CF任务转向图上的用户项链接预测。 节点 图神经网络学习的主要目标之一是为节点分配表示。这导致节点的定义在很大程度上决定了GNN模型的规模，其中大多数参数由 layer-0嵌入占据。注意，边缘嵌入通常不考虑或基于节点嵌入计算。另一方面，确定是否区分不同类型的节点也是一个具有挑战性的问题。例如，在协同过滤任务中，可以对用户节点和项目节点进行不同的建模，也可以将其视为同一类节点。另一个挑战点是处理具体的输入，例如一些数字特征，如项目价格，这些特征总是连续的数字。为了在图中表示这些特征，一种可能的解决方案是将其离散化为分类特征，然后可以将其表示为节点 边边的定义在进一步传播和聚合以及模型优化中高度影响图的质量。在一些琐碎的任务中，推荐系统的数据输入可以被视为一种关系数据，例如用户-项目交互或用户-用户-社会关系。在一些复杂任务中，其他关系也可以表示为边。例如，在bundle推荐中，bundle由几个项组成。连接束和项目的边缘可以反映隶属关系。好的边设计在构造图时应该充分考虑图的密度。过于密集的图意味着存在度数极高的节点。这将使嵌入传播由大量邻居进行。这将进一步使传播的嵌入不可区分和无用。为了处理过于密集的边，对图进行采样、过滤或剪枝是很有希望的解决方案。当然，过于稀疏的图也会导致嵌入传播的效用较差，因为传播将仅在一小部分节点上进行。 3.2 网络设计（传播和聚合的设计）使GNN不同于传统的图学习方法的是传播层。对于传播，如何选择路径是建立推荐系统高阶相似性模型的关键。此外，传播也可以是参数化的，为不同的节点分配不同的权重。 在传播中，也有各种聚合函数的选择，包括均值池、LSTM、max、min等。由于在所有推荐任务或不同数据集中没有一个选项可以表现最好，因此设计一个特定且适当的选项至关重要。此外，传播&#x2F;聚集的不同选择严重影响计算效率。例如，均值池在基于GNN的推荐模型中被广泛使用，因为它可以高效地计算，特别是对于包含高度节点的图，例如非常流行的项目（可以连接大量用户）。此外，可以堆叠传播&#x2F;聚合层，以帮助节点访问更高跳数的邻居。太浅的层使高阶图结构无法很好地建模，太深的层使节点嵌入过度平滑。这两种情况中的任何一种都会导致推荐性能较差。 总结一下 网路设计分为 传播和聚合 可以堆叠传播&#x2F;聚合层 来帮助节点访问更高的邻居层 3.3 模型优化为了优化基于图神经网络的推荐模型，推荐系统中的传统损失函数总是转向图学习损失。例如，优化中的对数损耗可以视为逐点链路预测损耗。类似地，BPR损耗[126]通常用于图上的链路预测任务。另一个方面是数据采样。在基于GNN的推荐中，要对正项目或负项目进行采样，采样方式在很大程度上取决于图结构。例如，在社交推荐中，在图上执行随机游走可以生成弱正项目（例如朋友互动的项目）。此外，有时，基于GNN的推荐可能涉及多个任务，例如不同类型边缘上的链路预测任务。那么在这种情况下，如何平衡每项任务并使它们相互促进是一个挑战。 3.4 计算效率为了保证基于GNN的推荐模型的应用价值，应认真考虑其计算效率。与传统的非GNN推荐方法（如NCF或FM）相比，GNN模型的计算成本要高得多。特别是对于谱GNN模型，如GCN，每个GCN层都涉及复杂的矩阵运算。随着GCN层的多层堆叠，计算成本进一步增加。因此，PinSage等空间GNN模型更容易在大规模工业应用中实现。通过在邻域之间采样或剪枝图结构，只要我们能够承受推荐性能的下降，就可以始终保持效率。 4. 论文总结分阶段的顶会 阶段 模型名 论文名 年 会议 Matching GCMC Graph convolutional matrix completion 18 Matching PinSage Graph convolutional neural networks for web-scale recommender systems 18 Matching NGCF Neural graph collaborative filtering 19 Matching LightGCN Lightgcn: Simplifying and powering graph convolution network for recommendation 20 Ranking Fi-GNN Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction. 19 Ranking PUP Incorporating Price into Recommendation with Graph Convolutional Networks 20 Ranking L0-SIGN Detecting Beneficial Feature Interactions for Recommender Systems 21 Ranking DG-ENN Beyond clicks: Modeling multi-relational item graph for session-based target behavior prediction 21 Re-ranking IRGPR Personalized Re-ranking with Item Relationships for E-commerce 20 5. 总结6. 自己的想法"},{"title":"saint+ saint的时间正确使用版本","date":"2022-08-01T02:11:58.897Z","url":"/2022/08/01/saint+-saint%E7%9A%84%E6%97%B6%E9%97%B4%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8%E7%89%88%E6%9C%AC/","categories":[["undefined",""]],"content":"说些废话前面不是说过 saint的 变种 因为时间属性没有使用正确 导致 其效果甚至都不如原来的本体 这个估计就是时间正确的适用版本 这篇文章 transformer 下 时间属性怎么使用 人家的改进方法 前言论文名： 链接： 论文链接  代码链接  文章介绍链接：（知乎） 已经运行的环境：   相关知识： transformer 知识追踪 saint 1. 相关背景saint没有很好的将时间参数引入 embedding 中 1.1 核心思想其中的时间分为两种 elapsed time 做题的时候所花费的时间 提出了两种编码方式 连续嵌入 （公式） et 是一个时间（int） w 是一个可以学习的参数 范畴嵌入为每个整数秒分配唯一的潜在向量。我们将最大经过时间设置为300秒，超过该时间的任何时间都限制为300秒。 lag time 滞后时间是交互之间的时间间隔，是影响学生学习过程中出现的复杂现象的重要因素。例如，随着时间的推移，学生往往会忘记所学的内容 连续嵌入 （公式） it 是一个时间（int） w 是一个可以学习的参数 范畴嵌入其细粒度为分钟 0, 1, 2, 3, 4, 5, 10, 20, 30, . . . , 1440.从结果上面来看 一共分为了150类 （看看人家的embedding） 2. 实证分析时间分布 一样存在长尾问题（划分的理由）细粒度划分的理由 3. 问题描述各一个序列 题目 加 响应 然后回答序列的最后一个答案 4. 方法（基本为论文方法部分） 论文 的 整体方法 大致 和 saint 差不多 具体可以看看实验是怎么run出来的 时间种类的划分 取得最好的效果 （这里的时间 是放在 解码器之中） 对于时间的消融实验 对于添加时间特征在哪里好的实验 5. 总结此外，通过将时间特征合并到解码器输入中获得了最佳结果，验证了分别处理练习信息和学生反应信息适合于知识跟踪的假设。未来工作的途径包括1）不仅对学生的问题解决记录进行建模，而且对各种学习活动进行建模，例如观看讲座和学习每个练习的解释；2）探索知识跟踪模型的体系结构，而不是分别处理练习信息和学生反应信息的基于转换器的编码器-解码器模型。 6. 自己的想法 对方的消融实验做得十分不错 对于时间的划分 可以通过 观察时间的分布 来确定离散量的分布 对于 lag time 的 使用 是不是 显得有点草率 看看 能不能像lpkt一样用出来 "},{"title":"VKT 视觉知识追踪","date":"2022-07-29T12:14:13.301Z","url":"/2022/07/29/VKT-%E8%A7%86%E8%A7%89%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/","categories":[["undefined",""]],"content":"说些废话这个文章 是 我在看 知识追踪 的 paperwithcode 时候 偶然 发现一篇文章 前言论文名：Visual Knowledge Tracing 链接： 论文链接  代码链接  文章介绍链接：（知乎） 数据集下载： 相关知识： 视觉分类 人类类别表示 度量学习 联合估计 1. 相关背景​ 人类学习者成功的关键是我们能够从我们周围的世界中提取信息丰富和可概括的表示，以及我们在相对稀疏的反馈下更新这些表示的能力。 ​ 其目的是解决三个疑问。 ​ （i）使人类学习的表征如此有效的属性是什么， ​ （ii）这些表征是如何学习的 ​ （iii）我们可以预测人类在学习过程中的分类行为吗？ ​ 最终的目的是了解人类为什么学习如此有效 **我们的目标是估计人类学习者使用的图像分类函数，该函数已提供一系列图像和相应的地面真值类标签作为训练数据 ** 1.1 相关工作 度量学习： 该工作试图从稀疏的人类注释中学习人类对齐的视觉表示 提出了 更具挑战性的视觉知识跟踪设置，其中假设学习者在学习过程中是非平稳的，即他们用于执行手头的分类任务的视觉特征可能会随着时间而变化 提出了一种基于递归神经网络的视觉知识跟踪方法。经过训练后，我们的模型能够预测训练期间未观察到的人类学习者的分类行为。所提出的模型利用以前学习者反应的历史、图像和基本真理类标签来预测他们未来的反应。 度量学习 我们可以在特定的任务通过选择合适的特征并手动构建距离函数。然而这种方法会需要很大的人工投入，也可能对数据的改变非常不鲁棒。度量学习作为一个理想的替代，可以根据不同的任务来自主学习出针对某个特定任务的度量距离函数。 如果注释者正在学习感兴趣的视觉概念，则违反了这一假设。在这项工作中，我们解决了这种非平稳设置，并表明通过这样做，我们可以更准确地预测真实人类学习者的视觉分类行为。 人类类别表征人类会根据任务的不同 使用多种不同的类别学习系统 目前的共识是，根据手头任务的具体性质，人类可能会使用多种不同的类别学习系统[5,3]。例如，在基于规则的任务中，最优策略可能很容易表达，因此可以通过一组规则进行有效编码。然而，在实践中，感知任务（如细粒度视觉分类）可能更难用这种方式表示[8]。 在更具挑战性的环境中，在学习分类任务的过程中，我们的学习者不是静态的。 知识追踪洒洒水 机器教学机器教育人类（机器飞升） 机器教学算法通过生成向新手学习者展示的教学示例序列来解决教学问题，以提高他们完成给定任务的能力。 1.2 核心思想1.3 贡献（i）一个新的视觉知识跟踪模型，该模型联合估计了非平稳人类学习者使用的视觉特征和每时间步分类函数。（ii）从参与学习具有挑战性的视觉分类任务的人那里收集的三个基准评估数据集的一组新注释。（iii）详细比较了这些数据集上的几种视觉知识跟踪方法。 2. 实证分析没有 3. 问题描述具体来说，在每个训练时间步，向学习者k呈现图像x，他们提供其响应r，并以正确的类标签y的形式给出反馈。 相反，为了克服这种有限的信息设置，我们跨多个学习者训练模型φ，允许该模型发现所有学习者共享的知识状态和学习规则。 4. 方法（基本为论文方法部分）跟踪模型φ 特征提取 f CNN 分类函数ψ softmax 特点 特征提取器在实验的时间间隔内保持不变（针对所有学习者使用相同的底层提取器，且随时间间隔内保持不变） 并且他们只是在对不同视觉特征的相对重要性上有所不同 但是 学习者使用的分类函数 不保持静态 分类器 简单静态分类器 我们探索的第一个模型是最简单的。在这里，我们假设所有学习者使用相同的分类器，该分类器不会随时间变化。在此设置中，ψ是一个多类线性分类器，具有权重矩阵w和每类偏差b该模型类似于传统的度量学习方法，它不试图捕捉与个体偏差或时间变化相关的任何注释者特定差异。在训练时，我们只需为所有学习者估计一组参数。该模型不考虑响应历史 时间敏感追踪模型 静态跟踪模型的一个明显局限性是，它没有考虑到学习者可能会随着时间的推移而变化的事实，即他们可能在早期的新分类任务中表现得更差，但随着时间的流逝，随着示例图像序列及其相关的基本真理类标签的显示，他们可能会有所改善。一种更先进的模型捕捉到了这种时间演变，即每个时间步长都有不同的分类器， 在每一个时间步，使用不同的偏差和权重 w b i.e. wt 不等于 wt−1. ### 跟踪模型 以前的问题即单个学习者可能从不同的能力水平开始，并根据他们所获得的信息以不同的方式更新其内部知识状态。[34]表明，递归网络可用于跟踪参与学习数学测验问题的人类学习者的技能习得。 作者使用了 DKT 作为基础 设计了一个新的基于视觉的DKT模型 直接响应模型特征提取 CNN 分类函数 lstm该模型假设学习者在时间t的知识状态由他们之前看到的图像和他们过去的分类响应定义。递归模型可以通过调节学习者的隐藏状态为其生成独特的转换。在这种情况下，在共享特征提取器将图像转换为特征向量后，模型根据学习者的隐藏状态通过一系列非线性变换修改特征向量。最后一个线性层将特征向量转换为预测响应。注意，该模型还以当前查询图像z&#x3D;f（x）和相应的真值类标签y为条件 分类器预测模型与之前的直接预测递归模型不同，我们现在明确表示单个学习者使用的独立的分类函数。 5. 总结在这项工作中，我们探索了视觉知识跟踪的问题，即预测人类学习者使用的内部、可能随时间变化的图像分类功能的任务。为此，我们提出了一系列复杂度从基本静态线性分类器到递归模型的模型，这些模型在预测学习者未来行为时考虑了学习者先前的反应历史。我们从参与视觉学习任务的人那里收集了三个具有挑战性的视觉分类任务的新注释，以便对这些不同模型的性能进行基准测试。 6. 自己的想法 找出开放性问题 知识追踪来说 有个学习的权重分配。相近的权重相对较高，离的较远的权重分配的较低 这个是一个特别新的工作 论文出来估计才两个星期 需要好好阅读 "},{"title":"论文分析的板子","date":"2022-07-29T08:07:34.453Z","url":"/2022/07/29/%E8%AE%BA%E6%96%87%E5%88%86%E6%9E%90%E7%9A%84%E6%9D%BF%E5%AD%90/","categories":[["undefined",""]],"content":"说些废话目前 论文分析要求 很重 而且 我心很杂 需要一套班子 前言论文名： 链接： 论文链接 代码链接 文章介绍链接：（知乎） 相关知识： 1. 相关背景1.1 核心思想2. 实证分析3. 问题描述4. 方法（基本为论文方法部分）5. 实验6. 总结7. 自己的想法"},{"title":"saint论文 扫盲","date":"2022-07-29T07:42:33.970Z","url":"/2022/07/29/saint%E8%AE%BA%E6%96%87-%E6%89%AB%E7%9B%B2/","categories":[["undefined",""]],"content":"前言2022&#x2F;7&#x2F;29 作为 知识追踪的sota saint+ 前身 主要的作用就是体现出 transformer 变种 在 知识追踪的体现 链接： 论文地址： github：  （不太可信）  参考链接： 相关知识： Pre-LN  transformer 1. 相关背景目前的transformer 模型有两个限制 transformer 深度不够 qvk的交互有问题 1.1 核心思想 将 运动序列 和 响应序列 分别应用于编码器和解码器 2. 实证分析（有的有 有的没有）这篇没有分析 3. 问题描述 4. 方法（基本为论文方法部分）参数使用 练习ID：将潜在向量分配给每个练习唯一的ID 练习类别：每个练习属于领域主题的一个类别。为每个类别分配一个潜在向量 位置：输入序列中练习或响应的位置（第一、第二、…）表示为位置嵌入向量。位置嵌入在运动序列和反应序列中共享 响应：将潜在向量分配给学生响应ri的每个可能值（0或1） 已用时间：学生以秒为单位的响应时间被舍入为整数值。将潜在向量分配给0到300（包括0和300）之间的每个整数。任何超过300秒的时间都被限制为300秒 时间戳：记录学生收到每个练习的绝对时间的月、日和小时。为月、日和小时的每个可能组合分配唯一的潜在向量 论文 使用了三种embedding 作为数据的输入 多头这里使用的是上掩码的权重操作，目的是避免看到未来的信息 Pre-LN 把Transformer架构中传统的Add&amp;Norm做layer normalization的方式叫做Post-LN，并针对Post-LN，模型提出了Pre-LN，即把layer normalization加在残差连接之前 编码器M &#x3D; SkipConct(Multihead(LayerNorm(Qin,Kin,Vin)))O &#x3D; SkipConct(FFN(LayerNorm(M))) 这里的o 是 encoder 最后的一个输出 在每一层 ln和残差连接都有使用 解码器M1 &#x3D; SkipConct(Multihead(LayerNorm(Qin,Kin,Vin)))M2 &#x3D; SkipConct(Multihead(LayerNorm(M1,O,O)))L &#x3D; SkipConct(FFN(LayerNorm(M2))) O是编码器的最终输出。译码器中第一层的Qin、Kin和Vin都是Re 我的变体 ltmti区别是 输入解码器序列 变成了 I 不是 e 使用了下掩码 （ 目的是为了让模型关注 most recent (i-1) 来预测。会考虑不同长度的历史信息进行预测，可以当作一种增强 ） UTMTI UTMTI模型遵循与SAINT相同的架构，仅在输入序列的选择上有所不同。 SSAKT 作者报告，当注意力块被多次叠加时，AUC降低。SSAKT通过在将练习作为查询提供之前在练习上应用自我注意力来解决这个问题。运动自注意力块和运动交互注意力块的输出进入相应的以下块，作为其注意力层的输入 5. 总结SOTA 6. 自己的想法 在信息的堆叠 不一定能将效果变得更好（UTMTI和saint） 作者堆变体的样子 像是 能发好文章的姿势 可以 像lpkt那种 试着将时间信息加入其中 "},{"title":"EdNet 数据集","date":"2022-07-29T02:24:52.342Z","url":"/2022/07/29/EdNet-%E6%95%B0%E6%8D%AE%E9%9B%86/","categories":[["undefined",""]],"content":"前言作为知识追踪的先锋，这个数据集 就像 刚进监狱的吴亦凡哥哥 一样 大规模分层 又大又圆 一个配备人工智能教学系统的多平台自学解决方案。EdNet包含2年多来收集的784309名学生的131417236次互动，是迄今为止发布的最大的公共IES数据集 看看 与其他数据集比较 EdNet具有层次结构，将学生行为分为4个不同的抽象层次 第一章 介绍student： 784309 交互： 131417236 图！ 学生使用santa的可能场景。在学生购买了50天的通行证后，他们解决了一个LC问题。当他们解决问题时，他们的所有动作，包括音频播放和选择消除都被记录下来 特性 大规模 EdNet由2017年以来从78 4309名Santa学生收集的总计131441538个互动组成。每个学生在使用Santa时平均产生441.20个互动。基于这些交互作用，EdNet使研究人员能够访问大规模真实世界的IES数据。此外，Santa提供了总计13169个问题和1021个讲座，标记了293种技能，每个问题和讲座分别消耗了95294926次和601805次。据我们所知，就学生总数、互动和互动类型而言，这是可供公众使用的最大的教育数据集 多样性 行为比较多， 数据的丰富性使研究人员能够从不同角度分析学生。例如，购买日志可能有助于分析学生对学习过程的参与程度。 层次结构 为了以一致和有组织的方式提供各种类型的数据，EdNet在四个不同的数据集中提供数据，分别命名为KT1、KT2、KT3和KT4。 多平台 安卓 ios 网络 不同的数据集捆绑是共享一篇文章、图片或听力材料的问题集合。例如，ID为q2319、q2320和q2321的问题可能共享相同的阅读文章。 KT1一个 问题 回答的对 (q1, r1), (q2, r2), · · · , (qt, rt) 可以用于 知识追踪 KT2 解决的问题 问题-回答序列格式的一个主要限制是，它是学生活动的一个非常简明的摘要。例如，在决定一个答案并提交最终答案之前，学生可以在两个答案中的一个选项之间进行选择。这可能表明他们已经将答案缩小到两个选项中的一个，但不确定这两个选项中哪一个是正确的。现代IESs能够记录此类详细信息，但问题响应格式无法有效表示此类情况，限制了使用EdNet-KT1进行的分析 解释 item-id b开头 为捆绑 q开头 为 问题 其他的 自己看 能看明白 EdNet-KT2是EdNet中最简单的基于动作的数据集，由与问题解决活动相关的动作组成 KT3在Santa，学生可以参加除解决问题外的各种学习活动。这包括阅读专家评论或观看系统提供的讲座。EdNet-KT3整合了有关这些学习活动的信息。这些信息可以用来推断学习活动对每个学生的知识状态的影响。例如，可以分析每个学生学习某些专家评论的时间，并观察其对不同学习行为和表现的影响 可以算 这个记录了一个 宏观上的学习过程 e 开头 阅读 解释 I 开头 观看 课程 KT4在微观上 能够 体现出 先选了什么 在修改的一个过程 很强 EdNet-KT4中的示例学生数据。学生购买物品后，他们解决了LC问题q878。记录了他们播放和暂停音频的时间戳。他们还去掉了“a”，选择了“c”作为答案。 我能干什么知识追踪KT1 EdNet-KT1的大规模数据允许该模型通过深度注意力层捕捉学生互动之间的复杂关系。 移动学习环境中的学习会话退出预测 这个是 示例论文 KT4 标签缺少教育问题-预训练任务 使用EdNet-KT4作为训练数据集，评估建模在考试分数和复习正确性预测方面显示了最先进的结果，优于自然语言处理社区开发的学习学习学习项目内容表示的预训练方法 强化学习强化学习（RL）是一种突出的方法[11,6,22,18,17,12]。在RL的背景下，训练策略（例如辅导策略）以最大化奖励函数，该函数评估代理（导师）随着时间的推移的整体教育效果。 KT1 通过历史 来 结果问题的响应 KT4 可以执行更详细的操作，例如讲师匹配，可以用同样的方法模拟产品购买或答案选择消除。每种选择都权衡了简单性和保真度 结论本文介绍了EdNet，一个由多平台服务提供商收集的大规模教育数据集。EdNet包含每个用户活动的高分辨率记录，到目前为止，它比教育领域的任何其他公共数据集都大得多。EdNet的层次结构允许研究人员从不同的抽象层次处理AIEd中的不同任务 "},{"title":"知识追踪综述","date":"2022-07-16T06:25:52.667Z","url":"/2022/07/16/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"0 说点废话（讲一下怎么找到这个文章的）2022年7月16日 在床上百无聊耐的看视频 突然看到这样的东西  嗯嗯 我就去这个网站试了试  查了查 我留下的二课题 知识追踪（cool） 发现了一篇 2022-4-1 号的 中文的一篇综述 就很开心 1. 前言-知识追踪是啥​ KT算法将学生的知识掌握程度随着时间的推移建模预测，从而能够准确地预测学生在未来互动中的表现据此有针对性地为学生订制不同的学习路线，提升学习效率.学生通过在线学习平台进行学习交互，形成答题行为时间序列，KT算法通过对学习者和序列联合建模，预测其对于新知识的认知概率分布，进一步推理出学习者的技能和认知水平。 2. kt问题定义以及数据集2.1 定义 2.2 数据集ASSISTments Data是KT领域最为经典的数据集，其中ASSISTments2009数据集 是绝大多数KT模型的标准数据集；EdNet［8］ 发布于2019年，是KT领域最新的数据集，提供了超过1亿条 学习者交互记录；Synathetic是 DKT模型所附带的数据集，包括了超过 20万条学习记录信息；其他数据集包括：Junyi15，algebra 2006 2007及Statics2011等. dataset Website Field Records&#x2F;k ASSISTments2009  Math 32 ASSISTments2017  Math 94 Ednet  English 13 000 Junyi15  Math 2 500 algebra 2006 2007  Math 180 Synathetic  Math 1 435 Statics2011  Engineering 19 KT过程分析知识关系– 使用图来表示知识关系 论文名 方法 效果 年份 Graph ⁃ based knowledge tracing：modeling student proficiency using graph neural network 构建知识图谱并进行KT 的方法，方法基于图结构 提高了模型预测的可解释性 2019 HGKT：introducing hierarchical exercise graph for knowledge tracing 引入了问题模式的概念，构造了一个分层的练习图，可以对学习依赖关系进行建模，并采用两种注意机制 突出学习者的重要历史状态 2022 Modeling relational data with graph convolutional networks 基于关系图卷积神经网络（CNN）的知识图谱构建方法 2018 Peer ⁃inspired student performance prediction in interactive online question pools with graph neural network 在 R-GCN 的基础上，利用学生互动过程，构建了“学生—互动—问题”网络，提出了 R2GCN 模型 适用于异构情况下的网络学习 2020 Machine learning and knowledge discovery in databases 提出了一种端到端的DKT框架，能够利用“高阶问题—技能”关系 缓解数据稀疏性和多知识点问题 2021 因素关系处理DKT方面，由于问题与概念的不平等性，使得大多数KT方法都把模型中的知识点数量等价于问题数量，从而丢了问题所具有的个性化特性.如果缺乏对于问题的甄别，则无法确定问题的信度，进而损失模型的预测精度. 项目反应理论（IRT）项目反应理论（IRT）该理论假设学习者的学习能力不随时间和实践变化。（该理论认为 只有答会高水平的题目 才能证明学生是高水平的） 举个例子： 考生答对的题目难度是判断考生能力的标准。某考生答对10道难度为1的题目，获得的能力值依然是1，另一考生答对1道难度为8的题目，能力值则为8 其中rasch模型 使用难度描述 输入模型的问题 其中，θ 代表学习者的学习能力；b 代表问题的难度 .Rasch 模型在可解释性、问题区分性等方面性能优越，GHOSH等通过在深度模型中使用Rasch编码，提高了DKT的可解释性，取得了卓越的预测性能. 深度技术下 IRT模型 模型名 论文名 干啥 年份 Deep ⁃IRT Deep ⁃ IRT：make deep learning based knowledge tracing explainable using item response theory 它是 IRT 模型与 DKVMN 模型的结合 . 2022 DKVMN Dynamic key ⁃ value memory networks for knowledge tracing 在知识追踪中使用了内存机制 2017 EKPT Learning or forgetting？A dynamic approach for tracking the knowledge proficiency of student 提出知识熟练度追踪（KPT）模型和练习关联的知识熟练度（EKPT）模型，应用于知识估计、分数预 测和诊断结果可视化三个重要任务. 2020 KTMs Knowledge tracing machines：factorization machines for knowledge tracing 综合IRT，AFM，PFA等模型，提出了知识追踪机（KTMs）框架，KTMs利用所有特征的稀疏权值集，对学习者答题结果的概率进行建模. 2019 学习认知机制和遗忘机制认知机制WANG等［27］提出了一种通用的神经认知诊断框架，摒弃人工特征，将神经网络集成到复杂的非线性交互模型中，解决认知诊断问题，并且结合CNN，提出了Neural CDM+模型，通过自动提取系统中的知识点信息，补充知识点相关度矩阵，避免了主观性甚至错误. 遗忘机制DKT模型使用RNN一定程度上实现了对记忆过程模拟，但是仍然没有真正意义上模拟人类思维习惯. 模型 论文名 方法 年份 LPKT Learning Process-consistent Knowledge Tracing 加了个遗忘层 主要是sigmoid 2021 DKVMN Dynamic key ⁃ value memory networks for knowledge tracing 过类似于计算机内存管理的方式，建立知识记忆遗忘矩阵，在模型可解释性上取得了很大的进步 2017 CKT Context-aware attentive knowledge tracing 基于Transformer的模型框架 上引入了注意力衰减机制，模拟全局遗忘行为，从而取得了较好的模型效果 2020 KT方法传统kt方法 BKT特点 标准BKT模型建模过程中将知识点设置为“永不忘记”，并且假设一个题目只对应一个知识点（按道理来说 一个题目是对应多个知识点的） 在其中有几种概率 2.1 P ( L)是初始知识状态下学生掌握相关知识点的概率 2.2 P (T )为经过练习后学生掌握目标知识点的概率 2.3 P (G)表示学生猜对答案的概率（有趣） 2.4 P ( S)为学生掌握知识点但做错题目的概率 DKT基于RNN的KT整体来讲，基于RNN结构的追踪模型在性能和可用性方面大幅度超越了传统模型，但是在解释性上略显不足. 模型 论文 方法 年份 DKT-DSC Deep knowledge tracing and dynamic student classification for knowledge tracing 通过在每个时间间隔内将学生分组，预测学生的学习效果 2018 .。。 Incorporating features learned by an enhanced deep knowledge tracing model for stem&#x2F;non-stem job prediction 采用DKT进行知识状态预测，证明了DKT模型在实际工作中的有效性. 2019 EERNN Exercise⁃enhanced sequential modeling for student performance prediction 通过追踪学生的练习记录和相应练习的文本内容，提出了一个通用的练习增强循环神经网络（EERNN）框架 2018 基于注意力的DKT通过注意力机制，可以在过去的交互序列中寻找到与当前问题相关的重信息，从而做出更为准确的预测，并且证明了基于 Transformer的模型比基于 RNN的模型在运算速度上快了一个数量级. 模型 论文 效果 年份 transformer A self ⁃ attentive model for knowledge tracing 必然伴随着对过去相关练习交互的回忆 2019 双向transformer Towards an appropriate query，key，and value computation for knowledge tracing 将练习序列和回答序列分别进行编码，从而寻找到了更为合适的 Query 2020 Saint+ Saint+：integrating temporal features for EdNet correctness prediction 将经过时间、滞后时间两个特征编码与学生答题响应的编码进行结合，从而增强了 模型的预测精度. 2021 基于hawkes过程的DKT .Hawkes过程则假设过去事件会在一定程度上提高未来事件发生的概率，并且这种影响会随着时间指数衰减，这种思想比较符合认知遗忘规律下的学习者能力. 模型 论文 效果 年份 LSTM The neural Hawkes process：a neurally self⁃modulating multivariate point process 利用Hawkes过程对长短期记忆（LSTM）节点的时间效应（遗忘效应）进行衰减处理.KT领域的学习者交互过程可以被看作是一系列的连续事件流，但是泊松过程假定事件相互独立，并不符合多知识点状态下学习者交互的逻辑 2017 Hawkes Process Temporal cross⁃effects in knowledge tracing 定事件相互独立，并不符合多知识点状态下学习者交互的逻辑忘效应）进行衰减处理.KT领域的学习者交互过程可以被看作是一系列的连续事件流，但是泊松过程假 深入研究了不同知识点之间的时间交叉效应，并且提高了深度模型的可解释性 2021 展望本文作者对比讨论了目前主流的KT模型，分析了主流模型的优缺点.目前的研究主要针对知识点与题目间的关系进行建模，很少有研究从模型效果评价指标、学习潜力预测、深度记忆过程模拟等方面进行知识状态追踪和预测，同时也较少有对多知识点关系建模方法进行知识状态追踪的研究.通过分析KT领域目前主流的模型，梳理出KT领域未来的发展方向，从数据表征、认知建模、建模方法、解释及反馈方面对KT领域进行展望. 1）数据处理及数据表征.KT模型在运用输入数据方面越来越需要预处理、预训练操作.预训练模型在序列任务上表现出了良好的性能，采用可解释性较强的算法预处理输入数据变得越来越重要.比如使用Rasch编码预处理输入数据后，再进行注意力运算和模型预测，在模型性能和可解释性方面都取得了很好的效果 .在数据特征方面，引入学习者生物特征、更加丰富的习题特征都是未来重要的突破方向，KT模型应该向更高维度、更普适、更泛化的方向发展，如何对学习者的非结构性学习数据进行追踪也是重要的发展方向. 2）认知建模 .认知诊断和 KT分别应用于学习者静态数据分析和动态数据分析，但 KT模型内不应缺乏对学习者认知能力的建模.对于问题维度、知识点维度的建模不足以拟合学习者的知识状态变化，应在此基础上进一步对认知维度进行建模，从而在更高的维度上追踪学习者的状态变化情况. 3）模型方法及可解释性 .自从 DKT被提出以来，KT领域内的模型基本以深度模型为主，但越来越多的工作表明 DKT无法做到真正的动态自适应 KT.基于 RNN 的模型在数据拟合能力上逐步被以注意 力机制为核心的Transformer类模型超越，未来KT领域建模方法应该在注意力方向、图谱方向进一步发展.人脑记忆的形成过程中，人自身的注意力是重要的一环，这也是基于注意力机制模型结合遗忘建模取得不错效果的关键原因.知识图谱作为非结构化知识表征的重要手段，在KT领域有更进一步的潜力， 并且对于认知能力研究也可以加入图谱技术，从而在可解释性KT方向取得突破."},{"title":"大数据知识了解","date":"2022-07-15T12:34:28.033Z","url":"/2022/07/15/bigdata/","categories":[["undefined",""]],"content":"大数据技术概括 数据分析工具与算法框架 接口适用 只需要了解少部分的sql语言就好 数据生命周期 数据源 预处理（采集分类录入） 储存与索引（储存索引整合） 处理（统计分析 数据挖掘） 决策（可视化 决策） 知识 大数据 技术 批处理（对时间不敏感的） 交互式分析（强调用户参与交互过程） 流式计算（快速反应） 图计算（顶点 边）交互式分析通过多条件联合查询 满足时间，准确性，成本，处理能力要求流式计算满足少量条件下，快速返回结果 快速响应，结果精确，同时服务大量用户计算引擎 spark flink impata presto tidb kylin 数据分析工具 和 算法库 大数据技术发展路径 "},{"title":"留学的问题 嗯嗯","date":"2022-07-14T03:56:39.515Z","url":"/2022/07/14/%E7%95%99%E5%AD%A6%E5%B0%8F%E9%97%AE%E9%A2%98/","categories":[["undefined",""]],"content":"如何找到相对应的信息 微博 小红书（消息来源）如何找到相对应的要求 通过学校的官网 会有一个对应学生的list 1.1 list 主要会分为985 211 一本 二本学生有其相对应的要求（学历越高 要求越低） 1.2 list 内会有一个成绩平均分（还有个加权平均分）如果没有明确标注 选择对自己有利的成绩进行提交你是如何找到机构的 如果判断机构的好坏 选择对应的老师 而不是去选择机构 主要是通过设计一些问题 看看机构老师回答的怎么样 看风评一个机构能帮你做什么择校 申请 材料翻译 收offer 个人申请与简历 你学英语是怎么学的 得准备多长时间自学10个月 然后 参加线下班21天 花费2w4 同机构的情况 一样 要找到好的老师 而不是一个机构（小黄鱼 用的很不错）雅思的口语分数很难考 得准备相对较长的时间（口语根据老哥的说法是 只有一个在一个小机构 学习了很长的时间 才拿到的6.5 ）英语中口语 写作较难但相对来说 听力 阅读较简单 老哥最后的成绩为 6 6 5.5 5 最后为了求稳 报了个语言班 技巧 有的学校不仅仅看雅思 也看 多邻国和朗思（线上朗思相较简单） 有一同学准备了7天 拿到了对应的分数 选择看市场供需 有时候 好的选择有奇效 出去学习 只是过简历关 但 整体的硬实力 还是得靠自己 如果 想平平淡淡 没什么高的技术追求 可以去试试 国企和联通这样的企业花费 2w1 中介费用 2w4 英语学习 0.44w 两次的雅思考试费用 总体花销5w5 感谢老哥 "},{"title":"论文撰写","date":"2022-07-12T03:38:51.691Z","url":"/2022/07/12/idea/","categories":[["undefined",""]],"content":"idea 相关领域顶会（想法产生）会议是新的 领域大佬（自己领域的大佬） 组会交流（迁移，集成，多模态）– 知识迁移，模仿是最快的发论文方式（集成 boosting starting besting）领域的baseline调通 上一届师兄成果 准备工作 论文5篇 至少一个综述 1.1 挖掘新想法 nlp  1.2 aminer 研究方面：动机 问题 贡献点 最后是方式 2.1 谷歌学术 dblp web of science arxiv 溯源网：aminer 工程方面：代码 数据集 实验设置（超参数 sota） key point 对自己研究领域有初步认识 idea好坏 创新性 直观解释 数学分析 可行性和可验证性 怎么写 论文题目很重要 题目长度 不能太长 缩写和学术用语 题目最后要商讨 文章结构 discussion 可写可不写 最好不写 先word 在latex 到时候换模板 好转换title 摘要 问题 动机 方法 方法展开 实验 关键词 应用背景（应用类型文章） 背景 现实背景 方法背景（主要存在的问题） 类似方法背景（不用与优势） 提出的方法的概述 贡献点 相关工作 研究问题的相关工作 方法 问题定义（定义方式和符号） 总体架构-算法伪代码（图）一般要有 加分！ 算法流程图 实验流程 实验 数据集 评估标准 baseline 实验设置 比较论文的复现结果（如果差）打上鑫号 结果分析 图表是定性分析 投稿流程 杂谈 假如说会议六月份 一月份准备 实验要两三个月 论文一个星期就能做好 提前一个月吧事情都准备好 格式调整 需要一个月 期刊 会议有个摘要截止日期 （会议需要一个月准备）文章都写好这个样子 "},{"title":"pathway博客以及文章阅读（没看懂 自己水平太差）","date":"2022-07-06T06:41:32.167Z","url":"/2022/07/06/pathway%E5%8D%9A%E5%AE%A2%E4%BB%A5%E5%8F%8A%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/","categories":[["undefined",""]],"content":" 链接：  jeff dean 和 sanjay 是一个一起快乐结对编程好基友 0. 前言简单和常见的分布式 是在单机算完每一个梯度backward后，在调用一个allreduce（把每一个人算的梯度，在各个机器上面做一次相加）"},{"title":"基于知识细粒度查询设计方案(使用手册)","date":"2022-07-02T12:17:08.173Z","url":"/2022/07/02/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E7%BB%86%E7%B2%92%E5%BA%A6%E6%9F%A5%E8%AF%A2%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/","categories":[["undefined",""]],"content":"作者：谢昊 班级：计算机应用技术 1. 使用说明本系统涉及的环境有点多，本设计将从爬虫获取，知识点提取，知识点展示来说明。 2. 爬虫获取项目爬虫功能是一个代码开源的软件来完成对应的功能 软件设计清晰 且帮助文档完善 下载网址为： 3. 知识点提取这个是整个项目的难点和重点 知识点提取 是基于mooc视频中的视频提取 举个例子 在 python程序语言设计中RGB色彩体系这个课中，存在下列两个小的知识点。但是视频并没有对其进行区分。（没有划分出知识点的起始结束时间） 对于该功能的讲解 打算分成三个部分来说明 3.1 项目介绍 项目的文件夹三个 为pic text video 分别存取 图片 文字 视频的信息 重要的py文件有三个 video_pic ,pic_word, json_mysql 分别完成 下面介绍的 3.2 3.3 3.4 的功能 3.2 视频关键帧提取视频的关键帧提取使用的是 基于ffmpeg的pyav库来进行存取 输入：视频 输出：视频关键帧图片（名称含有视频关键帧时间） 关键代码： 3.2.1 使用方式项目文件为 video_pic 将我们下载好的视频总集 放入video文件夹内 然后修改好 我们要提取的文件夹名称 run 就生成了对应视频总集文件的png 你需要修改的位置 运行的情况 3.3 图片信息提取图片信息的提取，使用了百度提供的ocr服务，基本手敲了300行代码 这个为项目的难点和核心点 输入：视频关键帧图片 输出：含有课程名称，课件名，知识点（topic），知识点内容（content），起始时间，终止时间的json文件 工作流程： 先将一个课件名内的图片按时间戳顺序整理 ocr遍历课件内的图片 图片ocr处理 得到图片文字内容 ocr会返回含有文字的条数 如果条数小于3 则跳过 文字进行垃圾词清理 判断文字条数是不是为 0 为0跳过 和上一个content进行比较 如果存在相同的字段 则不存 若有不同 则添加字段(topic相同) 当topic 发生改变时候 将存下上一次topic改变的时间，和这一次topic的修改时间记为起始时间和结束时间 记下原topic的名称，和content 存为json存取 当遍历到最后一个图片时 将存下上一次topic改变的时间，和这一次topic的修改时间记为起始时间和结束时间 记下原topic的名称，和content 存为json存取 3.3.1 使用方式项目文件: pic_word 通过百度智能云 获得对应的api_key 和 secret_key  填入自己要获取课程知识点的课程名称 3.4 json数据转存通过读取json数据 生成能够插入数据库的.sql文件 输入：知识点json文件 输出：基于课程的sql文件 3.4.1 使用方式 在使用的时候 需要填入对应的className 和sqlroot 想要生成的sql文件名称 4. 数据库展示本次的数据库 展示使用的是 jeecgboot来完成我们的数据库展示的功能 其具体的开发开发文档： 比较好的视频文档： 展示功能完成了 基于 classname topic main的查询功能 完成了基础的增删改 且能够导入导出exel文档来进行查阅"},{"title":"模型调参","date":"2022-06-26T10:33:20.696Z","url":"/2022/06/26/%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/","categories":[["undefined",""]],"content":"看的是 李沐的视频   前言 开始一个好的基线 改一个值 重新训练模型 重复很多次 去获得直觉 什么超参数很重要 模型对于超参数 的敏感程度（Adam比SGD 调参要简单很多） 超参数的范围在哪里 结果会好 做好笔记 需要认真的管理 （训练日志和超参数）execl 或着 word tensorboardweight&amp;bias 重现很难（硬件，库，代码，随机性） 自动调参 HPO 超参数优化 更加泛化一点 NAS HPO的小一点版本 专注于神经网络 HPO 哈哈哈搜索空间 算法1 黑盒算法遍历！ 找到最好的 也是最傻的算法 算法2 多准确度（加速）目前再用的算法 小数据集 缩小模型规模 层 通道数 快点停止 搜索策略（黑盒子）第一个暴力穷举（贵） 第二个随机取（有效） 写代码很简单 第三个 贝叶斯 （李沐说这个大研究方向 咱不研究 咱不用） 搜索策略 （多准确度）深度学习较多sh算法 节省花销对于多超参数 随机选取n个参数 去训练m个epoch 算法过程 类似一种 递归的思想在其中 选取n&#x2F;2的超参数进行训练 训练m次epoch 选取n&#x2F;4的超参数训练 训练2m次epoch m n 的选取 取决于 你的预算花销 hyperband 用的较多多跑几个sh算法 在每次跑完后 减小n 增大m 每次都是用不同的m 和 n 影响是 对于 m n的选取 没那么大的影响 Neural Architecture Search (NAS)神经结构搜索 这里只关心 神经网络的参数 强化学习 nas代价非常贵 模型优美 one-shot方法学习模型架构 外加超参数 因为 这个东西看起来就很大 所以 关注点在于 只关心架构之间的排名关系 用一个近似的指标 ： 只训练很少的epoch 来验证 one-shot方法 – 可微架构搜索 多种候选层方法 在每一层 在layer层 中 所有的候选层方法 都会有个输出 对于这个输出 添加一个大于0 且 相加为1 的权重系数相乘 最后 学习 得到 在候选层 中权重最大的给保留下来 通过学习 来判断最好的那一条路径 使用DARTS 这个方法 可以在三个GPU天 完成sota效果 简单实用 effientNet调参不多 cnn优化 更多层 更多输出通道 输入图像更大 目前研究方向 可解释性 调整参数 能使用在边缘设备都能跑 整个流程更加的自动化 "},{"title":"研究的艺术","date":"2022-06-26T02:28:16.575Z","url":"/2022/06/26/%E7%A0%94%E7%A9%B6%E7%9A%84%E8%89%BA%E6%9C%AF/","categories":[["undefined",""]],"content":"第一部分 研究读者知道什么 想要知道什么 研究是什么东西 收集信息（提出解决方案） 回答一个疑问 解决一个问题 为什么要写作 会记住 帮助理解 测试你的想法 通过写作来看你的想法是不是对的 为什么要用论文格式对于 对方的方便 以及更好的理解 第二部分 和读者进行连接写作是一个想象中的对话作者自己定义的角色 和 给读者假定一个角色 而且 角色定好 就没有变了 理解作者角色 我找到一些有趣的信息 给你们看看 我找到一个解决方案 在实际问题上 我找到一个重要问题的答案 对应读者角色 娱乐我 （追求新奇的信息） 帮助我解决实际问题（直接跳干货位置） 帮助我更好的理解（大部分的研究者） 怎么去问问题 怎么去找答案 你的研究 是否让人感兴趣 预测读者的回应 我的观点是不是跟所有人的观点是对立的 会不会做标准的一些问句 来反对解决方案 是不是特别关心我是怎么一步一步的解决问题 第三部分 话题到问题针对领域 选择一个针对的话题。话题是一个途径去让你问问题，问题回答得好 别人就会有兴趣 整个领域的人都会有感兴趣。 针对话题问问题 问题的答案 别人会觉得比较重要 答案能在一定程度上改变领域 question or problem question不一定会带来问题 question是一个找到问题的途径 在学界 我们主要是解决problem 而不是question 兴趣去找话题 怎么把效果做出来（提出算法 之前不行 现在很行） 做大 （数据更大 规模更大） 做便宜 做安全点 话题 缩小 到能研究的东西 话题 转换成 论点 针对话题 去问问题(找到解决方法 更能针对的去读文献) so what！当你对一个问题感兴趣的时候 你应该去问一个更难的问题 能不能带来好处 能不能推动领域发展 能不能启发工作 如果不做 整个领域会不会有损失 你最后的工作要别人值得去读，如果一个工作对自己和对别人来说 不知道有什么价值。可以考虑放弃这个工作。 如何问一个更难的问题（so what） 列出话题 话题加一个间接的问题（indirect question） 含有w词 who what when how where 评估问题的重要性 找到的问题 是领域关心的问题 （这时候就不需要在意东西的意义在什么位置）理工科领域 如果能很好解决痛点 一般是有意义 想一想事情的意义 question to problem（疑问到解决问题）通过三步来判断疑问是不是一个值得解决的问题 topic 回答topic 的 question 找到意义 这样 就能很好的找到了一个解决问题的problem 一直去问so what（面向读者的角度） 直到 觉得什么时候 可以停 找到好的研究问题（很难） 找人帮忙 读的时候去找问题 写作中找 第四部分 怎么讲好一个故事（论证）假设研究正常展开，开展到一半或者一大半。我的读者有兴趣去看。如何让我的读者信我的东西或者说使大家相信我的新方法。通过讲一个故事，使读者信我们的东西。 故事 和 信息搜索故事 是 给我们收集信息 给定一个方向（做实验） 而同样的 故事 也能回答读者可以预测的问题，研究还剩下多少东西能做 而同样 在故事发生变更的情况下，可能需要重新去做实验，在不同的超参数或者不同的情况，甚至需要更换数据集。 一边做实验一边讲故事，早一点讲故事给想出来。 最后的来说 跟读者做一个合作探索，达到相同的一个认知水平，方法不一定是最好的。但一定要激发读者的认知，发现一个更好的想法。在方法中可以在做改进 how 足够多的原因和论据 要在读者的角度去问（为什么我要相信这个事情）和 so what 这两个问题 要在做研究的过程中 不断地去问这两个问题 做好一个论证首先要提出一个论点，用原因和证据来支撑论点。有时候你需要承认和回复一些别的观点，最后你要提供一下推理一些逻辑的原则。 写作是一个假想的对话，使得在跟真人对话之前，把所有的可能性，别人攻击你的地方以及缺失的理由，论点，论据全部给补充起来。 论点：我们核心的理由是什么东西，很多时候文章最后就是一个核心的论点，当然论点还有别的子论点支撑； 理由：为什么我们的论点是对的； 论据：一些数据点或者别人的工作； 承认和回复：对于别的一个观点的一个说明； 保证：这个逻辑是怎么样过来的，如果读者不理解的话，应该把它说出来，解释一下我们的理由为什么能解释我们的结论好； 核心要干的事情（支撑论点） 怎么用原因和论据啊来支撑你的论点： 用原因来支撑的话，一般会有一个因为这一个词在这个地方 在一般的情况下，很少只用一个理由来支撑你的论点，很有可能会用多个理由，而特别的是说，其实理由的本身它也是一个论点； 什么时候可以结束说明理由： 理由必须是在论据之上的，结论是基于好的原因，理由又是基于好的论据； 所谓的论据就包括了，做实验得出来的一些实验的结果，或者前面的值得信任那些工作里面的一些论点，这个是现实存在；理由很多时候更多是一个思维的逻辑，是存在你脑海中的； 一旦我们成功的把论点通过理由和论据支撑住了之后，被大家认可之后，我们的论点也会成为别人工作的一个论据。 我们一个论点需要有原因来支持，这些原因又是在基于我们的论据上面的，所以我们要保证说原因能够合理的解释我们的论点，反过来讲每一个原因也需要有他的论据来支撑才是合理存在的； 使用承认和回复 作者提前的预测读者可能会提的一些反对意见，然后把答案写在这里，这样子在读者读的过程中心中产生问题的时候，作者就在下面就把这些问题给回答掉了。 其实回答本身不是最难的问题，最难的是说在写的时候要假设他们这些问题的存在，就是你要想到你的读者可能会提这样子的问题。 核心是说我们得尽量的去考虑到很周全，去想象你的读者会问这样子的问题； 我们的论点可能有一些反对的意见，或者不同的解释、不同的看法，这样子我们需要去承认这些东西的存在，并且给予回复 用理由去支撑论文时，读者可能会看不出它们之间的关系 在读者看不明白的时候，需要补充说明他们之间的联系，不然读者是不会买账的； 这个补充说明一般是一些通用的原则，就是一些大家都能接触的东西，然后把它作为一个通用的原则。在这个原则之下，能够特立出我们的推理逻辑； 当原因和论点之间隔得比较远的时候，需要给出一些推理的保证，来使得读者能清楚的认识到我们的原因和论点是怎么样联系起来的。 实际上我们正在写的时候，整个逻辑可能是比较复杂的，可能就想说一个很简单的东西也可以变得比较复杂。 想把一个文章写的有理有据、滴水不漏是一件很难的事情 要把argument弄得“厚一点”：在支撑论点的时候，正正反反啊都要多讲一点，因为我们的目的是要通过这些比较厚实的论述，让读者能够相信我们所说的内容； 关于声明声明就是对研究问题的答案，就是把答案浓缩成一句话变成声明，然后整个文章呢主要是围绕去支撑这个声明； 声明要考虑那么下面这3个问题： 我在做一个什么样类别的声明：因为不同类别的声明，导致可能要支撑他的这些证据是不一样的； 声明够不够具体：因为对于比较空洞的声明，大家读起来会觉得比较空洞； 声明啊够不够重要：读者觉不觉得有必要去花一篇文章去支撑我这样子的声明； 我们要解决一个有价值的问题，然后我的解决的方法本身应该也要是有价值的 声明需要是具体的而且是重要的 怎么样把声明变得更重要一些 对一个大家感兴趣的话题提供新的证据； 不仅仅把数据展示出来而是要用数据去回答一个大家有争论、不那么确定的问题的答案； 如何让论点更加可信 怎么样把论点变得更加可信一点：​ 如果想让别人信我们说的话，最好不要把话说的特别的满；​ 可以承认一些局限性的条件​ 要去想的限制条件的时候，是从读者角度来出发的。从他们的角度来讲，去想想我们的理由也好我们的论据也好，在哪些地方更加薄弱一点，把这些薄弱的地方作为限制条件给出来之话，那么对整个的可信度就会增加 使用一些降低语气确信度的词，使得论点显得没那么的强硬： 如果讲的特别自信的话，读者会觉得更加的难以置信一些； 如果用了大量这样子的不确定的词汇呢，整个文章可能会显得比较弱，别人会觉得你可能自己也不是那么的确信。 尽量要避免这些词汇：all、no one、every、always、never 第五部分 理由、论证读者怎么看待理由和论证 读者首先会去看我们论述的核心（论点和它的一些支撑的论据）； 对于支撑来说，读者会先去看我们提过的那一些理由，然后去看一下它是不是有道理； 如果理由靠谱的话，读者会去看它的整个逻辑，然后把它排好序，读者会顺着这个思路往下看，看看这个逻辑是不是过得去； 如果这些理由看上去还不错的话，那么接下来他去会去看论据，论据是整个论述的一个基石，论据理应是不容质疑的，但是如果读者不相信你的论据的话，那么他也就不会相信我们的理由。 核心是说，我们有一个论点，然后通过理由，这个是能够架在我们的论据上面，理由是能够撑住你的论点的，如果中间任何一个部分没有做好的话，那么导致我们的论点是支撑不起来，就会导致大家不会信你 如果要去收集整个论述要怎么办 首先应该给读者提供一些合理的理由； 然后这些理由需要在一个清晰的有逻辑的顺序之下； 最后所有这些理由必须要是基于论据的，而且读者是可以接受这些论据的 怎么区分 论据和理由 不是由我们来决定什么是理由什么是论据，而是读者来决定的； 需要把整个论据写的非常的脚踏实地 如何评估证据好坏 一个好的证据必须是准确的、精确的、足够的、有代表性的和权威的； 必须要很准确的来报告证据，作为证据来讲一般来自于两种可能性：从自己收集而来的（比如说做实验采集到了证据）；来自于前面人的一个工作； 所以不管是哪一种都要很准确的报告你的数据怎么来的： 对第一种来讲，要说整个实验是怎么做的，流程是什么样子，然后这数据是怎么样采集的，这需要我们能够准确的描述这个流程，而且大家是认可我们的这个方法的； 如果论据是来自于别人方法的话，那么把证据从别人的报告搬到我们这里来的时候，要足够准确，至少是ctrl C + ctrl V过来的，不要把一些数字搞错了 需要证据准确精确就说不要使用这种很模棱两可的词，而要使用一些比较精确的语言；所谓的模棱两可的词就包括了some most many almost often usually frequently generally这都是一些比较模糊的词，我们要尽量得避免它 需要足够的且有代表性文章举了个例子 即 莎士比亚 一定是 憎恨女性 （因为在麦克白和哈姆雷特里面 将女性描述成虚伪的代表） 出现了两个问题 例子的不全面 这个例子不够有代表性 不能体现作者的意图 证据权威性好的会议引用好的文章 不要使用一些差很多的文章 （最好不用维基百科 这样的内容） 读者不会相信 总结 对于论点怎么样用理由和论据来支撑； 理由通常有多个而且是要合理的，而且需要用合适的顺序把它组织起来； 证据是要需要强有力的而且是读者要认可的，包括你的报告是准确的； 然后这数据的精度是合适的，全面的有代表性的以及是权威的； 第六部分 承认和回应 需要去回应读者心中的那些不同的看法； 写文章的时候要去预测、承认、回应这些读者在读你文章过程中间产生出来的一些问题、反对意见和一些另外的解决方法； 难点是说，我们的写作是一个假想的对话，我们并不知道读者在真的看到我们这么写的时候的反馈，所以在这个时候，我们需要去想象读者的情况和我们要怎么样回应； 在本章节作者会告诉我们，可以从两个方法来想象读者可能会怎么样提供一些不一样的看法，然后也告诉大家怎么样去承认和回复这样子的看法； 读者通常有两种方法去挑战我们说的东西： 内在的完备性，也就是说他们会挑战论点是不是讲的很清楚，然后理由是不是相关的，以及说论据的质量是怎么样的； 外部的完备性：包括了 是不是有一个别的方法来重新来讲我们的问题，或者是说 是不是有一些我们自己没有注意到论据，还有是说 是不是有别人的工作也写过相似的一些话题，但是他们提供了不一样的意见，我们并没有引用他们； 如何找到没有发现的问题（很难） 这是一个很难的事情，比如已经花了很多时间去想象各种理由、各种证据的好坏，如果读者还是有一些不一样的问题的话，那肯定是没有想到的，当时如果已经花了很长时间的话，那再花更多时间可能也不一定能想到；所以要去找别人来看一看，因为别人看的话，反正他也不知道你怎么想的，所以他没有先验的偏见，也许他可以提供一些不一样的看法； 所以在做研究的时候，时不时找人（不管是同学、师兄师姐、师弟师妹或者是导师或者是同事）能够让他们来帮忙看一看现在有的东西也是非常有用的； 如果还是找不到怎么办，作者给大家提供了一些问题可以去问自己，就当自己是读者，从这些方向去看，看看是不是能够找出什么漏洞出来： 这为什么是一个问题（问题应该是指我们所研究的东西）？因为有时候问题可能不是真正存在，只是我们构造出来的或者你想象出来的啊 问题是不是已经很好地定义；（有时候 定义清晰 就很解决问题） 看看自己的解决方案： 看一下自己这些原因和论据 能不能提供一些别的种类的论据； 有很多数字但可能希望一些关于真实案例的情况，或者说只有一些真实的案例但是并没有统计上的一些数字； 所有的论据可以去看它的一个质量（是否准确、精度是不是高、是不是有代表性、是不是权威的）；最难的是说，需要更多的证据； 我们可以用这些问题来客观的去看你现在有的所有的东西，从而发现你的论述中间薄弱的一些地方，在找到论述里面的薄弱点之后，接下来要去决定我们应该把哪一些拿出来写。 如果你承认了太多不一样的观点的话，会导致你的文章很长，别人会觉得说你说了那么多有的没的，那么你的自己的方案在哪里；如果你承认太少，别人会觉得说你可能想地不够深入，你的置信度就没那么高，所以我们需要有一个合适的一个平衡点； 你需要更多的证据 可能会遇到一些问题是无法回答的： 要纠正一个思想是说我们的文章解决了某一个问题的所有，在绝大部分的情况下，我们的研究工作，只是回答了一个小的问题的一部分，所以肯定会漏想了一些别的部分，以及说跟这个问题更大的一些问题的东西，可能是无法回答的； 有时候承认自己无法回答别的问题也很正常了，大家也是这么理解的。对于无知来讲，知道自己无知的无知和不知道自己无知的无知，后者当然更加无知一点，所以宁可自己做前者。 第七部分 推理的保证五点建议 让我们看看准则或者公理是不是合适的： 它是不是有道理的，因为我们对一个这样的担保，通常不会用原因去证明它是合理的，所以你至少让读者觉得他是一个合理的； 这个是不是它覆盖面不用特别广，因为如果一个担保想覆盖更多通用的情况下，就会显得它更加的薄一点，可能反例就更多一点。很多时候我们只要这个担保，能够足够覆盖到论点和原因就行了。 有没有别的一些更好的担保，有时候我们在数学里面做公理的时候，当我们要选一个最好的一个公理而不是选一些公理下面的一些东西； 对于我们的这个领域（大家的研究文章都是发在某个领域上面）是不是合适的，就这个领域的人能不能接受这样子的观点； 我们得覆盖住论点和原因； 什么时候需要用到担保 读者是在领域之外的时候（写教科书的时候经常会用它），大量的新来读者不清楚里面的一些隐藏逻辑，把它给大家讲出来是非常有用的，不然读者可能会看不懂。如果说发一篇文章，必须要是自然杂志的话，它的读者相对来说比较广的时候，也尽量要把这个领域相关的一些东西给大家写出来，让大家明白我们的这一个逻辑； 如果我们使用的原则对这个领域的读者来说，比较新或者是有争议的时候，也应该把它讲一讲； 当我们的论点特别有争议性可能读者觉得很难接受的时候，那么在大家都会接受的情况下在前面说一些准则（如果他接受第一句，然后再过渡到第二句的时候）就显得没那么难接受一些 我们选择去把这样子担保说出来时候，意味着是说我们其实关心读者，生怕他不懂我在说什么，所以把前面这一些逻辑给他们交代的更清楚； "},{"title":"虚拟机和mobasterm连接","date":"2022-06-24T13:53:11.268Z","url":"/2022/06/24/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%92%8Cmobasterm%E8%BF%9E%E6%8E%A5/","categories":[["undefined",""]],"content":"未来可能这个教程会被反复的翻阅 我纠结这个东西纠结了好久 现在写一下 首先感谢：   这里贴一个centos 安装 ssh 教程  0. 前言这个连接 主要看两个东西 第一个看 虚拟机 即看是不是在同一网段内 第二个 看 虚拟机的ssh服务是不是安装着 我在安装ssh时候出现了错误 感谢：  解决方式： ok 这时候 应该就能快乐连接了"},{"title":"python遇到安装不了的包的解决","date":"2022-06-22T05:03:17.638Z","url":"/2022/06/22/python%E9%81%87%E5%88%B0%E5%AE%89%E8%A3%85%E4%B8%8D%E4%BA%86%E7%9A%84%E5%8C%85%E7%9A%84%E8%A7%A3%E5%86%B3/","categories":[["undefined",""]],"content":"感谢： 操作流程  包编译好的文件 2022&#x2F;6&#x2F;22 今天试了试 paddle ocr 包 说实话 安装的的过程有点慢吞吞 于是乎出现了这个样子的错误 错误的大致描述是 c 没有编译好这个包 嗯！ 1. 破局假如 你的python环境是 3.9 你就在这个网站 找人家编好的文件 然后下载完成后按照：文件位置+文件名的格式，直接pip install，例如我的安装的是： 然后 这个包就转好了！ 2. 感悟 有问题 多看看论坛 github不好使的时候，用用gitee pytorch不好使的时候 用用paddle "},{"title":"GRU4Rec 推荐论文","date":"2022-06-12T03:15:52.399Z","url":"/2022/06/12/gru4rec-%E6%8E%A8%E8%8D%90%E8%AE%BA%E6%96%87/","categories":[["undefined",""]],"content":"这是16年出的一篇基于session时序建模的召回模型paper，在当时大部分模型只考虑用户最后点击行为，而忽略用户历史点击行为。虽然用户最后一次点击行为与用户下一次点击的item相关度很高，但是用户历史点击行为一方面丰富了用户画像，另一方面用户的兴趣是多峰的，不一定最后一次点击是最相关的。 模型到现在看的话 实在是简单了一些 重要的是得看他的亮点 1. 亮点说实话 第一个两点我看了半天（可能是上午的吃的太少 抑或是 钱给的太少） 1.1 亮点1 训练时session重组 好笑的是 我怎么忘了这个东西 本文的训练过程是 通过i1,1 来预测 i1.2 所以训练序列要比 会话序列 要短一个 假设一个batch有三条数据，代表不同的session。大家看到session1有4条数据，session2有3条数据，session3有6条，真正作为input的话，每个session中item序列长度只有3，2，5，next item是作为groud truth。记录下batch最短的input 长度为2。首先组装 作为第一个batch的input，output是 ，然后组装第二个batch，input： ，output： ，这时候session2已经处理完了，但是session1，session2还没有处理完，这时候用session4来替换session2，切换session的时候要重新初始化下hidden state。这时候第三个batch的input变成了 ，output是 。到第四个batch的时候session1也处理完了，使用session5来替换。这样下来就能支持多个session并行处理，极大的提升了训练速度。 注意为了保证Session内的连续性和Session间的独立性： 1.2 亮点2 negative sampling作者解释这种负采样方法，首先提出了一个假设，那就是用户不太可能将没见过的东西标记为负样本。如果一个物品很流行，而用户却没有在next-item位置进行点击，那么用户大概率在该时序点不喜欢这个物品。 基于这样的假设和观察，作者认为，应该基于物品的流行程度进行负采样，那么作者就提出了一个巧妙的快速负采样方法，那就是一个batch中的其他序列的next-item作为负样本。如下图: 序列的next-item的gt是1, 5, 8对于1来讲 5，8是负样本。和随机抽样相比，个人觉得优势有两个: 一个是基于pop的采样，可解释性更强，另一个是对于batch内需要计算的item是一致的，那么可以将batch内所有序列的next-item预测，统一成一个矩阵运算，大大降低显存占用 同一个batch 时序 其他的mini-batch 为负样本 表示 用户在这个时间内 不喜欢该样本 2. 作者的发现 多layer不一定好 one-hot的编码更好 每一步输入前面所有的信息和前一步信息性能差不多 提高GRU的宽度有帮助 作者的发现表明 GRU 足够可以编码 item 信息(one-hot更好)，单个layer就可以达到很好的效果，对序列的建模能力比较好。"},{"title":"TAGNN 文章阅读","date":"2022-06-10T13:41:20.736Z","url":"/2022/06/10/TAGNN-%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB-%E6%9C%AA%E5%86%99%E5%AE%8C/","categories":[["undefined",""]],"content":"自己瞎说一点 这篇文章 是发表在SIGIR上 一篇文章 作者一开始指出 顺序模型 无法有效有效的捕捉项目之间的来回关系。 建议通过会话图发现会话下的复杂过渡模式[2，10，11]。通过将会话中的项目建模为会话图，这种对会话中丰富的时间模式进行编码的自然方法可以为每个项目生成更精确的表示。 然后 使用一个固定的向量 限制了推荐模型的表达能力 观察到，在对特定候选项进行预测时，没有必要将所有用户兴趣嵌入到一个向量中。例如，假设客户的历史会话为（游泳衣、钱包、牛奶、煎锅）。如果我们想为她推荐一个手提包，我们会关注她对钱包的兴趣，而不是对煎锅的兴趣。也就是说，如果给定一个目标项，可以具体激活具有丰富行为的用户的兴趣 这篇论文在SR-GNN的基础上，沿用了门控图神经网络（Gated Graph Neural Networks，GGNN）模型，并加入了对预测目标敏感的embedding表示，下面进行介绍。 0. 前言文章贡献 会话中的项目建模为会话图，以捕获会话中复杂的项目转换 图神经网络来获得项目嵌入 会话中自适应地激活用户的不同兴趣，我们提出了一种新的目标关注网络 （重点） "},{"title":"模型过拟合处理","date":"2022-06-09T08:51:28.478Z","url":"/2022/06/09/%E8%BF%87%E6%8B%9F%E5%90%88%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"0. 前言这篇文章想写好久了，一直拖着。其一开始 在阅读 SASrec的时候 发现为了加入注意力机制 给了三个处理过拟合的方法 为此 想写一个文章 来整理一下 这三个技术的作用 1. dropout感谢： 1.1 出现的原因在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。 过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。 综上所述，训练深度神经网络的时候，总是会遇到两大缺点： （1）容易过拟合 （2）费时 Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。 1.2 啥是dropoutDropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。 Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征 1.3 why？（1）取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。 （2）减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。 （3）Dropout类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝 2. 残差网络(Residual Network)，残差连接(skip-connect)感谢： 2.1 动机深度神经网络的退化问题 以及 梯度弥散&#x2F;爆炸 问题 残差网络很好地解决了深度神经网络的退化问题，并在ImageNet和CIFAR-10等图像任务上取得了非常好的结果，同等层数的前提下残差网络也收敛得更快。这使得前馈神经网络可以采用更深的设计。除此之外，去除个别神经网络层，残差网络的表现不会受到显著影响 3. 归一化 BN 和 LN区别  还是区别  这个应该是最能让人清楚的bn 和 ln 区别的文章  归一化的整体理解  norm pytorch 怎么用 3.1 作用 Norm起作用的本质是它平滑了Loss，保持了梯度下降过程中的稳定。 3.2 layer normalization Latch Normalizaiton在NLP中的直观图中，是对一个btz中的同一句话中每个字进行归一化，即图中红色箭头方向，对该方向这一桶计算均值和方差后，计算归一化；以此对整个btz进行归一化。 3.3 Batch Normalizaiton Batch normalizaiton在NLP中的直观图中，是对一个btz中的每句话同一个位置的字进行归一化，即图中红色箭头方向，对这一桶计算均值和方差后，计算归一化；以此对整个btz进行归一化。 BN缺点* Btz太小会影响。对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；* BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的，换句话说RNN的深度不是固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其他sequence长很多，这样training时，计算很麻烦BN不适用于RNN等动态网络，适用于CNN；LN适用于RNN。"},{"title":"图神经网络入门","date":"2022-06-09T06:42:51.894Z","url":"/2022/06/09/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"感谢： 博客  李沐介绍视频 0. 前言首先，让我们确定什么是图。图代表了一组实体（结点）之间的关系（边）。 V node 节点 E link 连接 U master node 整个图 1. 描述（抽象）为了进一步描述每个节点、边或整个图，我们将图中的每一部分信息进行储存。 在存储方面 我们存下了点，边，整个图的信息 进行存储 图 也分为有向图和无向图 2. 图的实例化讲解作者 使用了两个反直觉的数据类型用图来表示 2.1 图像我们通常认为图像是具有图像通道的矩形网格，将它们表示为数组（例如244x244x3的浮点）。另一种思考图像的方式是具有规则结构的图，其中每个像素代表一个节点，并通过边连接到相邻的像素。每个非边界像素正好有8个邻居，每个节点存储的信息是一个代表像素RGB值的3维矢量。 通过邻接矩阵来可视化图形的连通性的一种方法。我们对节点进行排序，在这种情况下，在一个简单的5x5的笑脸图像中，每个节点都有25个像素，如果两个节点共享一条边，就用一个条目来填充n×n的矩阵。请注意，下面这三种表示方法都是对同一份数据的不同看法。 图像的三种表示方法 2.2 文本我们可以通过为每个字符、词或标记关联索引来数字化文本，并将文本表示为这些索引的序列。这就形成了一个简单的有向图，其中每个字符或索引都是一个节点，并通过一条边与后面的节点相连。 当然，在实践中，这通常不是文本和图像的编码方式：这些图表示是多余的，因为所有图像和所有文本都会有非常规则的结构。例如，图像的邻接矩阵有一个带状结构，因为所有的节点（像素）都是以网格方式连接的。文本的邻接矩阵只是一条对角线，因为每个词只与前一个词和后一个词相连接 3. 什么样子的问题会用到图结构3.1 图级看起来很弱智的 找圈圈个数 3.2 节点级​ 节点级任务关注的是预测图中每个节点的身份或角色。​ 节点级预测问题的一个典型例子是Zach的空手道俱乐部。​ 该数据集是一个单一的社会网络图，由在政治裂痕后宣誓效忠于两个空手道俱乐部之一的个人组成。正如故事所言，Hi先生（教练）和John H（管理员）之间的争执在空手道俱乐部中造成了分裂。节点代表空手道练习者个人，边则代表这些成员在空手道之外的互动关系。预测问题是对一个给定的成员在争斗后是否会效忠于Hi先生或John H进行分类。在这种情况下，一个节点与教官或管理员之间的距离与这个标签高度相关 [] ​ 按照图像的类比，节点级预测问题类似于图像分割，我们试图标记图像中每个像素的作用。对于文本，类似的任务是预测句子中每个词的语音部分（如名词、动词、副词等）。 3.3 边级 通过语义分割 分析出实体对象 然后对实体对象之间的关系 使用机器学习进行预测 之后用图来进行表示 4. 图的表示 自己的看法 dim &#x3D; 0 的位置上 每个节点一一对应 节点有节点 自己的分类 边也是一样 如果想表示一个图两个节点是否相互连接 可以使用【node，node】来表示 global 也有其自己的分类表示 4.1 网络表示 这个GNN在图的每个分量上使用一个单独的多层感知器（MLP）（或你最喜欢的可微分模型）；我们称之为GNN层。对于每个节点向量，我们应用MLP并得到一个学习的节点向量。我们对每条边做同样的工作，学习每条边的嵌入，也对全局背景向量做同样的工作，为整个图学习一个单一的嵌入。 4.2 通过池化信息 来完成图预测（简单gnn模型）可能 我们知道了边的信息 如果通过边的信息来 计算 节点的信息 对节点进行预测池化分两步 收集节点的边信息 通过汇总这些信息来进行预测 所以 如果我们只知道 边的信息 使用池化来路由（或传递）信息到它需要去的地方。该模型看起来像这样。（这个机制貌似叫 消息传递） 使用点信息来 信息传递出 边的总类 使用点信息来算出 图的种类 对点的信息进行汇聚 然后 就像cnn 一样 对这个图的信息进行处理 4.3 通过信息传递 完成 图的更新信息传递分为三步 收集每个节点 所有相邻节点的嵌入 g函数 通过聚合函数 聚合消息 所有汇集的消息 通过一个更新函数（通常学习神经网络） 正如池化可以应用于节点或边一样，消息传递也可以在节点或边之间发生。 这些步骤是利用图的连接性的关键。我们将在GNN层中建立更详细的消息传递变体，以产生表现力和力量不断增强的GNN模型。 "},{"title":"2022年会话推荐综述","date":"2022-06-05T11:45:38.441Z","url":"/2022/06/05/2022%E5%B9%B4%E4%BC%9A%E8%AF%9D%E6%8E%A8%E8%8D%90%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"最近对于会话推荐有了新的兴趣 文章题目： A Survey on Session-based Recommender Systems 0. 前言 提供了一个统一的框架来对SBRSs研究进行分类 SBRS的统一问题陈述，其中SBRS建立在正式概念之上：用户、项目、动作、交互和会话我们全面概述了会话数据的独特特性以及由此带来的SBRSs挑战 会话任务方法进行了系统的分类和比较 全面了解如何应对挑战以及SBRS领域取得了哪些进展简要介绍了SBRSs的每一类方法以及关键技术细节 讨论了SBRS研究中存在的问题和前景 1. 序列推荐和会话推荐的区别 作者对于Boundary解释 是指在事务事件中启动和结束特定会话的开始-结束交互对 会话 可以分为 有序会话 和 无序对话 这个session 内 交互 内item 是不是按顺序分布的来区分是否为 无序有序 对于边界间隔 session 有很多个 而 序列 只有单一一个 对于 其 嵌入的主要关系 基于 会话的 是 共现关系 而 基于 序列的 是 顺序依赖关系 2. 会话推荐2.1 目的会话推荐需要注意attention SBRS旨在通过学习会话内或会话间的依赖关系，预测给定已知部分的会话的未知部分（例如，一个项目或一批项目），或给定历史会话的未来会话（例如，下一个篮子）。 原则上，SBRS不一定依赖会话内的顺序信息，但对于有序会话，可以利用自然存在的顺序依赖性进行建议。 2.2 框架主要工作分为三个子领域 其子领域 可以分为 下一次交互推荐 下一次部分会话推荐（即 下一个会话出现了一部分 预测剩余的部分） 下一次会话推荐 Point-Of-Interest (POI) 生词 其实 在框架体现中 可以看出 下一个项目的推荐是 最多的还是一个交互的推荐 2.3 相关的研究作者认为现有的研究没有发现任何系统地将这一研究领域正规化的研究，或全面分析会话数据的独特特征和SBRS所面临的关键挑战。更不用说提供一个深入的的总结，或详细说明该领域存在的公开研究问题。 对于 相关的研究 习惯将 RS 和 SBRS 混为一谈 且 特别针对 SBRS 的研究特别少。工作主要集中在序列感知RSs上，只讨论了一小部分基于有序会话数据的SBRS工作，而忽略了基于无序会话的SBRS。 2.4 会话推荐的主要符号 一个表征通常被指定为一个潜在的向量 通过这里 可以看出 不同于 序列推荐的 ui 组合 在会话推荐中 更倾向于 UVO 这样的三元组组合 2.5 SBRS问题陈述一个RS可以被看作是一个系统[8, 9]，它由多个基本实体组成，包括用户。物品和它们的行为，例如，用户与物品的互动。这些基本实体和行为构成了会话的核心成分，也就是SBRS的核心实体。因此，我们首先介绍这些实体和行为的定义和属性，然后在此基础上定义SBRS问题。基于它们的定义。这些定义和属性将被进一步用于SBRS的特征和这些定义和属性将进一步用于SBRSs的特征和分类，等等。 2.5.1 用户表示（与序列推荐有些不同）在SBRS中，用户是对物品（如产品）采取行动的主体，如点击、购买。并接受推荐结果。让u表示一个用户，每个用户都有一个唯一的ID和一组描述她的属性，例如，一个用户的性别，它有多个值。如：男性和女性。一个用户的属性可能会影响她对项目的操作，并进一步影响相应的会话。 除了可以明显观察到的显性属性外，还有一些隐性属性，它们反映了用户的内部状态。例如她的情绪和意图，也可能对她的行为产生重大影响。所有的用户共同组成了用户集，即U &#x3D; {u1,u2,. . . ,u |U |}。需要注意的是，一个会话的用户信息可能并不总是可用的，原因有二： (1)由于隐私保护，它不会被记录下来； (2)一些用户在与在线平台互动时不会登录 如amazon.com。因此，会话成为匿名的 2.5.2 item表示 v表示一个项目，该项目与唯一ID和一组属性相关联，以提供项目的描述信息，例如项目的类别和价格。数据集中的所有项目构成项目集，即V&#x3D;{v1，v2，…，V | V |} 没什么好说的 很简单 2.5.3 动作表示 用户通常在会话中对某个项目执行操作，例如单击某个项目。让a表示一个动作，该动作与一个唯一ID和一组属性相关联，以提供其属性信息，例如动作的类型，并具有多个值，例如单击、查看和购买。请注意，某些操作可能与特定项目无关，例如搜索操作或目录导航操作。但如参考文献中所述，它们仍可能为SBRS提供有用的信息 2.5.4 交互 user 交互 available o &#x3D; &lt;u, v, a&gt; no available o &#x3D; &lt;v, a&gt; no available and action only one o &#x3D; 2.5.5 会话会话包含交互!! s &#x3D; {o1,o2, . . . ,o |s |}. 注意一点 单一会话中 可能会出现重复的交互 每一个对话 都与一组属性相关联 例如 持续时间s （ 20分钟或40分钟 ） 属性 定义与影响 session length 会话的长度定义为会话中包含的交互总数。这是会话的一个基本属性 internal order（内部秩序） 一个会话的内部秩序指的是其内部交互的秩序。通常情况下，在不同的会话中存在着不同类型的灵活秩序，即无秩序、灵活秩序和秩序。 action type 在现实世界中，一些会话只包含一种类型的操作，例如购买，而其他会话可能包含多种类型的操作，例如单击、购买。会话中动作类型的数量决定会话内依赖关系是否是同质的（基于单个动作）或异构（基于多种类型的操作），这对于准确的建议很重要 user information 用户信息在连接同一用户在不同时间发生的会话方面起着重要作用，因此其可用性决定了为特定用户跨多个会话建模长期个性化偏好的可能性。用户信息的属性是指会话中用户信息的可用性。实际上，SBRS最初被提议用于处理用户信息不可用的匿名会话 session-data structure 是指与会话相关的层次结构组成的多层次结构。交互层由每个会话中的交互组成，而会话层则由当前用户的多个历史会话组成。（即区分会话和交互） 这个图 可以看出 会话的第五个属性 ： 会话的层次结构 2.6 问题定义2.6.1 输入(1) 当前会话的已知部分（即已发生的交互的列表），这是SBRS的输入。它只为下一个交互（项目）建立会话内的依赖关系，或下一个部分会话的建议（参见第2.2节）。 (2) 已知的历史会话列表，它是主要为下一个会话（如abasket）推荐建立会话间依赖关系模型 (3) 前两者的组合，这是SBRSs的输入为下一次交互的推荐建立会话内和会话间依赖关系的模型。或下一个部分会话建议。 在特定情况下，当前会话或历史会话的输入部分可以是匿名或非匿名、有序或无序的，并具有单一或多种类型的操作。根据我们的观察，大多数现有的SBRS假设输入会话是有序的，并且具有单一类型的操作 2.6.2 输出 SBRS的目标是根据给定的会话上下文，即已知的会话信息，提出建议 （1）在下一次交互建议中，输出是备选交互（项目）的列表，按最佳匹配排序为会话中的下一次交互（项目）；（2） 在下一部分会话建议中，输出是完成当前会话的交互（项目）列表；（3）在下一次会议建议中，输出是组成下一次会议的补充互动（项目）列表 2.7 挑战针对2.5.5 对于会话提出的5个属性 提出了相对应的5个挑战 2.7.1 会话长度根据会话长度，会话大致可分为三种类型：长会话、中会话和短会话，而长会话、中会话和短会话的具体定义可能因特定数据集而异 会话 描述 挑战 长会话 一个长会话包含相对较多的交互，例如超过10次。总的来说，通过更多的互动，长时间的会话可以提供更多的上下文信息，以获得更准确的建议。然而，由于用户行为的不确定性，长会话更有可能包含与其中其他交互无关的随机交互。这会带来嘈杂的信息，从而降低建议的性能 第一个挑战是如何有效地减少不相关交互中的噪声信息。另一个挑战是如何有效地学习复杂的依赖关系以获得更好的推荐性能 中会话 中等会话通常包含中等数量的交互，例如4到9次。根据我们对电子商务行业交易记录生成的会话数据的观察，中间会话是最常见的情况。与长会话和短会话相比，中等会话不太可能包含太多不相关的交互，而它通常包含基于会话的推荐（SBR）所需的上下文信息。 即如何有效地提取相关和准确的上下文信息以获得准确的建议。 短对话 一个简短的会话包含非常有限的交互，例如，通常少于4次，导致可供推荐的信息有限。例如，在由两个交互组成的脱机匿名会话中，可以用来推荐第二个交互（项目）的唯一上下文信息是会话中的第一个交互。一种极端情况是建议会话的第一次交互。 如何在有限的背景信息下有效地提出建议 2.7.2 内部顺序 （会话内是否存在顺序） 内部顺序 描述 挑战 无序对话 无序会话包含的交互之间没有任何时间顺序，也就是说，会话中的交互发生得早或晚没有区别。因此，通常使用的序列模型不适用。与顺序依赖相比，基于共现的依赖通常相对较弱且模糊，更难学习。此外，交互之间大多数基于共现的依赖关系都是集体依赖关系，即会话中的多个上下文交互协同导致下一个交互的发生，而下一个交互更难捕获。 如何有效地学习交互之间相对较弱和模糊的依赖关系，尤其是那些集体依赖关系 有序对话 有序会话包含多个具有严格顺序的交互，它们之间通常存在强的顺序依赖关系 长顺序会话中有效学习级联的长期顺序依赖是一个挑战 灵活安排 灵活排序的会话既不是完全无序的，也不是完全有序的，即会话的某些部分是有序的，而其他部分不是 须仔细考虑和准确了解灵活排序会话中的复杂依赖关系 来自于如何有效地学习复杂和混合的依赖关系，即有序交互之间的顺序依赖关系和无序交互之间的非顺序依赖关系 2.7.3 动作类型 动作类型 描述 挑战 单一动作 单一类型的操作会话仅包括一种类型的操作，例如单击项目，因此只有一种类型的依赖关系来自同一类型的操作，这相对容易学习 很好学习 多种动作 一个多类型的动作会话包括多种类型的动作[54]，从而导致多种类型的交互。在多类型操作会话中存在复杂的依赖关系。具体来说，依赖性不仅存在于同一类型的交互上（例如，点击项目），还存在于不同类型的交互上（例如，点击和购买）。 如何有效准确地了解行动内和行动间类型的依赖关系，以获得准确的建议 2.7.4 用户信息 用户信息 描述 挑战 不匿名 非匿名会话包含与相关用户信息的非匿名交互，从而支持同一用户在不同时间生成的不同会话之间的连接。这使得了解用户的长期偏好以及其在会话中的演变成为可能 准确地了解个性化的长期偏好，而不是多个非匿名会话，这是一个相当具有挑战性的问题 匿名 在匿名会话中，由于缺少连接同一用户生成的多个会话的用户信息，因此几乎不可能为当前会话收集以前的历史会话。因此，只有来自当前会话的上下文信息才能用于建议。 利用有限的上下文信息精确捕获用户的个性化偏好以提供准确的推荐是一个挑战 2.7.5 会话数据结构 数据结构 描述 挑战 单级会话 单级会话数据集通常是一组匿名会话，其中每个会话由多个没有属性信息或历史会话信息的交互组成。在这种情况下，建议只能使用单级依赖项，即会话内的交互依赖项。因此，由于缺乏其他级别的辅助信息，基于单级别会话数据构建的SBRS很容易受到冷启动或数据稀疏问题的影响 即当只有交互依赖关系可用时，如何克服冷启动和稀疏性问题以获得准确的建议 多级会话 多级会话数据涉及至少两个级别的层次结构，即交互级别加属性级别和&#x2F;或会话级别。在这种情况下，每个级别内和不同级别之间的依赖关系都会影响后续的建议。例如，多个项目的类别（属性级别）可能会影响这些项目是否会在一个会话中一起购买（交互级别） 如何全面了解级别内和级别间的依赖关系以获得有效和准确的建议，成为构建在多级会话数据上的SBRS面临的一个关键挑战 3 SBRS方法的分类和比较分类部分 这里写了个大概 还得仔细去看 三大方法为 常规SRS方法 潜在特征方法 深度神经网络方法 3.1 常规方法 3.2 SBRSs的潜在表示方法 SBRSs的潜在表示方法首先使用浅层模型为会话内的每个交互构建低维潜在表示。学习到的信息表示对这些交互之间的依赖关系进行编码，然后将用于后续基于会话的建议 3.3 SBRSs的深度神经网络方法 4. SBRS应用程序、算法和数据集4.1 SBRS应用"},{"title":"快速傅里叶变换","date":"2022-06-03T01:52:58.905Z","url":"/2022/06/03/%E5%BF%AB%E9%80%9F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/","categories":[["undefined",""]],"content":"6&#x2F;3 日 经过两天的阅读 大致掌握了 快速傅里叶变换是个啥 在这里 致谢：   视频对于我理解不到的部分 知乎的文字部分做了充分的解释 0. 前言快速傅里叶是一个怎么样的算法 一个有效 且 漂亮的算法 1. 举个例子 有一个多项式a 和一个多项式b 在a为两阶多项式 b同样为两阶多项式 算出来的c为四阶函数 对于一个d阶的多项式 可以通过 d+1 个点 来确认曲线 线性方程 转换成 矩阵形式 当点互不相同时，对于未知数而言，其系数矩阵为范德蒙矩阵，必可逆，故有唯一解，这唯一确定了多项式系数也即唯一确定了多项式 2. 回到方程本身 对于 方程进行多项式拆分成 偶函数和奇函数 然后划分成这个鬼样子 这个鬼样子的方程 有一个特性 为什么要去x^2 呢 应为 当x取正负的时候 x^2 总是为正 这样就能少算一个值 然后 多获得 一个点 后面有个奇函数项 其值是不存在配对项的 所以 这个循环是有问题的 3. 复数的提出在这里 最创新的思维来了 有一些复数的平方之后 依旧是正负成对出现 通过 将x替换成复数范围内的值 既可以完成我们点与点成对出现的目标 欧拉方程 使用w将多项式中x替换 其时间复杂度为 因为值是对应的 所以 只需要计算一半的值就好了"},{"title":"python mysql输出","date":"2022-05-28T12:08:29.860Z","url":"/2022/05/28/python-mysql%E8%BE%93%E5%87%BA/","categories":[["undefined",""]],"content":"这个文章会有点长 从安装到mysql基础 到 python 操作 mysql OK 现在开始 mysql安装和卸载 8.0.26安装感谢： 文章没什么大问题 注意 安装配置的问题 下载安装包（没错） 安装配置（有一些很注意的东西） 解压 编写MySQL配置文件 在解压目录下新建my.ini文件 将下面文本拷贝进my.ini文件中 注意 下面的 两个地址basedir 和 datadir 这个很重要 配置环境变量 （没错） 卸载感谢： 教程完全无误 很不错 mysql 教程感谢： 这边找到了一个很好的工具网站 可以通过 json 转 mysql  Python之pymysql详解pymysql 是个python 操作数据库的工具 然后 我连了下数据库 发现能用 很不错 感谢 ： "},{"title":"会议总结","date":"2022-05-28T12:02:32.002Z","url":"/2022/05/28/%E4%BC%9A%E8%AE%AE%E6%80%BB%E7%BB%93/","categories":[["undefined",""]],"content":"5&#x2F;28日 有幸 有两个同学读博士 9&#x2F;30 - 12&#x2F;17 咱也不晓得是个好事还是个坏事 今天的帮老师 真的是 把我震惊到了 还有教育机器人 不会说招我是来干这个的吧 吐槽完成 开始码字 徐老师的总结 问题化学习 基础 发文章 新文章 掌握 创新点 思路 把握好 个人战斗 和 团队工作 top顶会 博客 论文 中文文献也得看 模型-代码复现-idea 做好文献整理（从几个方向进行总结 给文章打好标签） 项目申请 申请项目 注意申请内容 记住后期结题 毕业答辩 注意格式 两章算法 一章系统 小论文 多发小论文 代码和模型（问的多） 2-3篇 实验做得好 文章差不多一个星期能写好 工作方向突感感觉 自己就只有一年的时间 来准备自己的工作计划了 挺焦急的 现在是秋招提前批 注意胆子大 加油！"},{"title":"代码开始","date":"2022-05-15T12:46:17.091Z","url":"/2022/05/15/%E4%BB%A3%E7%A0%81%E5%BC%80%E5%A7%8B/","categories":[["undefined",""]],"content":"​ 如同我想说的那样 我希望在我写代码或者复现别人代码的开始的时候 就能使用一下这一篇文章对于我的思路的梳理 有较大的作用 感谢：  2022年5月15日 晚上 我看到了这篇文章 导入包和版本查询 可复现性在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。 显卡设置 如果只需要一张显卡 如果需要指定多张显卡，比如0，1号显卡。 也可以在命令行运行代码时设置显卡： 清除显存（在跑高显存的代码） 也可以使用在命令行重置GPU的指令 "},{"title":"caser 学习总结","date":"2022-05-14T07:38:35.023Z","url":"/2022/05/14/caser-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","categories":[["undefined",""]]},{"title":"修改意见","date":"2022-05-12T01:20:08.628Z","url":"/2022/05/12/%E4%BF%AE%E6%94%B9%E6%84%8F%E8%A7%81/","categories":[["undefined",""]],"content":"贡献那块位置 第一章knowledge point - &gt; skill （1）situation -&gt; state （2）layer -&gt; 模块 （3）模型改成原来的方法 第二章​ 相关工作（相对来说 可以乱写） 对于同一篇文章的不同方法 是可以引用多次 使用不同的方法 可以通过中文的参考文献 来找到外文的资料 个性化学习 考虑到学生因素 KT知识追踪 第三章3.1 拆散 3.2 改名字 学习者因素分析 提出 我们创新的两个点 3.3 embedding 总结 lpkt的embedding过程 总结 dif sdf learning 阶段 4.4 改名字"},{"title":"专利的撰写","date":"2022-05-07T03:12:09.358Z","url":"/2022/05/07/%E4%B8%93%E5%88%A9%E7%9A%84%E6%92%B0%E5%86%99/","categories":[["undefined",""]],"content":"昨天2022年5月6号的昆工的王老师 来到我实验室对门 我穿着拖孩 去听老师的课 收益良多 感谢 昆工王老师 1. 前言小论文改专利 十分easy （因为 小论文在发的过程中，编辑就帮你审核过了） 专利通过第五个专利号 就能区分 1 发明 2 实用 3 外观 这三种 不要被外观专利给欺骗了 哈哈哈 创新思维 + 技术手段 &#x3D; 授权专利 2. 大方向专利怎么写在问题方向为解决实际问题和解决理论上的问题 2.1 实际方向提出问题+怎么解决问题 2.2 理论上给定一个设定场景 总结现有的问题 现有的方法是如何解决问题的 3. 专利内容3.1 背景技术第一段 大背景 第二段 现有专利方法 100 - 200字 （现有技术） 3.2 发明内容针对解决的问题 技术方案 效果 对于 要保护的步骤 提出 总步骤 对于自己创新点的步骤 写出具体的细分步骤 （具体创新点进行详尽解释） 具体的效果 3.3 权利要求书可以算是具体实施方式的简化版 3.4 具体实施方式对于专利步骤进行完整的解释 越多越好 要有图表 完整"},{"title":"colab使用","date":"2022-05-07T02:49:27.418Z","url":"/2022/05/07/colab%E4%BD%BF%E7%94%A8/","categories":[["undefined",""]],"content":"首先说第一句话 这玩意真难用 不能上传压缩包 然后网上解压（可能我还没找到解决方法） 可能是我水平不太行 感谢我的师兄 李子杰师兄 给予我账号 和 验证码（每次如一日的验证码给予） 在使用的时候 主要出现了三个问题 不会运行.py 文件 不会运行google硬盘的.ipynb文件(我的解决方法相当暴力) 文件上传（文件夹上传） 问题1 文件上传和py文件运行 上传google drive 打开Google drive并登陆 在空白处右键，可在drive中上传文件\\文件夹 （时间比较长） 打开colab 新建笔记本 挂载 Google Drive 进入文件所在目录（当然这里的path 是需要改的啦） 运行目录下的.py文件（执行系统命令，需要在命令前加感叹号） ​ 问题二 不会运行google硬盘的.ipynb文件 优雅的方法来了！！！！怎么理解的呢 当然是 在跑colab跑王老师的代码的时候 自己做琢磨出来的 第一步 打开文件 第二步 打开笔记本 第三步 选你想要 "},{"title":"深度框架 4D数据格式","date":"2022-05-04T02:28:26.245Z","url":"/2022/05/04/%E6%B7%B1%E5%BA%A6%E6%A1%86%E6%9E%B6-%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/","categories":[["undefined",""]],"content":"感谢： 实验室的涛哥  0. 前言对于 Caser竖直卷积 的操作 嗯嗯嗯 在阅读论文的时候 看的不是特别明白 咱就想着 看看代码 于是乎 代码也看得不是怎么明白 最简单最简单 的就是 自己讲想要用的代码给co下来 自己跑一边 于是乎 于是乎 我发现了一个自己的一个很基础的错误 吐槽一下 可能本人功底不行 咱实在没看懂 论文这一段是啥 1. 基本概念深度学习框架，数据为4D 用NCHW或NHWC表示 N - Batch C - Channel H - Height W - Width 在pytorch 为 NCHW 在tensorflow 缺省NHWC GPU支持NCHW 2. 逻辑表达 假定N &#x3D; 2，C &#x3D; 16，H &#x3D; 5，W &#x3D; 4，那么这个4D数据，看起来是这样的： 3. 举例子 本人的竖直卷积 tensor([[-0.5610, -0.0843, 0.8302, 0.0297, 0.3825, 0.6435, 0.2758, 0.3105, -0.1820]], grad_fn&#x3D;) torch.Size([1, 9]) 9 &#x3D; 输出通道数 * 要卷积的次数 3*3 "},{"title":"argparse操作","date":"2022-05-03T12:28:12.894Z","url":"/2022/05/03/argparse%E6%93%8D%E4%BD%9C/","categories":[["undefined",""]],"content":"0. 前言对于 argparse 这个命令行小助手 其 对于深度网络的开发具有相当重要的作用 一开始 对于开发者来说 对于其描述 add_argument 定义 一眼就能看出来 这个东西需要啥 要给啥 其使用 具体三个步骤 实例化 ArgumentParser 使用add_argument函数添加参数 使用parse_args 解析参数 1. 实例化ArgumentParser（挺固定的） 2. 添加参数举一下本人的例子 在add_argument 中间有三个参数 参数一： 看作变量名 前面得加 – 参数二： 变量数据类型 参数三： 缺省值（default） 目前 俺觉得俺能学这三个参数就好了 3. 解析参数最后会被解析成 神奇操作 这里存在 我叫做变量的继承 挺有意思的 被封装成namespace对象 不能在使用add_argument 函数对其 变量进行添加"},{"title":"对于embedding操作 新的理解","date":"2022-05-01T13:15:55.905Z","url":"/2022/05/01/%E5%AF%B9%E4%BA%8Eembedding%E6%93%8D%E4%BD%9C-%E6%96%B0%E7%9A%84%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"2022年5月1日21点20分 先放一张图片 high！ 一下 在这里终于能理解到 为什么pytorch 的好用 和我对embedding 操作 将近一个月的误解！！ 我真的是high到不行了 0. 前言 （很重要）​ 对于一位5月1号 在看着风骚律师 在看着一个名字叫做caser推荐论文的同学。这里面论文里面的一句话，深刻的激发了我。即 俺终于知道如何编码序列推荐数据了！ ​ 其实 在这里面最主要的一个体会是！ ​ 勇敢 即 主动迈出那一步！！ ​ 说太多了 ​ 先看看论文里面的那句话！ ​ 这里的set和universe 用的太美了 brilliant！ ​ 下面这句话非常重要 ​ 用户的交互序列其实就是一个物品序列的组合！！！ 1. 自己以前的误解1.1 误解1本人一直以为 embedding 一定得是one-hot 转换成 embedding-vector 即 我认为简单数字是不能embedding的 大错特错！！！ 数字为什么就不能表示物品的特征呢！ 数字为什么就不能表示物品的特征呢！ 数字为什么就不能表示物品的特征呢！ 数字 简单，那么可爱 就应该被embedding 我觉得我这里的误解 应该是犯了教条主义的错误！！！ 哈哈哈 1.2 误解2即 我认为的交互序列是存在点击和不点击这样子的其他属性的 正好最近阅读了 知识追踪 反而没起到正作用 反而放这个让我的误解加深了！ 论文里面的一句话！ 用户序列 中的元素 即为物品序列的子元素 1.3 误解3我觉得这里 应该是犯了 实践-理论-实践 这个基本道路的错误 为什么 你在没有实践后 就贸然的翻阅理论呢！ 可笑!! 我这一周 基本一直在思考 如何找人家 是如何讲数据集 处理成 序列推荐 模型的数据 然后 一直在抱怨 为什么 我的数据集 相对 cv 的 数据 是多难处理 最后 在读caser 这篇文章的时候 才发现自己是多么的沙雕！ 本来就是一句话的事情 自己想的太复杂了 ​ 心得1 多看论文 说不定有个人能说的很明白 一句话的事儿 心得2 多实践 只有多试试 才能 发现找到新的道路 2. 上代码 观察两个output 其 元素是对应的 其元素是对应的 对应的 好的 我说完了！"},{"title":"PyTorch 张量操作","date":"2022-04-26T12:53:22.333Z","url":"/2022/04/26/pytorch-%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C/","categories":[["undefined",""]],"content":"感谢：  0. 前言​ 在阅读别人编写的源码的过程中，对于他人能够灵活操作torch向量 感觉十分的神奇，于是乎俺也来写一个文档作为一个手册的使用不就好了嘛。（先统计） 1. 基本数据形式读Caser论文有感！其对于数据的操控 我作为一个单纯的研究生感到大受震撼 4d数据结构深度学习框架，数据为4D 用NCHW或NHWC表示 N - Batch C - Channel H - Height W - Width 在pytorch 为 NCHW 在tensorflow 缺省NHWC GPU支持NCHW tensor.size [N,C,H,W] 1.1 有些数据 这样子的[N,H,W]当然 在我推荐领域 channel 也不存在rgb 这么丰富的颜色 直接看代码 给channel 设置成1 就好 1.2 还有些数据 是这个亚子 [w,h]那就这个亚子吧 因为 加batch（一般不为1） 所以 我也没啥好的方法来着 1.3 有些数据 是最后这个样子 [N,H] [N,W]那这个样子呢 是存在补救空间的 嗯嗯！ 2. 我看到的操作张量的数据类型 PyTorch有9种CPU张量类型和9种GPU张量类型 张量基本信息 数据类型转换 torch.div 可以看出来 这玩意是要对其的 squeeze()当其中没有参数的时候，其缺省值是将里面维度为1的进行压缩 感谢： 压缩 原来是压缩 维度为1的操作啊 首先 先说明白 维度一维 线 二维 面 三维 正方体 在程序中 就是 0，1，2 来表示上面的三维 本人自己的操作 unsqueeze()我的很直接的说法就是 这玩意是拿来扩维的 这个函数必须在其中填入dim&#x3D;？这个参数 如果单纯从数字角度来考虑这个事情，那就很简单 例如 torch.cat我觉得我在代码中的注释 很好的解释了这个问题 torch.view感谢：  网上的看不懂啊 网上的看不懂啊 网上的看不懂啊 可能是我水平差了 我用我的大白话来讲明白这个事儿 ​ 日常我们输入卷积层的数据形式 为 4D，view这个小妖精呢，我的感觉就像是捏泥巴。我脑子响起了东北玩神曲 给大家看看有意思的 注意看两次的size 打印 我对于这个玩意的感觉就是将数字打散 然后重新组合起来 十分有趣 torch.mm torch.bmm torch.baddbmm感谢：  "},{"title":"git 沙雕使用","date":"2022-04-26T12:26:08.345Z","url":"/2022/04/26/git-%E6%B2%99%E9%9B%95%E4%BD%BF%E7%94%A8/","categories":[["undefined",""]],"content":"我作为个人开发者，对于git的使用了解个大概就好 首先 先拉一张图 在git的分级中存在 这样的工作目录 这个暂存区 言如其名 我也不晓得是啥 本地仓库 就是文件发送的最后一个位置 远程仓库 对于 我们来说就是github了 1. 本人可能进行的操作 够了！"},{"title":"毕业论文思路","date":"2022-04-23T06:40:11.842Z","url":"/2022/04/23/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E6%80%9D%E8%B7%AF/","categories":[["undefined",""]],"content":"前言做一个能扫描视频内容的搜索框  我能思考到的技术点 ocr扫描 视频内容对其 模糊搜索 关键帧抽取 时间 ：2022&#x2F;4&#x2F;23 功能 生成视频目录 视频信息ocr读取 "},{"title":"顺序推荐模型综述","date":"2022-04-21T11:32:08.954Z","url":"/2022/04/21/%E9%A1%BA%E5%BA%8F%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"感谢： 名字：Sequential Recommender Systems: Challenges, Progress and Prospects ​ 首先：在阅读这一篇文献的时候，这篇综述不仅仅像我看过的普通的文章的整理，它对于整个问题的分析，感想后，对于这个问题目前的解决的方法，对于行业目前的状况进行了把握，虽然这一篇19年的文献，本人更希望这篇文章是21年的，毕竟能给予更多的指导。 ​ 文章提出了目前序列推荐模型5个困难，对于模型进行了分类，且对于开放方向进行了整理。 ​ 本次我最明显的收益是 即序列中的短序列为会话（） ​ 哦吼 多说无益 干活！ 0. 前言啥子是序列推荐模型（SRS）咧？ 大师或者小学二年级学生会这么回答 input: u,i交互 S output:一个排名靠前的候选列表 R 过程：对复杂顺序关系建模 公式呢？ R &#x3D; argmax f(S) S &#x3D; {i1, i2, …, i|S|} ij &#x3D;&lt; u, a, v &gt; 元素 解释 u 用户 a 行为 v 物品 1. 推荐模型的挑战按作者总结，一共有5个挑战。 1.1 长序列的处理 序列的高阶依赖 长期的顺序依赖关系 挑战这一个工程非常有限，其主要的方法是对应上面的两个小问题进行分模型处理 1.2 以灵活的顺序处理用户项目交互序列​ 并非所有相邻的交互都在序列中顺序相关，eg 购物序列S2&#x3D;{牛奶、黄油、面粉}中，先买牛奶和黄油并不重要，但是面粉的顺序取决于它们的组合。 ​ 看到上面的例子，可以得到，对于灵活顺序的序列处理，做好捕捉集体的顺序依赖关系，而不是逐点依赖关系。如何在柔性顺序的假设下捕获集体序列依赖成为SRS中处理柔性顺序序列的关键挑战。 ​ 目前的工作相对较少，在深度学习领域，CNN在其体现出不同区域的之间的局部和全局依赖关系，来应对这个工作。（之前小看了这篇CNN文章 等下看） ​ 题目：Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding ​ 模型名字： CASER 1.3 处理噪声数据​ 噪声的定义：在序列中，与交互预测产生干扰，在用户项交互序列中，一些历史交互是弱相关甚至不相关。（不晓得兴趣漂移算不算噪声） ​ 一篇自己看不懂的文章，但是完美的解决了这个问题 ​ 简单的说一下这个文章的意思吧，其通过分析一个序列中 非常重要的序列项和一些无关紧要的序列项，通过替换非常重要的 建立负样本。替换无关紧要的建议正样本。通过对比学习来实现这篇。（文章的具体内容还得看） 其难点与新颖点 序列项的分类 对比学习的使用 ​ 题目：CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation ​ 模型名：题目里面有 1.4 处理异构关系的用户交互​ 长期顺序依赖关系与短期顺序依赖关系大不相同，它们不能以相同的方式建模 ​ 混合模型是其解决问题的方式 1.5 具有层次结构的交互序列​ 在这里 我终于理解 会话对于推荐模型的意义 ​ 与用户项交互序列相关联的层次结构主要有两种： 元数据和用户项交互之间的层次结构。 子序列和用户项交互之间的层次结构。 ​ SRSs的另一个关键挑战是如何将嵌入这两种层次结构中的层次依赖性合并到顺序依赖学习中，以生成更准确的顺序推荐。 2 常用模型这里分为常用的和高级的 2.1 基础的 RNN 在这里主要写问题 任何相邻的交互必须相互依赖 只能捕获点相关 CNN 解决了RNN的问题 其卷积核的方式 能以相关的序列附近的关系 没有很强的顺序假设 无法捕获长期依赖 题目： A Simple Convolutional Generative Network for Next Item Recommendation 模型名字：NextitNet GNN 图点线结构 是最近工作的方向，相关工作目前较少 2.2 高级 attention 应用于浅层网络，处理带有噪声的交互序列 memory 内存网络，提高模型的表达能力 混合模型 解决短序列和长序列的组合 2.3 开放方向上下文感知 外界环境对于选择的影响 整合买卖交互（买卖之间的聊天） 跨域 不好理解 给个例子 在现实世界中，用户在特定时间段内购买的物品通常来自多个域，而不是一个域。本质上，来自不同领域的项目之间存在一些顺序依赖关系，例如在购买汽车后购买汽车保险。这种跨域顺序依赖在大多数SRS中被忽略"},{"title":"cin的沙雕理解","date":"2022-04-16T11:53:20.269Z","url":"/2022/04/16/cin%E7%9A%84%E6%B2%99%E9%9B%95%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"0. 前言感谢：     在主流的FM与DNN的结合方法中，这个明显是并行的结构 大家晓得xdeepFM 这个东西嘛 这个东西与deepFM 的区别是 根本没有什么相似性 不信 看看网络结构图 这个是deepFM 这个是大名鼎鼎的xdeepFM ​ 个人的瞎吉儿理解：在结构上，这两个玩意如果实在工程使用上 我肯定会选择deepFM。xdeepFM在网络结构上就存在这一些复杂性，在deepFM的基础上增加了linear层和cin这个不晓得是啥的玩意。在理解中 这个应该是我们xdeepFM 想要替换在deepFM 中 FM的部分 1. 模型的结构难点和自己的思考既然说这个模型我都看了两天 自然有其难点的地方 cin 的操作啊 还有其为什么强的地方 1.1 难以理解的cin讲明白一个东西首先先讲明输入输出 输入：field后的embedding层向量 输出：sum pooling后concat一个向量 1.2 介绍角色说明啊 m 是 filed的个数 D 为 embedd维度 Hk 为第k层特征向量的数量 Xk 为第k层的隐向量 冤大头 首先说明白冤大头的样子 embedd 1 embedd 2 ******** embedd m 这个东西会反复使用（的确是冤大头） 假如说 我有一个这样的隐藏层 我这边感觉第一个隐藏层肯定是随机生成的 其与冤大头 拥有一个相同长度的D边 与隐藏层进行外积操作 于是可以得到一个 长方形的过渡张量 其作用是求出下一个隐藏层 1.3 下一个隐藏层求法本人将用最简单的话来说明白下一个图的意思 欧克 图里面最直接就能看到一个躺着的张量 ，上面能看到，一个平铺的向量 其宽为D 长度为Hk+1 先从最基础的绿色的点开始 每一层都会得到一个绿色的点，张量的高为D 所以得到隐向量的宽为D 隐向量的长度为Hk+1,这个数据的设置 没有任何根据 属于随心所欲 绿色点的计算 会使用到一整个面的橙色的点 1.4 张量的生成我们获得了 隐向量 将其与 冤大头做外积 既可以获得下一层的过渡张量 然后 根据1.3的做法 生成对应的隐藏层 1.5 （1.3，1.4）循环噢噢噢噢 终于有k个隐藏层了呢 1.6 汇总 每个隐藏层为，对于第层，将所有的特征映射进行一个池化操作（sum pooling）【例如对上图Feature map 1向量进行一个累加】： 因此便得到一个池化向量对于第隐藏层。 最后在对于所有的隐藏层的池化向量进行一个拼接： 2. cin的性能 空间复杂度 主要的学习参数就是，经过上述分析，第层的共有个参数。假设包括CIN经过一个二元分类任务，那么CIN总共的学习参数为 时间复杂度 计算的时间复杂度为，那么对于层CIN的总时间复杂度为 优点 （1）交互是向量的交互，不是位（元素）级别（bit-wise）的交互； （2）高阶特征交互是显示的； （3）网络的复杂性不会随着交互程度的增加而呈指数增长； 3. 个人的理解对于xDeepFM模型，我们假设CIN的深度与特征映射的数量都为1，则「xDeepFM就相当于DeepFM的一个泛化」。当进一步删除DNN部分，并且对于特征映射使用一个sum filter，那xDeepFM就将为一个传统的FM模型。【联想之前的分解】 "},{"title":"小看的FM","date":"2022-04-13T07:04:51.808Z","url":"/2022/04/13/%E5%B0%8F%E7%9C%8B%E7%9A%84FM/","categories":[["undefined",""]],"content":"感谢：    -1 大前提这个是无奈之举 发现 能写的太多了 强调一下 one-hot 编码问题 首先展示： 里面有 像click的分类值 有 country,day,ad_type则是对应的特征。对于这种categorical特征，一般都是进行one-hot编码处理。 在代码中是这样处理的 画风一变！！ 因为是categorical特征，所以经过one-hot编码以后，不可避免的样本的数据就变得很稀疏。举个非常简单的例子，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。由此可见，数据的稀疏性，是我们在实际应用场景中面临的一个非常常见的挑战与问题。 one-hot编码带来的另一个问题是特征空间变大。同样以上面淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。 0. 前提啊啊啊~~ 咱这一周2022年4月13日这个时间呐 发现！！ FM 很重要 其理论的推理。现在看起来十分的重要 在工业界，这个模型 简直是yyds 其在CTR预估和推荐领域广泛使用 特征组合对于推荐排序是非常非常重要的，而FM这个思路已经很简洁优雅地体现了这个思想了（主要是二阶特征组合） 牛逼的点： 在embedding前夕 提出了类embedding的处理方式 FM对于每个特征，学习一个大小为k的一维向量，于是，两个特征 和 的特征组合的权重值，通过特征对应的向量 和 的内积 来表示。这本质上是在对特征进行embedding化表征，和目前非常常见的各种实体embedding本质思想是一脉相承的，但是很明显在FM这么做的年代（2010年），还没有现在能看到的各种眼花缭乱的embedding的形式与概念。所以FM作为特征embedding，可以看作当前深度学习里各种embedding方法的老前辈。当然，FM这种模式有它的前辈模型吗？有，等会会谈。其实，和目前的各种深度DNN排序模型比，它仅仅是少了2层或者3层MLP隐层，用来直接对多阶特征非线性组合建模而已，其它方面基本相同。 1. FM 前老弟（看起来就很复杂）这个式子吧 从此公式可以看出组合特征一共有n(n-1)&#x2F;2个，如果特征n上百个，组合特征上万个，就是任意两个wij相互独立，样本数据很稀疏，xixj为非零的项会非常的少，导致训练样本的不足，很容易导致参数 wij 不准确，最终将严重影响模型的性能和稳定性，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。 这个东西 在未来 将为一直成为推荐FM的垫脚石（敲砖引玉） 2. FM 老弟在 工业界 还在使用 大多工业推荐排序系统采取LR这种“线性模型+人工特征组合引入非线性”的模式。因为LR模型具有简单方便易解释容易上规模等诸多好处，所以目前仍然有不少实际系统仍然采取这种模式。但是，LR模型最大的缺陷就是人工特征工程，耗时费力费人力资源，那么能否将特征组合的能力体现在模型层面呢？ 使用了特征的隐向量 作为特征的相对应权重 上面的wij 使用 特征隐向量（辅助向量）的乘积 本人大白话讲一遍，啊啊啊 上面的式子太暴力，脑子思考一下就发现权重项太多，我在上面的这篇文献中发现这么一句：**对于任何正定实矩阵 只要k足够大，都存在k维向量组成的矩阵 使得 ** 在这个公式下 这个V啊 就是辅助向量。 这个k相对于n来说 可太小了 所以 就从目前来说 权重系数 降低到了 3. 时间复杂度的减小多图 警惕！ 一个相对复炸的式子 3.1 step1 3.2 step2 3.3 step3 第三步转换不是太直观，可能需要简单推导一下，很多人可能会卡在这一步，所以这里解释解释。 其实吧，如果把k维特征向量内积求和公式抽到最外边后，公式就转成了上图这个公式了（不考虑最外边k维求和过程的情况下）。它有两层循环，内循环其实就是指定某个特征的第f位（这个f是由最外层那个k指定的）后，和其它任意特征对应向量的第f位值相乘求和；而外循环则是遍历每个的第f位做循环求和。这样就完成了指定某个特征位f后的特征组合计算过程。最外层的k维循环则依此轮循第f位，于是就算完了步骤三的特征组合 3.4 step4对上一页公式图片展示过程用公式方式，再一次改写（参考上图），其实就是两次提取公共因子而已，这下应该明白了吧？要是还不明白，那您的诊断结果是数学公式帕金森晚期，跟我一个毛病，咱俩病友同病相怜，我也没辙了。 "},{"title":"Capsule 网络","date":"2022-04-10T14:19:13.807Z","url":"/2022/04/10/Capsule%20%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C/","categories":[["undefined",""]],"content":"0. 简单总结感谢：  为了改善CNN对旋转不具备不变性，学习不到3D空间信息和CNN只关注要检测的目标是否存在，而不关注这些组件之间的位置和相对的空间关系。 CNN 自己的处理 虽然max pooling在很多任务上提高了原始CNN的准确率，但是我们也可以看到max pooling丢失了很多有价值的信息，并没有很好地理解内容 Capsule尝试去解决这些问题 （优势） Capsule可以学习到物体之间的位置关系，例如它可以学习到眉毛下面是眼睛，鼻子下面是嘴唇，可以减轻前面的目标组件乱序问题 Capsule可以对3D空间的关系进行明确建模，capsule可以学习到上面和下面的图片是同一个类别，只是视图的角度不一样。Capsule可以更好地在神经网络的内部知识表达中建立层次关系。 在训练的时候 能使用相对较少的数据集，但相对来说 其训练时间较CNN变得更长 1. capsule的结构​ capsule 是向量，其可以理解为object的某个类别，其模长表示某个entity存在的概率，其方向表示某个entity属性 其计算的方式 为以下四步： 对输入向量做乘法，其中 和 分别来自与前面的 capsule 的输出，在单个 capsule 内部，对 和 分别乘上 和 得到了 新的 和 。 对输入向量进行标量加权，令与相乘，与相乘，其中和均为标量，且。 对得到向量求和，得到。 向量到向量的非线性化，将得到的结果向量 进行转换，即通过函数 得到结果 ，作为这个capsule 的输出，且这个结果 可以作为下一个 capsule 的输入 2. 细节（训练方式）动态寻路算法 鬼都看不懂下面这个 直观理解 其中两个高层胶囊的输出用紫色向量 表示，橙色向量表示接受自某个低层胶囊的输入，其他黑色向量表示接受其他低层胶囊的输入。左边的紫色输出 和橙色输入 指向相反的方向，所以它们并不相似，这意味着它们点积是负数，更新路由系数的时候将会减少。右边的紫色输出 和橙色输入 指向相同方向，它们是相似的，因此更新参数的时候路由系数 会增加。在所有高层胶囊及其所有输入上重复应用该过程，得到一个路由参数集合，达到来自低层胶囊的输出和高层胶囊输出的最佳匹配。 "},{"title":"transformer 详解！（写的像人话一点）","date":"2022-04-10T12:50:30.634Z","url":"/2022/04/10/transformer%20%E8%AF%A6%E8%A7%A3%EF%BC%81%EF%BC%88%E5%86%99%E7%9A%84%E5%83%8F%E4%BA%BA%E8%AF%9D%E4%B8%80%E7%82%B9%EF%BC%89/","categories":[["undefined",""]],"content":"0. 奠基大佬 Bahdanau Attention &amp; Luong Attention 1. 自注意力和多头 Self Attention &amp; Multi-head Attention 为什么自注意力呢？ 相对于 RNN，考虑长距离依赖，还要可以并行！ constant path length &amp; variable-sized perceptive field：任意两个位置（特指远距离）的关联不再需要通过 Hierarchical perceptive field 的方式，它的 perceptive field 是整个句子，所以任意两个位置建立关联是常数时间内的。 parallelize : 没有了递归的限制，就像 CNN 一样可以在每一层内实现并行。 1.1 宏观角度看自注意力机制​ 随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。 ​ 如果你熟悉RNN（循环神经网络），回忆一下它是如何维持隐藏层的。RNN会将它已经处理过的前面的所有单词&#x2F;向量的表示与它正在处理的当前单词&#x2F;向量结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。 1.2 微观角度在NLP 中 ，k &#x3D; v 具体流程（本人的脑子思考）： 词向量（为x1） 生成qkv（词向量乘对应的权重矩阵 wq wk wv） qk计算注意力分数（scaled-dot product） 注意力分数softmax （alignment function） 通过注意力分数softmax 计算最后的值向量 求和 （context vector） 1.3 代码实践softmax 的 dim &#x3D; 1 2. 注意力简单解释 自己发现 softmax dim 范围其实是dim &#x3D; 1"},{"title":"attention机制 简单理解（废弃） 说实话看不太懂 太抽象","date":"2022-04-10T07:05:28.523Z","url":"/2022/04/10/attention%20%E6%9C%BA%E5%88%B6/","categories":[["undefined",""]],"content":"在反复回卷的attention中，本人感觉心里憔悴 一定得总结一个俺能看懂的文章 最近发现了这样的一篇 感谢：    阿里妹导读：曾被 paper 中各种各样的 Attention 搞得晕晕乎乎，尽管零零散散地整理过一些关于Attention 的笔记，重点和线索依然比较凌乱。今天，阿里巴巴工程师楠易，将 Attention 的知识系统性地梳理、回顾、总结，不求深刻，但求浅显，希望能帮助对 Attention 有疑惑的同学。 0. 什么是attention Attention（注意力）机制如果浅层的理解，跟他的名字非常匹配。他的核心逻辑就是「从关注全部到关注重点」 1. attention 分类涉及所有的 attention 都继承于这个抽象类。这里我写了两个抽象类，一个叫 alignment-based，一个叫 memroy-based。 1.1 alignment-based 模型 c 为 context y1 y2 —–yn 为输入 input 输出为z 1.2 拆分 attention model分为三部曲 score function ：度量环境变量与当前输入向量的相似性；在当前环境下，应该关注哪些信息 ​ alignment function：计算attention weight （权重） 通常使用softmax进行归一化 ​ generate context vector function : 根据 attention weight 得到输出向量 ​ 在整体视角下，就像下图这个样子： ​ 1.3 memory-based 模型 长得很像transformer​ 另一种视角是 QKV模型，假设输入为 q，Memory 中以（k，v）形式存储需要的上下文。感觉在 Q&amp;A 任务中，这种设置比较合理，transformer 是采用的这种建模方式。k 是 question，v 是 answer，q 是新来的 question，看看历史 memory 中 q 和哪个 k 更相似，然后依葫芦画瓢，根据相似 k 对应的 v，合成当前 question 的 answer 1.4 建模方式三步 address memory （score function）： 在memory找相似的东西 ​ normalize（alignment function） ： ​ read content（ gen context vector function ）： ​ 2. attention 细节在attention机制中，其建模方式主要就是以下的三类 按人话说 找相关 度量环境向量与当前输入向量的相似性；找到当前环境下，应该 focus 哪些输入信息（ score-function ） 算权重 计算 attention weight，通常都使用 softmax 进行归一化 （ alignment function ） 出结果 根据 attention weight 得到输出向量 （ generate context vector function ） 2.1 score function 的区别score function 在本质上是度量两个向量的相似度。找出相关的部分 两个向量在一个空间 使用 dot 点乘方式（或者 scaled dot product，scaled 背后的原因是为了减小数值，softmax 的梯度大一些，学得更快一些），简单好使。 不在同一个空间 需要一些变换（在一个空间也可以变换），additive 对输入分别进行线性变换后然后相加，multiplicative 是直接通过矩阵乘法来变换 2.2 alignment function 区别在 soft attention 中，又划分了 global&#x2F;local attention 。 global attention 是所有输入向量作为加权集合，使用 softmax 作为 alignment function，local 是部分输入向量才能进入这个池子。 local的目的 背后逻辑是要减小噪音，进一步缩小重点关注区域。 如何缩小关注区域 local-m 基于的假设生硬简单，就直接 pass了。local-p 有一个预估操作，预计当前时刻应该关注输入序列（总长度为S）的什么位置 pt（引入了两个参数向量，vp，wp），然后在 alignment function 中做了一点儿调整，在 softmax 算出来的attention wieght 的基础上，加了一个以 pt 为中心的高斯分布来调整 alignment 的结果。 在应用中 发现 从global&#x2F;local 视角的分类来看，更常用的依然还是 global attention，因为复杂化的local attention 带来的效果增益感觉并不大 2.3 generate context vector functionsoft&#x2F;hard attention 最直观的一种理解是，hard attention 是一个随机采样，采样集合是输入向量的集合，采样的概率分布是alignment function 产出的 attention weight。因此，hard attention 的输出是某一个特定的输入向量。soft attention 是一个带权求和的过程，求和集合是输入向量的集合，对应权重是 alignment function 产出的 attention weight。hard &#x2F; soft attention 中，soft attention 是更常用的（后文提及的所有 attention 都在这个范畴），因为它可导，可直接嵌入到模型中进行训练，hard attention 文中 suggests a Monte Carlo based sampling approximation of gradient。 "},{"title":"Amazon数据集的处理 生动（没写完）","date":"2022-04-08T01:38:52.891Z","url":"/2022/04/08/Amazon%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"  数据集： 一个数据集3g 有点大 这个数据集按 作者的说法是处理好了的 先于之前一篇文章的写法，这次打算按照之前criteo数据集的编写方法 在写一个 算是数据集的第二篇 一共三篇 0. 观察数据（有一点重要） 类别有 label user itemID cateID 重要的是 后面的 hist_item_list 和 hist_cate_list 这两个列表 可以看到这两个列表使用了 | 作为分隔符 对于后两个列表的处理 将是比较重要的特点 可以观察到的数值特征 只有标签label一个 1. 读取数据"},{"title":"各种慢","date":"2022-04-06T13:34:45.459Z","url":"/2022/04/06/%E5%90%84%E7%A7%8D%E6%85%A2/","categories":[["undefined",""]],"content":"一个常用的各种慢的解决复制帖 pip慢 -i  conda 慢添加清华镜像源，代码如下所示： "},{"title":"criteo数据集的处理 生动","date":"2022-04-06T12:25:47.026Z","url":"/2022/04/06/criteo%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"之前 写过一篇 太烂了 在重新写一篇生动的 数据预处理与特征工程： 目前项目涉及三个数据集（criteo，amazon，movielen） 其中 criteo数据集肯定是使用的最多的啦 这里 对于数据的梳理 （对于数据的处理 我觉得也较通用） 特地 整理一个文档 0. 观察数据（其实很重要） 数据 有0 1 2 这样的离散数据 和 一些68fd1e64 80e26c9b fb936136 7b4723c4 不晓得是啥的数据 感觉能确定的是 第一列的数据 是标签数据 欧克 总结数据如下 有标签 离散 和 不晓得是啥的数据（先确定为连续数据吧） 按照习惯 其target一般特征后面 1. 读取数据 1.1 标准数据查看三件套 1.2 数据质量分析 查看缺失值 异常值分析 1.3 数据分类 2. 缺失值处理针对缺失值 的 离散 和连续缺失的值 进行缺失值处理 （缺失一般是异常值） 一般是四步操作 查看变量类别 查看变量缺失值情况 对于缺失值进行标注 查看标注完缺失值情况 3. sklearn.preprocessing数据预处理这里搬运一下 分箱的目的 离散变量便于特征的增加和减少，便于模型快速迭代 稀疏向量内积乘法更快，计算结果便于存储，容易扩展 离散化后的特征对异常数据有很强的鲁棒性，例如，连续异常值5000可能对模型影响很大，但如果分箱后，模型影响很小 为模型引入非线性，提升模型表达能力，加大拟合 模型更加稳定，不会因为各别数据增加而影响模型精度 简化模型，防止模型过拟合 3.1 处理连续型特征在处理连续型特征 有两个方法 二值化 与 分段 二值化 二值化使用的类是 sklearn.preprocessing.Binarizer 根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射到1。二值化是对文本计数数据的常见操作，分析人员可以决定仅考虑某种现象的存在与否。 分段 分段使用的类 preprocessing.KBinsDiscretizer，相对麻烦点，给出下面的参数列表 3.2 处理分类型特征在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，除了专用来处理文字的算法，其他算法在fit的时候全部要求输入数组或矩阵，也不能够导入文字型数据（其实手写决策树和普斯贝叶斯可以处理文字，但是sklearn中规定必须导入数值型）。 然而在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是[“小学”，“初中”，“高中”，“大学”]，付费方式可能包含[“支付宝”，“现金”，“微信”]等等。在这种情况下，为了让数据适应算法和库，我们必须将数据进行编码，即是说，将文字型数据转换为数值型 文字型数据 转换成数字 处理后的效果 [][qzetZn.md.png] 4. 划分标签和特征 5. 划分训练，验证，测试集 5.1 数据划分"},{"title":"从din到dien 推荐模型（文献部分）","date":"2022-04-04T06:17:53.421Z","url":"/2022/04/04/%E4%BB%8Edin%E5%88%B0dien%20%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%EF%BC%88%E6%96%87%E7%8C%AE%E9%83%A8%E5%88%86%EF%BC%89/","categories":[["undefined",""]],"content":"感谢：     ​ 最近比较迷茫，看了看自己的论文阅读表，这两篇阿里巴巴的文献映入眼帘，作为工业界的老大。其思维比较新颖。且较当时比较的模型有较大百分比的提升。CTR预测， 所谓的CTR，就是点击率Click Through Rate，而CTR预估算法在广告系统中起着至关重要的作用。从这两篇文章中可以看到阿里妈妈在深度学习方向上的探索，也可以窥视到阿里妈妈的CTR算法的发展脉络。 0. 一切从特征选取开始 主要有4个特征组，如下图所示， 1）用户画像特征， 2）用户行为特征，即用户点击过的商品 3）待曝光的广告，广告其实也是商品，后文中我们统称为candidate 4）上下文特征。 1. 模型的鼻祖 （embedding + mlp） 如果说有10个商品的embedding，embedding的维度为16。在业界，pooling的处理方式就两个。 sum pooling 对应的维度数字求和 mean pooling 对应的维度数字求平均 1.1 问题​ 在电商这个场景中，通常用户的兴趣具有多样性，可能在一段时间内点击过衣服，电子产品，鞋子等。而对于不同的candidate来说，浏览过的相关商品对于预测帮助更大，不相关的商品对于ctr预估可能并不起作用，例如用户看过的衣服，鞋子对于iphone的预测并没有帮助 2. DIN​ 为了解决上述的问题，既然提到了相关，那肯定得考虑到&#x3D;&#x3D;注意力机制&#x3D;&#x3D;了啦 ​ 解决思路是： 在pooling的时候，与candidate相关的商品权重大一些，与candidate不相关的商品权重小一些，这是一种Attention的思想。将candidate与点击序列中的每个商品发生交互来计算attention分数。具体计算方法如图3中右上角的小网络所示，输入包括商品和candidate的embedding向量，以及两者的外积。对于不同的candidate，得到的用户表示向量也不同，具有更大的灵活性。 ​ 模型基础上 提出了Activation Unit单元 来提取商品与目标广告之间的相关性。 其中activation unit的输入包括两个部分，一个是原始的用户行为embedding向量、广告embedding向量；另外一个是两者Embedding向量经过外积计算后得到的向量，文章指出这种方式有利于relevance modeling。 ​ 2.1 attention归一化处理​ 一般来说，做attention的时候，需要对所有的分数通过softmax做归一化，这样做有两个好处，一是保证权重非负，二是保证权重之和为1。但是在DIN的论文中强调，不对点击序列的attention分数做归一化，直接将分数与对应商品的embedding向量做加权和，目的在于保留用户的兴趣强度。例如，用户的点击序列中90%是衣服，10%是电子产品，有一件T恤和一部手机需要预测CTR，那么T恤会激活大部分的用户行为，使得根据T恤计算出来的用户行为向量在数值上更大，相对手机而言。 2.2 DIN的创新点​ DIN的论文中还提出了两个小的改进点。一个是对L2正则化的改进，在进行SGD优化的时候，每个mini-batch都只会输入部分训练数据，反向传播只针对部分非零特征参数进行训练，添加上L2之后，需要对整个网络的参数包括所有特征的embedding向量进行训练，这个计算量非常大且不可接受。论文中提出，在每个mini-batch中只对该batch的特征embedding参数进行L2正则化。第二个是提出了一个激活函数Dice。对于Relu或者PRelu来说，rectified point(梯度发生变化的点)都在0值，Dice对每个特征以mini-batch为单位计算均值和方差，然后将rectified point调整到均值位置。 3. DIENDIN在捕捉连续行为之间的依赖关系很弱，行为-》利益，其中隐性利益很难通过行为充分反映 DIEN，全称是Deep Interest Evolution Network，即用户兴趣进化网络。这个算法中用两层架构来抽取和使用用户兴趣特征： 兴趣抽取层Interest Extractor Layer: 从用户行为序列中提取信息 兴趣进化层Interest Evolving Layer: 从用户行为序列中找到目标相关的兴趣，对其进行建模 3.1 兴趣抽取层（这里使用的是GRU）​ 在广告与商品之间并不是单纯的商品对应的关系（即我喜欢的是这个） ​ 兴趣抽取层Interest Extractor Layer的主要目标是从embedding数据中提取出interest。但一个用户在某一时间的interest不仅与当前的behavior有关，也与之前的behavior相关，所以作者们使用GRU单元来提取interest。GRU单元的表达式如下。 其中，σ是sigmoid操作，而◦是内积操作。 作者的辅助测试 ​ 作者设计了一个二分类模型来计算兴趣抽取的准确性，我们将用户下一时刻真实的行为e(t+1)作为正例，负采样得到的行为作为负例e(t+1)’，分别与抽取出的兴趣h(t)结合输入到设计的辅助网络中，得到预测结果，并通过logloss计算一个辅助的损失 3.2 兴趣进化层 （带有注意力的GRU） interest在变化过程中遵循如下规律：1）interest drift：用户在某一段时间的interest会有一定的集中性。比如用户可能在一段时间内不断买书，在另一段时间内不断买衣服。2）interest individual：一种interest有自己的发展趋势，不同种类的interest之间很少相互影响，例如买书和买衣服的interest基本互不相关。 3.2.1 attention机制兴趣抽取层获得的隐向量，ht 与 被embedding后的广告向量 产生内积后softmax取相关参数 3.2.2 attention方式 GRU with attentional input (AIGRU) 这种方式将attention直接作用于输入，无需修改GRU的结构： ​ Attention based GRU(AGRU) 这种方式需要修改GRU的结构，此时hidden state的输出变为： ​ GRU with attentional update gate (AUGRU) 这种方式需要修改GRU的结构，此时hidden state的输出变为: ​ "},{"title":"RNN模型","date":"2022-04-03T13:18:49.397Z","url":"/2022/04/03/RNN%E7%B1%BB%E6%A8%A1%E5%9E%8B/","categories":[["undefined",""]],"content":"感谢： 0. RNN 先简单介绍一下一般的RNN。 这里： !() 为当前状态下数据的输入， 表示接收到的上一个节点的输入。 为当前节点状态下的输出，而 为传递到下一个节点的输出。 通过上图的公式可以看到，输出 h’ 与 x 和 h 的值都相关。 而 y 则常常使用 h’ 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。 对这里的y如何通过 h’ 计算得到往往看具体模型的使用方式。 通过序列形式的输入，我们能够得到如下形式的RNN。 1. LSTM 长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。 相比RNN只有一个传递状态 ，LSTM有两个传输状态，一个 （cell state），和一个 （hidden state）。（Tips：RNN中的 对于LSTM中的 ） 其中对于传递下去的 改变得很慢，通常输出的 是上一个状态传过来的 加上一些数值。 而 则在不同节点下往往会有很大的区别。 1.1 LSTM 结构深入下面具体对LSTM的内部结构来进行剖析。 首先使用LSTM的当前输入 和上一个状态传递下来的 拼接训练得到四个状态。 其中， ， ， 是由拼接向量乘以权重矩阵之后，再通过一个 激活函数转换成0到1之间的数值，来作为一种门控状态。而 则是将结果通过一个 激活函数将转换成-1到1之间的值（这里使用 是因为这里是将其做为输入数据，而不是门控信号） 下面开始进一步介绍这四个状态在LSTM内部的使用。（敲黑板） 其经典图片是这样的 1.2 主要的阶段LSTM内部主要有三个阶段： 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。 具体来说是通过计算得到的 （f表示forget）来作为忘记门控，来控制上一个状态的 哪些需要留哪些需要忘。 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 表示。而选择的门控信号则是由 （i代表information）来进行控制。 将上面两步得到的结果相加，即可得到传输给下一个状态的 。也就是上图中的第一个公式。 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 来进行控制的。并且还对上一阶段得到的 进行了放缩（通过一个tanh激活函数进行变化）。 与普通RNN类似，输出 往往最终也是通过 变化得到。 1.3 总结以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。 但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。 2. GRU GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的 。 其性能与LSTM相当 ，但是 其计算的消耗较前者少。 2.1 输入输出GRU的输入输出结构与普通的RNN是一样的。 有一个当前的输入 ，和上一个节点传递下来的隐状态（hidden state） ，这个隐状态包含了之前节点的相关信息。 结合 和 ，GRU会得到当前隐藏节点的输出 和传递给下一个节点的隐状态 。 2.2 内部结构 通过上一个传输下来的状态 和当前节点的输入 来获取两个门控状态。 其中 控制重置的门控（reset gate）， 为控制更新的门控（update gate）。 得到门控信号之后，首先使用重置门控来得到“重置”之后的数据 ，再将 与输入 进行拼接，再通过一个tanh激活函数来将数据放缩到**-1~1**的范围内。即得到如下图2-3所示的 。 这里的 主要是包含了当前输入的 数据。有针对性地对 添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“。 最后介绍GRU最关键的一个步骤，我们可以称之为”更新记忆“阶段。 在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控 （update gate）。 更新表达式： 首先再次强调一下，门控信号（这里的 ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。 有读者发现在pytorch里面的GRU[链接]写法相比原版对 多了一个映射，相当于一个GRU变体，猜测是多加多这个映射能让整体实验效果提升较大。如果有了解的同学欢迎评论指出。 GRU很聪明的一点就在于，我们使用了同一个门控 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。 ：表示对原本隐藏状态的选择性“遗忘”。这里的 可以想象成遗忘门（forget gate），忘记 维度中一些不重要的信息。 ： 表示对包含当前节点信息的 进行选择性”记忆“。与上面类似，这里的 同理会忘记 维度中的一些不重要的信息。或者，这里我们更应当看做是对 维度中的某些信息进行选择。 ：结合上述，这一步的操作就是忘记传递下来的 中的某些维度信息，并加入当前节点输入的某些维度信息。 可以看到，这里的遗忘 和选择 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （ ），我们就会使用包含当前输入的 中所对应的权重进行弥补 。以保持一种”恒定“状态。 2.3 总结GRU输入输出的结构与普通的RNN相似，其中的内部思想与LSTM相似。 与LSTM相比，GRU内部少了一个”门控“，参数比LSTM少，但是却也能够达到与LSTM相当的功能。考虑到硬件的计算能力和时间成本，因而很多时候我们也就会选择更加”实用“的GRU啦。"},{"title":"Recommender-System-Pytorch 项目 网络参数指南","date":"2022-03-28T12:43:29.890Z","url":"/2022/03/28/Recommender-System-Pytorch%20%E9%A1%B9%E7%9B%AE%20%E7%BD%91%E7%BB%9C%E5%87%BD%E6%95%B0/","categories":[["undefined",""]],"content":"个人觉得成熟的rebole的工具 不太适合萌新 来操作自己对于项目的建设 最近找了一个新的项目 来操作 感觉本项目 更贴近萌新到大佬写代码过程 于是乎 有了这篇指南 embedding操作0. 官方操作下面是官方例子 官方的解释： torch.nn.``Embedding(num_embeddings, embedding_dim, padding_idx&#x3D;None, max_norm&#x3D;None, norm_type&#x3D;2.0, scale_grad_by_freq&#x3D;False, sparse&#x3D;False, _weight&#x3D;None, device&#x3D;None, dtype&#x3D;None) num_embeddings：嵌入字典的大小（词的个数）； embedding_dim：每个嵌入向量的大小； padding_idx：若给定，则每遇到 padding_idx 时，位于 padding_idx 的嵌入向量（即 -padding_idx 映射所对应的向量）为0； max_norm：若给定，则每个大于 max_norm 的数都会被规范化为 max_norm； norm_type：为 max_norm 计算 p-范数的 p值； scale_grad_by_freq：若给定，则将按照 mini-batch 中 words 频率的倒数 scale gradients； sparse：若为 True，则 weight 矩阵将是稀疏张量。 1. 自己的瞎吉儿理解这里呀 就只需要理解好 前三个就好 对于前两个的理解 torch.nn.Embedding 的权重为 num_embeddings * embedding_dim 的矩阵，例如输入10个词，每个词用3为向量表示，则权重为10*3的矩阵； 对于 padding_idx 理解 可以看出 “6” 所对应映射的向量被填充了0。 网络初始化 1. 初始化函数 均匀分布torch.nn.init.uniform_(tensor, a&#x3D;0, b&#x3D;1)服从~U ( a , b ) U(a, b)U(a,b) 正太分布torch.nn.init.normal_(tensor, mean&#x3D;0, std&#x3D;1)服从~N ( m e a n , s t d ) N(mean, std)N(mean,std) 初始化为常数torch.nn.init.constant_(tensor, val)初始化整个矩阵为常数val Xavier 基本思想是通过网络层时，输入和输出的方差相同，包括前向传播和后向传播。具体看以下博文： 为什么需要Xavier 初始化？如果初始化值很小，那么随着层数的传递，方差就会趋于0，此时输入值 也变得越来越小，在sigmoid上就是在0附近，接近于线性，失去了非线性如果初始值很大，那么随着层数的传递，方差会迅速增加，此时输入值变得很大，而sigmoid在大输入值写倒数趋近于0，反向传播时会遇到梯度消失的问题 xavier初始化的简单推导 kaiming (He initialization) 以后再说 现在没用上 bug解决感谢涛哥 对于源码的修改 错误信息 在改错的时候注意看最后一行 即 修改在这里定位好 然后修改成： "},{"title":"推荐数据集处理（分桶） 重要（比较水）","date":"2022-03-28T03:06:43.179Z","url":"/2022/03/28/%E6%8E%A8%E8%8D%90%E6%95%B0%E6%8D%AE%E9%9B%86%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"1. 探索数据集1.1 数据读入 1.2 查看数据 1.3 数据分类 1.4 查看缺失值情况（标注） 1.5 数据分箱 1.6 训练数据与标签分离 2. 字段维度（field_dim） 与 数据划分 2.1 字段维度获取（field_dim） 2.2 数据划分 "},{"title":"软著申请流程","date":"2022-03-27T02:32:53.849Z","url":"/2022/03/27/%E8%BD%AF%E8%91%97%E7%94%B3%E8%AF%B7/","categories":[["undefined",""]],"content":"感谢   "},{"title":"python中axis=0和axis=1的理解","date":"2022-03-25T01:45:41.345Z","url":"/2022/03/25/axis%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"原文链接： axis的重点在于方向，而不是行和列。1表示横轴，方向从左到右；0表示纵轴，方向从上到下。 即axis&#x3D;1为横向，axis&#x3D;0为纵向，而不是行和列，具体到各种用法而言也是如此。当axis&#x3D;1时，如果是求平均，那么是从左到右横向求平均；如果是拼接，那么也是左右横向拼接；如果是drop，那么也是横向发生变化，体现为列的减少 axis &#x3D; 0 纵向处理 axis &#x3D; 1 横向处理 "},{"title":"信用卡交易数据解读与探索（数据合并）","date":"2022-03-21T07:01:44.023Z","url":"/2022/03/21/%E4%BF%A1%E7%94%A8%E5%8D%A1%E4%BA%A4%E6%98%93%E6%95%B0%E6%8D%AE%E8%A7%A3%E8%AF%BB/","categories":[["undefined",""]],"content":"数据分析首先还是对数据集进行解释，以及简单验证数据集的正确性。信用卡交易记录包括了两个数据集，分别是historical_transactions和new_merchant_transactions。两个数据集字段类似，只是记录了不同时间区间的信用卡消费情况： 这里的数据存在两个 一个18以前的数据集 一个18以后的 数据解读 首先简单查看有哪些字段一致： 并且我们进一步发现，交易记录中的merhcant_id信息并不唯一： 造成该现象的原因可能是商铺在逐渐经营过程动态变化，而基于此，在后续的建模过程中，我们将优先使用交易记录中表中的相应记录。 数据预处理 连续&#x2F;离散字段标注 首先也是一样，需要对其连续&#x2F;离散变量进行标注。当然该数据集中比较特殊的一点，是存在一个时间列，我们将其单独归为一类： 字段类型转换&#x2F;缺失值填补 "},{"title":"商户数据解读与探索(包含较为复杂的数据处理)","date":"2022-03-21T01:54:47.085Z","url":"/2022/03/21/%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E6%B8%85%E6%B4%97/","categories":[["undefined",""]],"content":"复杂的数据处理过程（含清洗）1. 数据解读 2. 数据探索 正确性检验 查看id出现次数是否唯一 缺失值分析 能够发现，第二个匿名分类变量存在较多缺失值，而avg_sales_lag3&#x2F;6&#x2F;12缺失值数量一致，则很有可能是存在13个商户同时确实了这三方面信息。其他数据没有缺失，数据整体来看较为完整。 3. 数据预处理3.1 离散&#x2F;连续字段标注由于商户数据集中特征同时存在分类变量和离散变量，因此我们首先可以根据字段的说明对不同属性特征进行统一的划分： 3.2 离散数据处理 离散变量数据情况 离散变量字典编码 接下来对离散变量进行字典编码，即将object对象类型按照sort顺序进行数值化（整数）编码。例如原始category_1取值为Y&#x2F;N，通过sort排序后N在Y之前，因此在重新编码时N取值会重编码为0、Y取值会重编码为1。以此类推。 需要注意的是，从严格角度来说，变量类型应该是有三类，分别是连续性变量、名义型变量以及有序变量。连续变量较好理解，所谓名义变量，指的是没有数值大小意义的分类变量，例如用1表示女、0表示男，0、1只是作为性别的指代，而没有1&gt;0的含义。而所有有序变量，其也是离散型变量，但却有数值大小含义，如上述most_recent_purchases_range字段，销售等级中A&gt;B&gt;C&gt;D&gt;E，该离散变量的5个取值水平是有严格大小意义的，该变量就被称为有序变量。 在实际建模过程中，如果不需要提取有序变量的数值大小信息的话，可以考虑将其和名义变量一样进行独热编码。但本阶段初级预处理时暂时不考虑这些问题，先统一将object类型转化为数值型。&#x3D;&#x3D;（object类型转换类型）&#x3D;&#x3D; 测试 3.3 连续变量数据探索 据此我们发现连续型变量中存在部分缺失值，并且部分连续变量还存在无穷值inf，需要对其进行简单处理。 无穷值处理 缺失值处理 不同于无穷值的处理，缺失值处理方法有很多。但该数据集缺失数据较少，33万条数据中只有13条连续特征缺失值，此处我们先简单采用均值进行填补处理，后续若有需要再进行优化处理。 "},{"title":"kaggle入门","date":"2022-03-20T11:02:48.709Z","url":"/2022/03/20/kaggle%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"环境安装和准备anaconda + jupyter 获取kaggle.json &amp;emsp;&amp;emsp;在安装完成kaggle之后，进入Kaggle的个人主页（点击右上角头像），点击Create New API Token，则可创建一个kaggle.json文件，并自动开始下载 ​ - 将kaggle.json文件移动到.kaggle文件夹内 安装内核使用anaconda虚拟环境作为jupyter notebook内核 删除内核 感谢： "},{"title":"深度推荐系统 下","date":"2022-03-19T07:31:56.636Z","url":"/2022/03/19/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(2)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（2）2.4 FM与深度学习模型2.4.1 FNN - 用FM的隐向量完成embedding层初始化 FNN相较于deep crossing模型的区别 对于embedding进行了改进，在模型初始化时引入了有价值的先验信息 在训练时特征被划分了不同的特征域，每个特征域有对应的embedding层 2.4.2 deepFM（啊哈哈 大怨种来啦）究竟是怎样的大佬 看得懂我下面这句话 反正我是看不懂的 强调 红色为 权重1连接 就是是啥就是啥 用FM代替wide部分 FM部分 这个FM layer 在图上面 直观的看啊 有 一个+ 和 好多个 * 这个+ 是 FM层 的线性部分 这个*呢 就是 FM层的 特征组合部分（这里应该是 vi 点积 vj） 其输出公式为 deep 部分 这个deep部分啊 就是多层感知机嘛 没啥难的 ​ ​ FM与深度模型的组合有两种，一种是二者并行，另一种是二者串行。DeepFM就是并行的一种结构。并行就是FM将输入部分计算完之后单独拿出来，得到一组特征表示，然后再利用深度模型（多层全连接）对输入部分进行高阶的特征组合。最后把二者的特征进行concact，得到一组特征，最后对这组特征进行分类或者回归。其实这只是特征的一种组合方式，目的就是为了得到特征的高阶表示。 输出公式 细节（权重共用） 下面解释好好的看 这里的第二点如何理解呢，假设我们的k&#x3D;5，首先，对于输入的一条记录，同一个field 只有一个位置是1，那么在由输入得到dense vector的过程中，输入层只有一个神经元起作用，得到的dense vector其实就是输入层到embedding层该神经元相连的五条线的权重，即vi1，vi2，vi3，vi4，vi5。这五个值组合起来就是我们在FM中所提到的Vi。在FM部分和DNN部分，这一块是共享权重的，对同一个特征来说，得到的Vi是相同的。 2.4.3 总结特征工程在这条路上已经穷尽了可能性的尝试，模型的提升空间会非常小。但是很重要 2.5 注意力机制的应用 Attention机制的本质 attention机制的本质是从人类视觉注意力机制中获得灵感(可以说很‘以人为本’了)。大致是我们视觉在感知东西的时候，一般不会是一个场景从到头看到尾每次全部都看，而往往是根据需求观察注意特定的一部分。而且当我们发现一个场景经常在某部分出现自己想观察的东西时，我们就会进行学习在将来再出现类似场景时把注意力放到该部分上。这可以说就是注意力机制的本质内容了。至于它本身包含的‘自上而下’和‘自下而上’方式就不在过多的讨论。 Attention机制的理解 Attention机制其实就是一系列注意力分配系数，也就是一系列权重参数罢了。 2.5.1 AFM - 引入注意力机制的FM 注意力网络的作用是为每一个交叉特征提供权重 2.5.2 DIN basemodel DIN通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给定的广告）。 注意力在其上面的形式是激活单元来生成注意力得分"},{"title":"训练集和数据集数据探索","date":"2022-03-18T13:03:49.043Z","url":"/2022/03/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","categories":[["undefined",""]],"content":"数据分析 1. 数据解读 上面的操作就很easy 2. 数据质量分析​ 接下来简单数据探索。在实际建模过程中，首先我们会先校验数据的正确性，并检验缺失值、异常值等情况。 数据正确性校验 所谓数据正确性，指的是数据本身是否符合基本逻辑，例如此处信用卡id作为建模分析对象独一无二的标识，我们需要验证其是否确实独一无二，并且训练集和测试集信用卡id无重复。 判断缺失值情况 3. 异常值分析 describe()方法 异常值检验。由于我们尚未对数据集特征进行预处理，因此我们先查看标签列的异常值情况。首先我们可以用describe()方法查看这一列的基本统计信息： 通过直方图观察 由于是连续变量可以借助概率密度直方图进行分布的观察： $3\\delta$原则进行异常值识别 ​ 能够发现，大部分用户忠诚度评分都集中在[-10,10]之间，并且基本符合正态分布，唯一需要注意的是有个别异常值取值在-30以下，该数据在后续分析中需要额外注意。我们可以简单查看有多少用户的标签数值是小于30的： 当然，对于连续变量，一般可以采用$3\\delta$原则进行异常值识别，此处我们也可以简单计算下异常值范围： &amp;emsp;&amp;emsp;需要注意的是，此处我们是围绕标签进行的异常值检测，而本案例中标签并不是自然数值测量或统计的结果（如消费金额、身高体重等），而是通过某种公式人工计算得出（详见赛题分析）。出现如此离群点极有可能是某类特殊用户的标记。因此不宜进行异常值处理，而应该将其单独视作特殊的一类，在后续建模分析时候单独对此类用户进行特征提取与建模分析。 4. 规律一致性分析&amp;emsp;&amp;emsp;接下来，进行训练集和测试集的规律一致性分析。 &amp;emsp;&amp;emsp;所谓规律一致性，指的是需要对训练集和测试集特征数据的分布进行简单比对，以“确定”两组数据是否诞生于同一个总体，即两组数据是否都遵循着背后总体的规律，即两组数据是否存在着规律一致性。 &amp;emsp;&amp;emsp;我们知道，尽管机器学习并不强调样本-总体的概念，但在训练集上挖掘到的规律要在测试集上起到预测效果，就必须要求这两部分数据受到相同规律的影响。一般来说，对于标签未知的测试集，我们可以通过特征的分布规律来判断两组数据是否取自同一总体 单变量分析 当然，我们需要同时对比训练集和测试集的四个特征，可以通过如下代码实现： 多级联合分布 ​ 接下来，我们进一步查看联合变量分布。所谓联合概率分布，指的是将离散变量两两组合，然后查看这个新变量的相对占比分布。例如特征1有0&#x2F;1两个取值水平，特征2有A&#x2F;B两个取值水平，则联合分布中就将存在0A、0B、1A、1B四种不同取值水平，然后进一步查看这四种不同取值水平出现的分布情况。 ​ 实际建模过程中，规律一致性分析是非常重要但又经常容易被忽视的一个环节。通过规律一致性分析，我们可以得出非常多的可用于后续指导后续建模的关键性意见。通常我们可以根据规律一致性分析得出以下基本结论 ​ 作用： 如果分布非常一致，则说明所有特征均取自同一整体，训练集和测试集规律拥有较高一致性，模型效果上限较高，建模过程中应该更加依靠特征工程方法和模型建模技巧提高最终预测效果 如果分布不太一致，则说明训练集和测试集规律不太一致，此时模型预测效果上限会受此影响而被限制，并且模型大概率容易过拟合，在实际建模过程中可以多考虑使用交叉验证等方式防止过拟合，并且需要注重除了通用特征工程和建模方法外的trick的使用； "},{"title":"深度推荐系统 上","date":"2022-03-15T13:20:16.514Z","url":"/2022/03/15/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(1)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（1）演化方式： 改变神经网络的复杂程度 改变特征交叉方式 wide&amp;deep模型 FM深度版本 注意力机制与推荐系统结合 序列模型与推荐模型结合 强化学习与深度学习结合 2.1 神经网络复杂程度2.1.1 Auto-rec–单层神经网络推荐模型（easy） 通过自编码器原理，还原输入的结果。 重建函数： 目标函数： 目标函数l2正则： 参考： 2.1.2 Deep Crossing模型–经典深度学习架构 应用场景 网络结构 embedding层，stacking层，multiple residual units层，scoring层 ​ 反思 embedding+多层神经网络，相较于传统的二阶特征交叉能力，deep crossing拥有深度交叉的能力 2.1.2 NeuralCF - CF与深度学习的结合先回忆一下传统的矩阵分解怎么做 物品-用户共现矩阵分解成用户向量和物品向量 向量embedding化 embedding后向量取内积（重要） 得到分数 这个模型 复杂的位置就是在第三步操作上 使用多层神经网络去替换这个卷积操作 &#x3D;&#x3D;优势&#x3D;&#x3D; 利用神经网络来拟合任意函数，灵活地组成不同的特征，按需增加或减少模型的复杂度 &#x3D;&#x3D;劣势&#x3D;&#x3D; 基于协同过滤构造,没有引入更多其他类型的特征 在实践中，防止过拟合的风险 2.2 加强特征交叉能力2.2.1 PNN模型感谢：  ​ ​ 相较于 deep crossing模型中的stacking层，PNN模型替换成了乘积层。其他的在模型的输入，embeding层，多层神经网络以及最终的输出层上没有结构上的不同。 ​ product layer层，左边为线性部分，认为 特征之间的关系是and“且”的一种关系，而非add”加”的关系。 ​ z&#x3D;concat([emb1,emb2..,embn],axis&#x3D;1) 其右边操作为乘积操作，有内积和外积的区别。外积在操作上会将问题的复杂度从原来的m到 $m^2$,在选择上更应该慎重。 优势 ​ 定义了外积和内积操作更有针对性地强调不同特征之间的交互 局限 ​ 在外积操作上，为了效率经行大量的简化操作，对所有特征进行无差别的交叉，在一定程度上忽略了原始特征中包含的价值信息。 2.2.2 product layer 内积 PNN中p的计算方式如下，即使用内积来代表pij： 2.2.3 product layer 外积 OPNN中p的计算方式如下： 此时pij为MM的矩阵，计算一个pij的时间复杂度为MM，而p是NNMM的矩阵，因此计算p的事件复杂度为NNMM。从而计算lp的时间复杂度变为D1 * NNM*M。这个显然代价很高的。为了减少负责度，论文使用了叠加的思想，它重新定义了p矩阵： 2.3 记忆能力与泛化能力的综合2.3.1 wide&amp;deep模型​ wide部分是让模型具有较强的“记忆能力”，deep部分是让模型具有泛化能力。这样的结构使模型兼具了逻辑回归和深度神经网络的优点–能快速处理并且记忆大量的历史行为特征，并且具有强大的表达能力。 在提出W&amp;D模型，平衡Wide模型和Deep模型的记忆能力和泛化能力。实际上是lr+dnn。记忆（memorization） 通过特征叉乘对原始特征做非线性变换，输入为高维度的稀疏向量。通过大量的特征叉乘产生特征相互作用的“记忆（Memorization）”，高效且可解释，但要泛化需要更多的特征工程。 泛化（generalization）只需要少量的特征工程，深度神经网络通过embedding的方法，使用低维稠密特征输入，可以更好地泛化训练样本中未出现过的特征组合。但当user-item交互矩阵稀疏且高阶时，容易出现“过泛化（over-generalize）”导致推荐的item相关性差 工程应用 2.3.2 wide&amp;deep进化 deep&amp;cross模型 Cross Network ​ 交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式： ​ xl和xl+1 分别是第l层和第l+1层cross layer的输出，wl和bl是这两层之间的连接参数。注意上式中所有的变量均是列向量，W也是列向量，并不是矩阵。xl+1 &#x3D; f(xl, wl, bl) + xl. 每一层的输出，都是上一层的输出加上feature crossing f。而f就是在拟合该层输出和上一层输出的残差。 ​ Cross Layer 设计的巧妙之处全部体现在上面的计算公式中，我们先看一些明显的细节：1) 每层的神经元个数都相同，都等于输入 的维度 DCN能够有效地捕获有限度的有效特征的相互作用，学会高度非线性的相互作用，不需要人工特征工程或遍历搜索，并具有较低的计算成本。1）提出了一种新的交叉网络，在每个层上明确地应用特征交叉，有效地学习有界度的预测交叉特征，并且不需要手工特征工程或穷举搜索。2）跨网络简单而有效。通过设计，各层的多项式级数最高，并由层深度决定。网络由所有的交叉项组成，它们的系数各不相同。3）跨网络内存高效，易于实现。4）实验结果表明，交叉网络（DCN）在LogLoss上与DNN相比少了近一个量级的参数量"},{"title":"传统推荐系统","date":"2022-03-14T13:48:48.181Z","url":"/2022/03/14/%E4%BC%A0%E7%BB%9F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","categories":[["undefined",""]],"content":"1. 推荐系统技术架构数据+模型 1.1 数据部分通过特征工程，将客户端或服务端采集到的数据进行特征处理 1.2 模型部分主题 一般由召回层，排序层，补充数据与算法层组成 2. 传统推荐模型（粗略整理）2.1 协同过滤分为 用户过滤和物品过滤 2.1.1 用户过滤（没人用）公式一 余弦相似度 公式二 皮尔逊相关系数（减少了用户评分的影响） 2.1.2 物品过滤 基于历史数据，构建用户-物品共现矩阵（m*n） 计算共现矩阵两两向量间的相似性 获得用户历史行为数据的正反馈物品列表 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的topK物品 对于相似度进行排序，生成最终的推荐列表 2.2 矩阵分解算法–狗都不用通过分解协同过滤生成的共现矩阵 得到用户和物品的隐向量 2.2.1 矩阵分解的求解方法梯度下降 目标函数 2.3 特征交叉单一特征的表达性 没有特征组合起来的表达性好 且 单一特征会损失一定量的信息 2.3.1 POLY2模型暴力将特征n个 变成了$ n^2 $ 会将数据更加稀疏 增加训练复杂度 2.3.2 FM模型-隐向量特征交叉（嗯嗯嗯！）感谢： 个人的愚蠢回忆说实话 有些东西一定要好好折磨一下 才能有新的收获 ！ 比如这个 本人大白话讲一遍，啊啊啊 上面的式子太暴力，脑子思考一下就发现权重项太多，我在上面的这篇文献中发现这么一句：**对于任何正定实矩阵 只要k足够大，都存在k维向量组成的矩阵 使得 ** 在这个公式下 这个V啊 就是辅助向量。 这个可爱的V呢 满足 （我想到了2077） 隐向量 就是为每个特征 学习一个隐权重向量（latent vector） 交互使用两个向量取内积就好 优势 1. 权重参数减少到nk ​ 2. 训练复杂度降低到nk级别 2.3.3 FFM模型 特征域感知概念（可笑 咱把东西想简单了）数据还是上一次的数据 FFM模型中引入了类别的概念，即field 在上面的广告点击案例中，“Day&#x3D;26&#x2F;11&#x2F;15”、“Day&#x3D;1&#x2F;7&#x2F;14”、“Day&#x3D;19&#x2F;2&#x2F;15”这三个特征都是代表日期的，可以放到同一个field中。同理，Country也可以放到一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户国籍，广告类型，日期等等 在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量 v_i,fj。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day&#x3D;26&#x2F;11&#x2F;15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来 训练过程中，需要学习n个特征在f个域上的k维隐向量，参数 n * k * f 复杂度为 k$ n^2 $ 远多于FM模型的 nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 O(kn^2)。 数据的处理像Country，day 这样的categorical特征 可以通过 one-hot进行生成 如果是 像price这样的数值特征，则并不需要对其进行单独编码，但需要为其生成一个field。 个人的纯真理解（一眼顶真） 敲散说明白假如说 我这里有4个field 一共有10个feature。 动动小脑就晓得 假如说 field1 对应 feat1到feat4 则对应 feat1到feat4 要生成对应的 field2到field4的权重（这个就是场感知啊！！！） 脑瓜子随便一想 哇靠 真复杂 2.4 GBDT+LR 特征工程模型化 原始特征向量x，通过树分裂 将转化的特征类似于one-hot的向量来表示原始的特征，特征组合能力特别强 但是容易产生过拟合，以及这样的过程丢失了大量特征数值信息。 2.5 MLR 深度学习开始的曙光2.5.1 MLR与LR的区别 普通的LR模型 无法拟合我们所需的曲线 但是MLR模型正常拟合出来了 2.5.2 目标公式 如果m为1 则为普通的LR模型 当m越大 模型的拟合能力越强 而同样 需要的训练样本也变得更大 （阿里巴巴 经验12） 2.5.3 优点 端到端的非线性学习能力 模型稀疏性强 "},{"title":"centos安装","date":"2022-03-13T06:47:18.013Z","url":"/2022/03/13/1.%20centos%E5%AE%89%E8%A3%85/","categories":[["undefined",""]],"content":"个人主页  1. centos安装1.1 centos安装注意 1.2 ssh的连接 "}]