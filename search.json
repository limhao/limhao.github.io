[{"title":"各种慢","date":"2022-04-06T13:34:45.459Z","url":"/2022/04/06/%E5%90%84%E7%A7%8D%E6%85%A2/","categories":[["undefined",""]],"content":"一个常用的各种慢的解决复制帖 pip慢 -i  conda 慢添加清华镜像源，代码如下所示： "},{"title":"criteo数据集的处理","date":"2022-04-06T12:25:47.026Z","url":"/2022/04/06/criteo%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"目前项目涉及三个数据集（criteo，amazon，movielen） 其中 criteo数据集肯定是使用的最多的啦 这里 对于数据的梳理 （对于数据的处理 我觉得也较通用） 特地 整理一个文档 0. 观察数据（其实很重要） 数据 有0 1 2 这样的离散数据 和 一些68fd1e64 80e26c9b fb936136 7b4723c4 不晓得是啥的数据 感觉能确定的是 第一列的数据 是标签数据 欧克 总结数据如下 有标签 离散 和 不晓得是啥的数据（先确定为连续数据吧） 1. 读取数据 1.1 标准数据查看三件套 1.2 数据质量分析 查看缺失值 异常值分析 "},{"title":"从din到dien 推荐模型（文献部分）","date":"2022-04-04T06:17:53.421Z","url":"/2022/04/04/%E4%BB%8Edin%E5%88%B0dien%20%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%EF%BC%88%E6%96%87%E7%8C%AE%E9%83%A8%E5%88%86%EF%BC%89/","categories":[["undefined",""]],"content":"感谢：     ​ 最近比较迷茫，看了看自己的论文阅读表，这两篇阿里巴巴的文献映入眼帘，作为工业界的老大。其思维比较新颖。且较当时比较的模型有较大百分比的提升。CTR预测， 所谓的CTR，就是点击率Click Through Rate，而CTR预估算法在广告系统中起着至关重要的作用。从这两篇文章中可以看到阿里妈妈在深度学习方向上的探索，也可以窥视到阿里妈妈的CTR算法的发展脉络。 0. 一切从特征选取开始 主要有4个特征组，如下图所示， 1）用户画像特征， 2）用户行为特征，即用户点击过的商品 3）待曝光的广告，广告其实也是商品，后文中我们统称为candidate 4）上下文特征。 1. 模型的鼻祖 （embedding + mlp） 如果说有10个商品的embedding，embedding的维度为16。在业界，pooling的处理方式就两个。 sum pooling 对应的维度数字求和 mean pooling 对应的维度数字求平均 1.1 问题​ 在电商这个场景中，通常用户的兴趣具有多样性，可能在一段时间内点击过衣服，电子产品，鞋子等。而对于不同的candidate来说，浏览过的相关商品对于预测帮助更大，不相关的商品对于ctr预估可能并不起作用，例如用户看过的衣服，鞋子对于iphone的预测并没有帮助 2. DIN​ 为了解决上述的问题，既然提到了相关，那肯定得考虑到&#x3D;&#x3D;注意力机制&#x3D;&#x3D;了啦 ​ 解决思路是： 在pooling的时候，与candidate相关的商品权重大一些，与candidate不相关的商品权重小一些，这是一种Attention的思想。将candidate与点击序列中的每个商品发生交互来计算attention分数。具体计算方法如图3中右上角的小网络所示，输入包括商品和candidate的embedding向量，以及两者的外积。对于不同的candidate，得到的用户表示向量也不同，具有更大的灵活性。 ​ 模型基础上 提出了Activation Unit单元 来提取商品与目标广告之间的相关性。 其中activation unit的输入包括两个部分，一个是原始的用户行为embedding向量、广告embedding向量；另外一个是两者Embedding向量经过外积计算后得到的向量，文章指出这种方式有利于relevance modeling。 ​ 2.1 attention归一化处理​ 一般来说，做attention的时候，需要对所有的分数通过softmax做归一化，这样做有两个好处，一是保证权重非负，二是保证权重之和为1。但是在DIN的论文中强调，不对点击序列的attention分数做归一化，直接将分数与对应商品的embedding向量做加权和，目的在于保留用户的兴趣强度。例如，用户的点击序列中90%是衣服，10%是电子产品，有一件T恤和一部手机需要预测CTR，那么T恤会激活大部分的用户行为，使得根据T恤计算出来的用户行为向量在数值上更大，相对手机而言。 2.2 DIN的创新点​ DIN的论文中还提出了两个小的改进点。一个是对L2正则化的改进，在进行SGD优化的时候，每个mini-batch都只会输入部分训练数据，反向传播只针对部分非零特征参数进行训练，添加上L2之后，需要对整个网络的参数包括所有特征的embedding向量进行训练，这个计算量非常大且不可接受。论文中提出，在每个mini-batch中只对该batch的特征embedding参数进行L2正则化。第二个是提出了一个激活函数Dice。对于Relu或者PRelu来说，rectified point(梯度发生变化的点)都在0值，Dice对每个特征以mini-batch为单位计算均值和方差，然后将rectified point调整到均值位置。 3. DIEN DIEN，全称是Deep Interest Evolution Network，即用户兴趣进化网络。这个算法中用两层架构来抽取和使用用户兴趣特征： 兴趣抽取层Interest Extractor Layer: 从用户行为序列中提取信息 兴趣进化层Interest Evolving Layer: 从用户行为序列中找到目标相关的兴趣，对其进行建模 3.1 兴趣抽取层（这里使用的是GRU）​ 在广告与商品之间并不是单纯的商品对应的关系（即我喜欢的是这个） ​ 兴趣抽取层Interest Extractor Layer的主要目标是从embedding数据中提取出interest。但一个用户在某一时间的interest不仅与当前的behavior有关，也与之前的behavior相关，所以作者们使用GRU单元来提取interest。GRU单元的表达式如下。 其中，σ是sigmoid操作，而◦是内积操作。 作者的辅助测试 ​ 作者设计了一个二分类模型来计算兴趣抽取的准确性，我们将用户下一时刻真实的行为e(t+1)作为正例，负采样得到的行为作为负例e(t+1)’，分别与抽取出的兴趣h(t)结合输入到设计的辅助网络中，得到预测结果，并通过logloss计算一个辅助的损失 3.2 兴趣进化层 （带有注意力的GRU） interest在变化过程中遵循如下规律：1）interest drift：用户在某一段时间的interest会有一定的集中性。比如用户可能在一段时间内不断买书，在另一段时间内不断买衣服。2）interest individual：一种interest有自己的发展趋势，不同种类的interest之间很少相互影响，例如买书和买衣服的interest基本互不相关。 3.2.1 attention机制兴趣抽取层获得的隐向量，ht 与 被embedding后的广告向量 产生内积后softmax取相关参数 3.2.2 attention方式 GRU with attentional input (AIGRU) 这种方式将attention直接作用于输入，无需修改GRU的结构： ​ Attention based GRU(AGRU) 这种方式需要修改GRU的结构，此时hidden state的输出变为： ​ GRU with attentional update gate (AUGRU) 这种方式需要修改GRU的结构，此时hidden state的输出变为: ​ "},{"title":"RNN模型","date":"2022-04-03T13:18:49.397Z","url":"/2022/04/03/RNN%E7%B1%BB%E6%A8%A1%E5%9E%8B/","categories":[["undefined",""]],"content":"感谢： 0. RNN 先简单介绍一下一般的RNN。 这里： !() 为当前状态下数据的输入， 表示接收到的上一个节点的输入。 为当前节点状态下的输出，而 为传递到下一个节点的输出。 通过上图的公式可以看到，输出 h’ 与 x 和 h 的值都相关。 而 y 则常常使用 h’ 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。 对这里的y如何通过 h’ 计算得到往往看具体模型的使用方式。 通过序列形式的输入，我们能够得到如下形式的RNN。 1. LSTM 长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。 相比RNN只有一个传递状态 ，LSTM有两个传输状态，一个 （cell state），和一个 （hidden state）。（Tips：RNN中的 对于LSTM中的 ） 其中对于传递下去的 改变得很慢，通常输出的 是上一个状态传过来的 加上一些数值。 而 则在不同节点下往往会有很大的区别。 1.1 LSTM 结构深入下面具体对LSTM的内部结构来进行剖析。 首先使用LSTM的当前输入 和上一个状态传递下来的 拼接训练得到四个状态。 其中， ， ， 是由拼接向量乘以权重矩阵之后，再通过一个 激活函数转换成0到1之间的数值，来作为一种门控状态。而 则是将结果通过一个 激活函数将转换成-1到1之间的值（这里使用 是因为这里是将其做为输入数据，而不是门控信号） 下面开始进一步介绍这四个状态在LSTM内部的使用。（敲黑板） 其经典图片是这样的 1.2 主要的阶段LSTM内部主要有三个阶段： 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。 具体来说是通过计算得到的 （f表示forget）来作为忘记门控，来控制上一个状态的 哪些需要留哪些需要忘。 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 表示。而选择的门控信号则是由 （i代表information）来进行控制。 将上面两步得到的结果相加，即可得到传输给下一个状态的 。也就是上图中的第一个公式。 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 来进行控制的。并且还对上一阶段得到的 进行了放缩（通过一个tanh激活函数进行变化）。 与普通RNN类似，输出 往往最终也是通过 变化得到。 1.3 总结以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。 但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。 2. GRU GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的 。 其性能与LSTM相当 ，但是 其计算的消耗较前者少。 2.1 输入输出GRU的输入输出结构与普通的RNN是一样的。 有一个当前的输入 ，和上一个节点传递下来的隐状态（hidden state） ，这个隐状态包含了之前节点的相关信息。 结合 和 ，GRU会得到当前隐藏节点的输出 和传递给下一个节点的隐状态 。 2.2 内部结构 通过上一个传输下来的状态 和当前节点的输入 来获取两个门控状态。 其中 控制重置的门控（reset gate）， 为控制更新的门控（update gate）。 得到门控信号之后，首先使用重置门控来得到“重置”之后的数据 ，再将 与输入 进行拼接，再通过一个tanh激活函数来将数据放缩到**-1~1**的范围内。即得到如下图2-3所示的 。 这里的 主要是包含了当前输入的 数据。有针对性地对 添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“。 最后介绍GRU最关键的一个步骤，我们可以称之为”更新记忆“阶段。 在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控 （update gate）。 更新表达式： 首先再次强调一下，门控信号（这里的 ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。 有读者发现在pytorch里面的GRU[链接]写法相比原版对 多了一个映射，相当于一个GRU变体，猜测是多加多这个映射能让整体实验效果提升较大。如果有了解的同学欢迎评论指出。 GRU很聪明的一点就在于，我们使用了同一个门控 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。 ：表示对原本隐藏状态的选择性“遗忘”。这里的 可以想象成遗忘门（forget gate），忘记 维度中一些不重要的信息。 ： 表示对包含当前节点信息的 进行选择性”记忆“。与上面类似，这里的 同理会忘记 维度中的一些不重要的信息。或者，这里我们更应当看做是对 维度中的某些信息进行选择。 ：结合上述，这一步的操作就是忘记传递下来的 中的某些维度信息，并加入当前节点输入的某些维度信息。 可以看到，这里的遗忘 和选择 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （ ），我们就会使用包含当前输入的 中所对应的权重进行弥补 。以保持一种”恒定“状态。 2.3 总结GRU输入输出的结构与普通的RNN相似，其中的内部思想与LSTM相似。 与LSTM相比，GRU内部少了一个”门控“，参数比LSTM少，但是却也能够达到与LSTM相当的功能。考虑到硬件的计算能力和时间成本，因而很多时候我们也就会选择更加”实用“的GRU啦。"},{"title":"Recommender-System-Pytorch 项目 网络参数指南","date":"2022-03-28T12:43:29.890Z","url":"/2022/03/28/Recommender-System-Pytorch%20%E9%A1%B9%E7%9B%AE%20%E7%BD%91%E7%BB%9C%E5%87%BD%E6%95%B0/","categories":[["undefined",""]],"content":"个人觉得成熟的rebole的工具 不太适合萌新 来操作自己对于项目的建设 最近找了一个新的项目 来操作 感觉本项目 更贴近萌新到大佬写代码过程 于是乎 有了这篇指南 embedding操作0. 官方操作下面是官方例子 官方的解释： torch.nn.``Embedding(num_embeddings, embedding_dim, padding_idx&#x3D;None, max_norm&#x3D;None, norm_type&#x3D;2.0, scale_grad_by_freq&#x3D;False, sparse&#x3D;False, _weight&#x3D;None, device&#x3D;None, dtype&#x3D;None) num_embeddings：嵌入字典的大小（词的个数）； embedding_dim：每个嵌入向量的大小； padding_idx：若给定，则每遇到 padding_idx 时，位于 padding_idx 的嵌入向量（即 -padding_idx 映射所对应的向量）为0； max_norm：若给定，则每个大于 max_norm 的数都会被规范化为 max_norm； norm_type：为 max_norm 计算 p-范数的 p值； scale_grad_by_freq：若给定，则将按照 mini-batch 中 words 频率的倒数 scale gradients； sparse：若为 True，则 weight 矩阵将是稀疏张量。 1. 自己的瞎吉儿理解这里呀 就只需要理解好 前三个就好 对于前两个的理解 torch.nn.Embedding 的权重为 num_embeddings * embedding_dim 的矩阵，例如输入10个词，每个词用3为向量表示，则权重为10*3的矩阵； 对于 padding_idx 理解 可以看出 “6” 所对应映射的向量被填充了0。 网络初始化 1. 初始化函数 均匀分布torch.nn.init.uniform_(tensor, a&#x3D;0, b&#x3D;1)服从~U ( a , b ) U(a, b)U(a,b) 正太分布torch.nn.init.normal_(tensor, mean&#x3D;0, std&#x3D;1)服从~N ( m e a n , s t d ) N(mean, std)N(mean,std) 初始化为常数torch.nn.init.constant_(tensor, val)初始化整个矩阵为常数val Xavier 基本思想是通过网络层时，输入和输出的方差相同，包括前向传播和后向传播。具体看以下博文： 为什么需要Xavier 初始化？如果初始化值很小，那么随着层数的传递，方差就会趋于0，此时输入值 也变得越来越小，在sigmoid上就是在0附近，接近于线性，失去了非线性如果初始值很大，那么随着层数的传递，方差会迅速增加，此时输入值变得很大，而sigmoid在大输入值写倒数趋近于0，反向传播时会遇到梯度消失的问题 xavier初始化的简单推导 kaiming (He initialization) 以后再说 现在没用上 bug解决感谢涛哥 对于源码的修改 错误信息 在改错的时候注意看最后一行 即 修改在这里定位好 然后修改成： "},{"title":"推荐数据集处理（分桶） 重要","date":"2022-03-28T03:06:43.179Z","url":"/2022/03/28/%E6%8E%A8%E8%8D%90%E6%95%B0%E6%8D%AE%E9%9B%86%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"1. 探索数据集1.1 数据读入 1.2 查看数据 1.3 数据分类 1.4 查看缺失值情况（标注） 1.5 数据分箱 1.6 训练数据与标签分离 2. 字段维度（field_dim） 与 数据划分 2.1 字段维度获取（field_dim） 2.2 数据划分 "},{"title":"软著申请流程","date":"2022-03-27T02:32:53.849Z","url":"/2022/03/27/%E8%BD%AF%E8%91%97%E7%94%B3%E8%AF%B7/","categories":[["undefined",""]],"content":"感谢   "},{"title":"python中axis=0和axis=1的理解","date":"2022-03-25T01:45:41.345Z","url":"/2022/03/25/axis%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"原文链接： axis的重点在于方向，而不是行和列。1表示横轴，方向从左到右；0表示纵轴，方向从上到下。 即axis&#x3D;1为横向，axis&#x3D;0为纵向，而不是行和列，具体到各种用法而言也是如此。当axis&#x3D;1时，如果是求平均，那么是从左到右横向求平均；如果是拼接，那么也是左右横向拼接；如果是drop，那么也是横向发生变化，体现为列的减少 axis &#x3D; 0 纵向处理 axis &#x3D; 1 横向处理 "},{"title":"信用卡交易数据解读与探索（数据合并）","date":"2022-03-21T07:01:44.023Z","url":"/2022/03/21/%E4%BF%A1%E7%94%A8%E5%8D%A1%E4%BA%A4%E6%98%93%E6%95%B0%E6%8D%AE%E8%A7%A3%E8%AF%BB/","categories":[["undefined",""]],"content":"数据分析首先还是对数据集进行解释，以及简单验证数据集的正确性。信用卡交易记录包括了两个数据集，分别是historical_transactions和new_merchant_transactions。两个数据集字段类似，只是记录了不同时间区间的信用卡消费情况： 这里的数据存在两个 一个18以前的数据集 一个18以后的 数据解读 首先简单查看有哪些字段一致： 并且我们进一步发现，交易记录中的merhcant_id信息并不唯一： 造成该现象的原因可能是商铺在逐渐经营过程动态变化，而基于此，在后续的建模过程中，我们将优先使用交易记录中表中的相应记录。 数据预处理 连续&#x2F;离散字段标注 首先也是一样，需要对其连续&#x2F;离散变量进行标注。当然该数据集中比较特殊的一点，是存在一个时间列，我们将其单独归为一类： 字段类型转换&#x2F;缺失值填补 "},{"title":"商户数据解读与探索(包含较为复杂的数据处理)","date":"2022-03-21T01:54:47.085Z","url":"/2022/03/21/%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E6%B8%85%E6%B4%97/","categories":[["undefined",""]],"content":"复杂的数据处理过程（含清洗）1. 数据解读 2. 数据探索 正确性检验 查看id出现次数是否唯一 缺失值分析 能够发现，第二个匿名分类变量存在较多缺失值，而avg_sales_lag3&#x2F;6&#x2F;12缺失值数量一致，则很有可能是存在13个商户同时确实了这三方面信息。其他数据没有缺失，数据整体来看较为完整。 3. 数据预处理3.1 离散&#x2F;连续字段标注由于商户数据集中特征同时存在分类变量和离散变量，因此我们首先可以根据字段的说明对不同属性特征进行统一的划分： 3.2 离散数据处理 离散变量数据情况 离散变量字典编码 接下来对离散变量进行字典编码，即将object对象类型按照sort顺序进行数值化（整数）编码。例如原始category_1取值为Y&#x2F;N，通过sort排序后N在Y之前，因此在重新编码时N取值会重编码为0、Y取值会重编码为1。以此类推。 需要注意的是，从严格角度来说，变量类型应该是有三类，分别是连续性变量、名义型变量以及有序变量。连续变量较好理解，所谓名义变量，指的是没有数值大小意义的分类变量，例如用1表示女、0表示男，0、1只是作为性别的指代，而没有1&gt;0的含义。而所有有序变量，其也是离散型变量，但却有数值大小含义，如上述most_recent_purchases_range字段，销售等级中A&gt;B&gt;C&gt;D&gt;E，该离散变量的5个取值水平是有严格大小意义的，该变量就被称为有序变量。 在实际建模过程中，如果不需要提取有序变量的数值大小信息的话，可以考虑将其和名义变量一样进行独热编码。但本阶段初级预处理时暂时不考虑这些问题，先统一将object类型转化为数值型。&#x3D;&#x3D;（object类型转换类型）&#x3D;&#x3D; 测试 3.3 连续变量数据探索 据此我们发现连续型变量中存在部分缺失值，并且部分连续变量还存在无穷值inf，需要对其进行简单处理。 无穷值处理 缺失值处理 不同于无穷值的处理，缺失值处理方法有很多。但该数据集缺失数据较少，33万条数据中只有13条连续特征缺失值，此处我们先简单采用均值进行填补处理，后续若有需要再进行优化处理。 "},{"title":"kaggle入门","date":"2022-03-20T11:02:48.709Z","url":"/2022/03/20/kaggle%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"环境安装和准备anaconda + jupyter 获取kaggle.json &amp;emsp;&amp;emsp;在安装完成kaggle之后，进入Kaggle的个人主页（点击右上角头像），点击Create New API Token，则可创建一个kaggle.json文件，并自动开始下载 ​ - 将kaggle.json文件移动到.kaggle文件夹内 安装内核使用anaconda虚拟环境作为jupyter notebook内核 删除内核 感谢： "},{"title":"深度推荐系统 下","date":"2022-03-19T07:31:56.636Z","url":"/2022/03/19/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(2)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（2）2.4 FM与深度学习模型2.4.1 FNN - 用FM的隐向量完成embedding层初始化 FNN相较于deep crossing模型的区别 对于embedding进行了改进，在模型初始化时引入了有价值的先验信息 在训练时特征被划分了不同的特征域，每个特征域有对应的embedding层 2.4.2 deepFM用FM代替wide部分 ​ ​ FM与深度模型的组合有两种，一种是二者并行，另一种是二者串行。DeepFM就是并行的一种结构。并行就是FM将输入部分计算完之后单独拿出来，得到一组特征表示，然后再利用深度模型（多层全连接）对输入部分进行告阶的特征组合。最后把二者的特征进行concact，得到一组特征，最后对这组特征进行分类或者回归。其实这只是特征的一种组合方式，目的就是为了得到特征的高阶表示。 2.4.3 总结特征工程在这条路上已经穷尽了可能性的尝试，模型的提升空间会非常小。但是很重要 2.5 注意力机制的应用 Attention机制的本质 attention机制的本质是从人类视觉注意力机制中获得灵感(可以说很‘以人为本’了)。大致是我们视觉在感知东西的时候，一般不会是一个场景从到头看到尾每次全部都看，而往往是根据需求观察注意特定的一部分。而且当我们发现一个场景经常在某部分出现自己想观察的东西时，我们就会进行学习在将来再出现类似场景时把注意力放到该部分上。这可以说就是注意力机制的本质内容了。至于它本身包含的‘自上而下’和‘自下而上’方式就不在过多的讨论。 Attention机制的理解 Attention机制其实就是一系列注意力分配系数，也就是一系列权重参数罢了。 2.5.1 AFM - 引入注意力机制的FM 注意力网络的作用是为每一个交叉特征提供权重 2.5.2 DIN basemodel DIN通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给定的广告）。 注意力在其上面的形式是激活单元来生成注意力得分"},{"title":"训练集和数据集数据探索","date":"2022-03-18T13:03:49.043Z","url":"/2022/03/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","categories":[["undefined",""]],"content":"数据分析 1. 数据解读 上面的操作就很easy 2. 数据质量分析​ 接下来简单数据探索。在实际建模过程中，首先我们会先校验数据的正确性，并检验缺失值、异常值等情况。 数据正确性校验 所谓数据正确性，指的是数据本身是否符合基本逻辑，例如此处信用卡id作为建模分析对象独一无二的标识，我们需要验证其是否确实独一无二，并且训练集和测试集信用卡id无重复。 判断缺失值情况 3. 异常值分析 describe()方法 异常值检验。由于我们尚未对数据集特征进行预处理，因此我们先查看标签列的异常值情况。首先我们可以用describe()方法查看这一列的基本统计信息： 通过直方图观察 由于是连续变量可以借助概率密度直方图进行分布的观察： $3\\delta$原则进行异常值识别 ​ 能够发现，大部分用户忠诚度评分都集中在[-10,10]之间，并且基本符合正态分布，唯一需要注意的是有个别异常值取值在-30以下，该数据在后续分析中需要额外注意。我们可以简单查看有多少用户的标签数值是小于30的： 当然，对于连续变量，一般可以采用$3\\delta$原则进行异常值识别，此处我们也可以简单计算下异常值范围： &amp;emsp;&amp;emsp;需要注意的是，此处我们是围绕标签进行的异常值检测，而本案例中标签并不是自然数值测量或统计的结果（如消费金额、身高体重等），而是通过某种公式人工计算得出（详见赛题分析）。出现如此离群点极有可能是某类特殊用户的标记。因此不宜进行异常值处理，而应该将其单独视作特殊的一类，在后续建模分析时候单独对此类用户进行特征提取与建模分析。 4. 规律一致性分析&amp;emsp;&amp;emsp;接下来，进行训练集和测试集的规律一致性分析。 &amp;emsp;&amp;emsp;所谓规律一致性，指的是需要对训练集和测试集特征数据的分布进行简单比对，以“确定”两组数据是否诞生于同一个总体，即两组数据是否都遵循着背后总体的规律，即两组数据是否存在着规律一致性。 &amp;emsp;&amp;emsp;我们知道，尽管机器学习并不强调样本-总体的概念，但在训练集上挖掘到的规律要在测试集上起到预测效果，就必须要求这两部分数据受到相同规律的影响。一般来说，对于标签未知的测试集，我们可以通过特征的分布规律来判断两组数据是否取自同一总体 单变量分析 当然，我们需要同时对比训练集和测试集的四个特征，可以通过如下代码实现： 多级联合分布 ​ 接下来，我们进一步查看联合变量分布。所谓联合概率分布，指的是将离散变量两两组合，然后查看这个新变量的相对占比分布。例如特征1有0&#x2F;1两个取值水平，特征2有A&#x2F;B两个取值水平，则联合分布中就将存在0A、0B、1A、1B四种不同取值水平，然后进一步查看这四种不同取值水平出现的分布情况。 ​ 实际建模过程中，规律一致性分析是非常重要但又经常容易被忽视的一个环节。通过规律一致性分析，我们可以得出非常多的可用于后续指导后续建模的关键性意见。通常我们可以根据规律一致性分析得出以下基本结论 ​ 作用： 如果分布非常一致，则说明所有特征均取自同一整体，训练集和测试集规律拥有较高一致性，模型效果上限较高，建模过程中应该更加依靠特征工程方法和模型建模技巧提高最终预测效果 如果分布不太一致，则说明训练集和测试集规律不太一致，此时模型预测效果上限会受此影响而被限制，并且模型大概率容易过拟合，在实际建模过程中可以多考虑使用交叉验证等方式防止过拟合，并且需要注重除了通用特征工程和建模方法外的trick的使用； "},{"title":"深度推荐系统 上","date":"2022-03-15T13:20:16.514Z","url":"/2022/03/15/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(1)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（1）演化方式： 改变神经网络的复杂程度 改变特征交叉方式 wide&amp;deep模型 FM深度版本 注意力机制与推荐系统结合 序列模型与推荐模型结合 强化学习与深度学习结合 2.1 神经网络复杂程度2.1.1 Auto-rec–单层神经网络推荐模型（easy） 通过自编码器原理，还原输入的结果。 重建函数： 目标函数： 目标函数l2正则： 参考： 2.1.2 Deep Crossing模型–经典深度学习架构 应用场景 网络结构 embedding层，stacking层，multiple residual units层，scoring层 ​ 反思 embedding+多层神经网络，相较于传统的二阶特征交叉能力，deep crossing拥有深度交叉的能力 2.1.2 NeuralCF - CF与深度学习的结合先回忆一下传统的矩阵分解怎么做 物品-用户共现矩阵分解成用户向量和物品向量 向量embedding化 embedding后向量取内积（重要） 得到分数 这个模型 复杂的位置就是在第三步操作上 使用多层神经网络去替换这个卷积操作 &#x3D;&#x3D;优势&#x3D;&#x3D; 利用神经网络来拟合任意函数，灵活地组成不同的特征，按需增加或减少模型的复杂度 &#x3D;&#x3D;劣势&#x3D;&#x3D; 基于协同过滤构造,没有引入更多其他类型的特征 在实践中，防止过拟合的风险 2.2 加强特征交叉能力2.2.1 PNN模型​ ​ 相较于 deep crossing模型中的stacking层，PNN模型替换成了乘积层。其他的在模型的输入，embeding层，多层神经网络以及最终的输出层上没有结构上的不同。 ​ product层，左边为线性部分，认为 特征之间的关系是and“且”的一种关系，而非add”加”的关系。 ​ z&#x3D;conca**t([emb1,emb2..,emb**n],axi**s&#x3D;1) 其右边操作为乘积操作，有内积和外积的区别。外积在操作上会将问题的复杂度从原来的m到 $m^2$,在选择上更应该慎重。 优势 ​ 定义了外积和内积操作更有针对性地强调不同特征之间的交互 局限 ​ 在外积操作上，为了效率经行大量的简化操作，对所有特征进行无差别的交叉，在一定程度上忽略了原始特征中包含的价值信息。 2.3 记忆能力与泛化能力的综合2.3.1 wide&amp;deep模型​ wide部分是让模型具有较强的“记忆能力”，deep部分是让模型具有泛化能力。这样的结构使模型兼具了逻辑回归和深度神经网络的优点–能快速处理并且记忆大量的历史行为特征，并且具有强大的表达能力。 在提出W&amp;D模型，平衡Wide模型和Deep模型的记忆能力和泛化能力。实际上是lr+dnn。记忆（memorization） 通过特征叉乘对原始特征做非线性变换，输入为高维度的稀疏向量。通过大量的特征叉乘产生特征相互作用的“记忆（Memorization）”，高效且可解释，但要泛化需要更多的特征工程。 泛化（generalization）只需要少量的特征工程，深度神经网络通过embedding的方法，使用低维稠密特征输入，可以更好地泛化训练样本中未出现过的特征组合。但当user-item交互矩阵稀疏且高阶时，容易出现“过泛化（over-generalize）”导致推荐的item相关性差 工程应用 2.3.2 wide&amp;deep进化 deep&amp;cross模型 Cross Network ​ 交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式： ​ xl和xl+1 分别是第l层和第l+1层cross layer的输出，wl和bl是这两层之间的连接参数。注意上式中所有的变量均是列向量，W也是列向量，并不是矩阵。xl+1 &#x3D; f(xl, wl, bl) + xl. 每一层的输出，都是上一层的输出加上feature crossing f。而f就是在拟合该层输出和上一层输出的残差。 ​ Cross Layer 设计的巧妙之处全部体现在上面的计算公式中，我们先看一些明显的细节：1) 每层的神经元个数都相同，都等于输入 的维度 DCN能够有效地捕获有限度的有效特征的相互作用，学会高度非线性的相互作用，不需要人工特征工程或遍历搜索，并具有较低的计算成本。1）提出了一种新的交叉网络，在每个层上明确地应用特征交叉，有效地学习有界度的预测交叉特征，并且不需要手工特征工程或穷举搜索。2）跨网络简单而有效。通过设计，各层的多项式级数最高，并由层深度决定。网络由所有的交叉项组成，它们的系数各不相同。3）跨网络内存高效，易于实现。4）实验结果表明，交叉网络（DCN）在LogLoss上与DNN相比少了近一个量级的参数量"},{"title":"传统推荐系统","date":"2022-03-14T13:48:48.181Z","url":"/2022/03/14/%E4%BC%A0%E7%BB%9F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","categories":[["undefined",""]],"content":"1. 推荐系统技术架构数据+模型 1.1 数据部分通过特征工程，将客户端或服务端采集到的数据进行特征处理 1.2 模型部分主题 一般由召回层，排序层，补充数据与算法层组成 2. 传统推荐模型（粗略整理）2.1 协同过滤分为 用户过滤和物品过滤 2.1.1 用户过滤（没人用）公式一 余弦相似度 公式二 皮尔逊相关系数（减少了用户评分的影响） 2.1.2 物品过滤 基于历史数据，构建用户-物品共现矩阵（m*n） 计算共现矩阵两两向量间的相似性 获得用户历史行为数据的正反馈物品列表 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的topK物品 对于相似度进行排序，生成最终的推荐列表 2.2 矩阵分解算法通过分解协同过滤生成的共现矩阵 得到用户和物品的隐向量 2.2.1 矩阵分解的求解方法梯度下降 目标函数 2.3 特征交叉单一特征的表达性 没有特征组合起来的表达性好 且 单一特征会损失一定量的信息 2.3.1 POLY2模型暴力将特征n个 变成了$ n^2 $ 会将数据更加稀疏 增加训练复杂度 2.3.2 FM模型-隐向量特征交叉特征交叉对于线性模型可以学习到非线性特征 隐向量 就是为每个特征 学习一个隐权重向量（latent vector） 交互使用两个向量取内积就好 优势 1. 权重参数减少到nk ​ 2. 训练复杂度降低到nk级别 2.3.3 FFM模型 特征域感知概念训练过程中，需要学习n个特征在f个域上的k维隐向量，参数 n * k * f 复杂度为 k$ n^2 $ 2.4 GBDT+LR 特征工程模型化 原始特征向量x，通过树分裂 将转化的特征类似于one-hot的向量来表示原始的特征，特征组合能力特别强 但是容易产生过拟合，以及这样的过程丢失了大量特征数值信息。 2.5 MLR 深度学习开始的曙光2.5.1 MLR与LR的区别 普通的LR模型 无法拟合我们所需的曲线 但是MLR模型正常拟合出来了 2.5.2 目标公式 如果m为1 则为普通的LR模型 当m越大 模型的拟合能力越强 而同样 需要的训练样本也变得更大 （阿里巴巴 经验12） 2.5.3 优点 端到端的非线性学习能力 模型稀疏性强 "},{"title":"centos安装","date":"2022-03-13T06:47:18.013Z","url":"/2022/03/13/1.%20centos%E5%AE%89%E8%A3%85/","categories":[["undefined",""]],"content":"个人主页  1. centos安装1.1 centos安装注意 1.2 ssh的连接 "}]