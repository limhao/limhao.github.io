[{"title":"Recommender-System-Pytorch 项目 函数指南","date":"2022-03-28T12:43:29.890Z","url":"/2022/03/28/Recommender-System-Pytorch%20%E9%A1%B9%E7%9B%AE%20%E5%87%BD%E6%95%B0%E6%8C%87%E5%8D%97/","categories":[["undefined",""]],"content":"个人觉得成熟的rebole的工具 不太适合萌新 来操作自己对于项目的建设 最近找了一个新的项目 来操作 感觉本项目 更贴近萌新到大佬写代码过程 于是乎 有了这篇指南 embedding操作0. 官方操作下面是官方例子 官方的解释： torch.nn.``Embedding(num_embeddings, embedding_dim, padding_idx&#x3D;None, max_norm&#x3D;None, norm_type&#x3D;2.0, scale_grad_by_freq&#x3D;False, sparse&#x3D;False, _weight&#x3D;None, device&#x3D;None, dtype&#x3D;None) num_embeddings：嵌入字典的大小（词的个数）； embedding_dim：每个嵌入向量的大小； padding_idx：若给定，则每遇到 padding_idx 时，位于 padding_idx 的嵌入向量（即 -padding_idx 映射所对应的向量）为0； max_norm：若给定，则每个大于 max_norm 的数都会被规范化为 max_norm； norm_type：为 max_norm 计算 p-范数的 p值； scale_grad_by_freq：若给定，则将按照 mini-batch 中 words 频率的倒数 scale gradients； sparse：若为 True，则 weight 矩阵将是稀疏张量。 1. 自己的瞎吉儿理解这里呀 就只需要理解好 前三个就好 对于前两个的理解 torch.nn.Embedding 的权重为 num_embeddings * embedding_dim 的矩阵，例如输入10个词，每个词用3为向量表示，则权重为10*3的矩阵； 对于 padding_idx 理解 可以看出 “6” 所对应映射的向量被填充了0。 网络初始化 1. 初始化函数 均匀分布torch.nn.init.uniform_(tensor, a&#x3D;0, b&#x3D;1)服从~U ( a , b ) U(a, b)U(a,b) 正太分布torch.nn.init.normal_(tensor, mean&#x3D;0, std&#x3D;1)服从~N ( m e a n , s t d ) N(mean, std)N(mean,std) 初始化为常数torch.nn.init.constant_(tensor, val)初始化整个矩阵为常数val Xavier 基本思想是通过网络层时，输入和输出的方差相同，包括前向传播和后向传播。具体看以下博文： 为什么需要Xavier 初始化？如果初始化值很小，那么随着层数的传递，方差就会趋于0，此时输入值 也变得越来越小，在sigmoid上就是在0附近，接近于线性，失去了非线性如果初始值很大，那么随着层数的传递，方差会迅速增加，此时输入值变得很大，而sigmoid在大输入值写倒数趋近于0，反向传播时会遇到梯度消失的问题 xavier初始化的简单推导 kaiming (He initialization) 以后再说 现在没用上 bug解决感谢涛哥 对于源码的修改 错误信息 在改错的时候注意看最后一行 即 修改在这里定位好 然后修改成： "},{"title":"推荐数据集处理（分桶） 重要","date":"2022-03-28T03:06:43.179Z","url":"/2022/03/28/%E6%8E%A8%E8%8D%90%E6%95%B0%E6%8D%AE%E9%9B%86%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"1. 探索数据集1.1 数据读入 1.2 查看数据 1.3 数据分类 1.4 查看缺失值情况（标注） 1.5 数据分箱 1.6 训练数据与标签分离 2. 字段维度（field_dim） 与 数据划分 2.1 字段维度获取（field_dim） 2.2 数据划分 "},{"title":"软著申请流程","date":"2022-03-27T02:32:53.849Z","url":"/2022/03/27/%E8%BD%AF%E8%91%97%E7%94%B3%E8%AF%B7/","categories":[["undefined",""]],"content":"感谢   "},{"title":"python中axis=0和axis=1的理解","date":"2022-03-25T01:45:41.345Z","url":"/2022/03/25/axis%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"原文链接： axis的重点在于方向，而不是行和列。1表示横轴，方向从左到右；0表示纵轴，方向从上到下。 即axis&#x3D;1为横向，axis&#x3D;0为纵向，而不是行和列，具体到各种用法而言也是如此。当axis&#x3D;1时，如果是求平均，那么是从左到右横向求平均；如果是拼接，那么也是左右横向拼接；如果是drop，那么也是横向发生变化，体现为列的减少 axis &#x3D; 0 纵向处理 axis &#x3D; 1 横向处理 "},{"title":"信用卡交易数据解读与探索（数据合并）","date":"2022-03-21T07:01:44.023Z","url":"/2022/03/21/%E4%BF%A1%E7%94%A8%E5%8D%A1%E4%BA%A4%E6%98%93%E6%95%B0%E6%8D%AE%E8%A7%A3%E8%AF%BB/","categories":[["undefined",""]],"content":"数据分析首先还是对数据集进行解释，以及简单验证数据集的正确性。信用卡交易记录包括了两个数据集，分别是historical_transactions和new_merchant_transactions。两个数据集字段类似，只是记录了不同时间区间的信用卡消费情况： 这里的数据存在两个 一个18以前的数据集 一个18以后的 数据解读 首先简单查看有哪些字段一致： 并且我们进一步发现，交易记录中的merhcant_id信息并不唯一： 造成该现象的原因可能是商铺在逐渐经营过程动态变化，而基于此，在后续的建模过程中，我们将优先使用交易记录中表中的相应记录。 数据预处理 连续&#x2F;离散字段标注 首先也是一样，需要对其连续&#x2F;离散变量进行标注。当然该数据集中比较特殊的一点，是存在一个时间列，我们将其单独归为一类： 字段类型转换&#x2F;缺失值填补 "},{"title":"商户数据解读与探索(包含较为复杂的数据处理)","date":"2022-03-21T01:54:47.085Z","url":"/2022/03/21/%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E6%B8%85%E6%B4%97/","categories":[["undefined",""]],"content":"复杂的数据处理过程（含清洗）1. 数据解读 2. 数据探索 正确性检验 查看id出现次数是否唯一 缺失值分析 能够发现，第二个匿名分类变量存在较多缺失值，而avg_sales_lag3&#x2F;6&#x2F;12缺失值数量一致，则很有可能是存在13个商户同时确实了这三方面信息。其他数据没有缺失，数据整体来看较为完整。 3. 数据预处理3.1 离散&#x2F;连续字段标注由于商户数据集中特征同时存在分类变量和离散变量，因此我们首先可以根据字段的说明对不同属性特征进行统一的划分： 3.2 离散数据处理 离散变量数据情况 离散变量字典编码 接下来对离散变量进行字典编码，即将object对象类型按照sort顺序进行数值化（整数）编码。例如原始category_1取值为Y&#x2F;N，通过sort排序后N在Y之前，因此在重新编码时N取值会重编码为0、Y取值会重编码为1。以此类推。 需要注意的是，从严格角度来说，变量类型应该是有三类，分别是连续性变量、名义型变量以及有序变量。连续变量较好理解，所谓名义变量，指的是没有数值大小意义的分类变量，例如用1表示女、0表示男，0、1只是作为性别的指代，而没有1&gt;0的含义。而所有有序变量，其也是离散型变量，但却有数值大小含义，如上述most_recent_purchases_range字段，销售等级中A&gt;B&gt;C&gt;D&gt;E，该离散变量的5个取值水平是有严格大小意义的，该变量就被称为有序变量。 在实际建模过程中，如果不需要提取有序变量的数值大小信息的话，可以考虑将其和名义变量一样进行独热编码。但本阶段初级预处理时暂时不考虑这些问题，先统一将object类型转化为数值型。&#x3D;&#x3D;（object类型转换类型）&#x3D;&#x3D; 测试 3.3 连续变量数据探索 据此我们发现连续型变量中存在部分缺失值，并且部分连续变量还存在无穷值inf，需要对其进行简单处理。 无穷值处理 缺失值处理 不同于无穷值的处理，缺失值处理方法有很多。但该数据集缺失数据较少，33万条数据中只有13条连续特征缺失值，此处我们先简单采用均值进行填补处理，后续若有需要再进行优化处理。 "},{"title":"kaggle入门","date":"2022-03-20T11:02:48.709Z","url":"/2022/03/20/kaggle%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"环境安装和准备anaconda + jupyter 获取kaggle.json &amp;emsp;&amp;emsp;在安装完成kaggle之后，进入Kaggle的个人主页（点击右上角头像），点击Create New API Token，则可创建一个kaggle.json文件，并自动开始下载 ​ - 将kaggle.json文件移动到.kaggle文件夹内 安装内核使用anaconda虚拟环境作为jupyter notebook内核 删除内核 感谢： "},{"title":"深度推荐系统 下","date":"2022-03-19T07:31:56.636Z","url":"/2022/03/19/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(2)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（2）2.4 FM与深度学习模型2.4.1 FNN - 用FM的隐向量完成embedding层初始化 FNN相较于deep crossing模型的区别 对于embedding进行了改进，在模型初始化时引入了有价值的先验信息 在训练时特征被划分了不同的特征域，每个特征域有对应的embedding层 2.4.2 deepFM用FM代替wide部分 ​ ​ FM与深度模型的组合有两种，一种是二者并行，另一种是二者串行。DeepFM就是并行的一种结构。并行就是FM将输入部分计算完之后单独拿出来，得到一组特征表示，然后再利用深度模型（多层全连接）对输入部分进行告阶的特征组合。最后把二者的特征进行concact，得到一组特征，最后对这组特征进行分类或者回归。其实这只是特征的一种组合方式，目的就是为了得到特征的高阶表示。 2.4.3 总结特征工程在这条路上已经穷尽了可能性的尝试，模型的提升空间会非常小。但是很重要 2.5 注意力机制的应用 Attention机制的本质 attention机制的本质是从人类视觉注意力机制中获得灵感(可以说很‘以人为本’了)。大致是我们视觉在感知东西的时候，一般不会是一个场景从到头看到尾每次全部都看，而往往是根据需求观察注意特定的一部分。而且当我们发现一个场景经常在某部分出现自己想观察的东西时，我们就会进行学习在将来再出现类似场景时把注意力放到该部分上。这可以说就是注意力机制的本质内容了。至于它本身包含的‘自上而下’和‘自下而上’方式就不在过多的讨论。 Attention机制的理解 Attention机制其实就是一系列注意力分配系数，也就是一系列权重参数罢了。 2.5.1 AFM - 引入注意力机制的FM 注意力网络的作用是为每一个交叉特征提供权重 2.5.2 DIN basemodel DIN通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给定的广告）。 注意力在其上面的形式是激活单元来生成注意力得分"},{"title":"训练集和数据集数据探索","date":"2022-03-18T13:03:49.043Z","url":"/2022/03/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","categories":[["undefined",""]],"content":"数据分析 1. 数据解读 上面的操作就很easy 2. 数据质量分析​ 接下来简单数据探索。在实际建模过程中，首先我们会先校验数据的正确性，并检验缺失值、异常值等情况。 数据正确性校验 所谓数据正确性，指的是数据本身是否符合基本逻辑，例如此处信用卡id作为建模分析对象独一无二的标识，我们需要验证其是否确实独一无二，并且训练集和测试集信用卡id无重复。 判断缺失值情况 3. 异常值分析 describe()方法 异常值检验。由于我们尚未对数据集特征进行预处理，因此我们先查看标签列的异常值情况。首先我们可以用describe()方法查看这一列的基本统计信息： 通过直方图观察 由于是连续变量可以借助概率密度直方图进行分布的观察： $3\\delta$原则进行异常值识别 ​ 能够发现，大部分用户忠诚度评分都集中在[-10,10]之间，并且基本符合正态分布，唯一需要注意的是有个别异常值取值在-30以下，该数据在后续分析中需要额外注意。我们可以简单查看有多少用户的标签数值是小于30的： 当然，对于连续变量，一般可以采用$3\\delta$原则进行异常值识别，此处我们也可以简单计算下异常值范围： &amp;emsp;&amp;emsp;需要注意的是，此处我们是围绕标签进行的异常值检测，而本案例中标签并不是自然数值测量或统计的结果（如消费金额、身高体重等），而是通过某种公式人工计算得出（详见赛题分析）。出现如此离群点极有可能是某类特殊用户的标记。因此不宜进行异常值处理，而应该将其单独视作特殊的一类，在后续建模分析时候单独对此类用户进行特征提取与建模分析。 4. 规律一致性分析&amp;emsp;&amp;emsp;接下来，进行训练集和测试集的规律一致性分析。 &amp;emsp;&amp;emsp;所谓规律一致性，指的是需要对训练集和测试集特征数据的分布进行简单比对，以“确定”两组数据是否诞生于同一个总体，即两组数据是否都遵循着背后总体的规律，即两组数据是否存在着规律一致性。 &amp;emsp;&amp;emsp;我们知道，尽管机器学习并不强调样本-总体的概念，但在训练集上挖掘到的规律要在测试集上起到预测效果，就必须要求这两部分数据受到相同规律的影响。一般来说，对于标签未知的测试集，我们可以通过特征的分布规律来判断两组数据是否取自同一总体 单变量分析 当然，我们需要同时对比训练集和测试集的四个特征，可以通过如下代码实现： 多级联合分布 ​ 接下来，我们进一步查看联合变量分布。所谓联合概率分布，指的是将离散变量两两组合，然后查看这个新变量的相对占比分布。例如特征1有0&#x2F;1两个取值水平，特征2有A&#x2F;B两个取值水平，则联合分布中就将存在0A、0B、1A、1B四种不同取值水平，然后进一步查看这四种不同取值水平出现的分布情况。 ​ 实际建模过程中，规律一致性分析是非常重要但又经常容易被忽视的一个环节。通过规律一致性分析，我们可以得出非常多的可用于后续指导后续建模的关键性意见。通常我们可以根据规律一致性分析得出以下基本结论 ​ 作用： 如果分布非常一致，则说明所有特征均取自同一整体，训练集和测试集规律拥有较高一致性，模型效果上限较高，建模过程中应该更加依靠特征工程方法和模型建模技巧提高最终预测效果 如果分布不太一致，则说明训练集和测试集规律不太一致，此时模型预测效果上限会受此影响而被限制，并且模型大概率容易过拟合，在实际建模过程中可以多考虑使用交叉验证等方式防止过拟合，并且需要注重除了通用特征工程和建模方法外的trick的使用； "},{"title":"深度推荐系统 上","date":"2022-03-15T13:20:16.514Z","url":"/2022/03/15/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(1)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（1）演化方式： 改变神经网络的复杂程度 改变特征交叉方式 wide&amp;deep模型 FM深度版本 注意力机制与推荐系统结合 序列模型与推荐模型结合 强化学习与深度学习结合 2.1 神经网络复杂程度2.1.1 Auto-rec–单层神经网络推荐模型（easy） 通过自编码器原理，还原输入的结果。 重建函数： 目标函数： 目标函数l2正则： 参考： 2.1.2 Deep Crossing模型–经典深度学习架构 应用场景 网络结构 embedding层，stacking层，multiple residual units层，scoring层 ​ 反思 embedding+多层神经网络，相较于传统的二阶特征交叉能力，deep crossing拥有深度交叉的能力 2.1.2 NeuralCF - CF与深度学习的结合先回忆一下传统的矩阵分解怎么做 物品-用户共现矩阵分解成用户向量和物品向量 向量embedding化 embedding后向量取内积（重要） 得到分数 这个模型 复杂的位置就是在第三步操作上 使用多层神经网络去替换这个卷积操作 &#x3D;&#x3D;优势&#x3D;&#x3D; 利用神经网络来拟合任意函数，灵活地组成不同的特征，按需增加或减少模型的复杂度 &#x3D;&#x3D;劣势&#x3D;&#x3D; 基于协同过滤构造,没有引入更多其他类型的特征 在实践中，防止过拟合的风险 2.2 加强特征交叉能力2.2.1 PNN模型​ ​ 相较于 deep crossing模型中的stacking层，PNN模型替换成了乘积层。其他的在模型的输入，embeding层，多层神经网络以及最终的输出层上没有结构上的不同。 ​ product层，左边为线性部分，认为 特征之间的关系是and“且”的一种关系，而非add”加”的关系。 ​ z&#x3D;conca**t([emb1,emb2..,emb**n],axi**s&#x3D;1) 其右边操作为乘积操作，有内积和外积的区别。外积在操作上会将问题的复杂度从原来的m到 $m^2$,在选择上更应该慎重。 优势 ​ 定义了外积和内积操作更有针对性地强调不同特征之间的交互 局限 ​ 在外积操作上，为了效率经行大量的简化操作，对所有特征进行无差别的交叉，在一定程度上忽略了原始特征中包含的价值信息。 2.3 记忆能力与泛化能力的综合2.3.1 wide&amp;deep模型​ wide部分是让模型具有较强的“记忆能力”，deep部分是让模型具有泛化能力。这样的结构使模型兼具了逻辑回归和深度神经网络的优点–能快速处理并且记忆大量的历史行为特征，并且具有强大的表达能力。 在提出W&amp;D模型，平衡Wide模型和Deep模型的记忆能力和泛化能力。实际上是lr+dnn。记忆（memorization） 通过特征叉乘对原始特征做非线性变换，输入为高维度的稀疏向量。通过大量的特征叉乘产生特征相互作用的“记忆（Memorization）”，高效且可解释，但要泛化需要更多的特征工程。 泛化（generalization）只需要少量的特征工程，深度神经网络通过embedding的方法，使用低维稠密特征输入，可以更好地泛化训练样本中未出现过的特征组合。但当user-item交互矩阵稀疏且高阶时，容易出现“过泛化（over-generalize）”导致推荐的item相关性差 工程应用 2.3.2 wide&amp;deep进化 deep&amp;cross模型 Cross Network ​ 交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式： ​ xl和xl+1 分别是第l层和第l+1层cross layer的输出，wl和bl是这两层之间的连接参数。注意上式中所有的变量均是列向量，W也是列向量，并不是矩阵。xl+1 &#x3D; f(xl, wl, bl) + xl. 每一层的输出，都是上一层的输出加上feature crossing f。而f就是在拟合该层输出和上一层输出的残差。 ​ Cross Layer 设计的巧妙之处全部体现在上面的计算公式中，我们先看一些明显的细节：1) 每层的神经元个数都相同，都等于输入 的维度 DCN能够有效地捕获有限度的有效特征的相互作用，学会高度非线性的相互作用，不需要人工特征工程或遍历搜索，并具有较低的计算成本。1）提出了一种新的交叉网络，在每个层上明确地应用特征交叉，有效地学习有界度的预测交叉特征，并且不需要手工特征工程或穷举搜索。2）跨网络简单而有效。通过设计，各层的多项式级数最高，并由层深度决定。网络由所有的交叉项组成，它们的系数各不相同。3）跨网络内存高效，易于实现。4）实验结果表明，交叉网络（DCN）在LogLoss上与DNN相比少了近一个量级的参数量"},{"title":"传统推荐系统","date":"2022-03-14T13:48:48.181Z","url":"/2022/03/14/%E4%BC%A0%E7%BB%9F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","categories":[["undefined",""]],"content":"1. 推荐系统技术架构数据+模型 1.1 数据部分通过特征工程，将客户端或服务端采集到的数据进行特征处理 1.2 模型部分主题 一般由召回层，排序层，补充数据与算法层组成 2. 传统推荐模型（粗略整理）2.1 协同过滤分为 用户过滤和物品过滤 2.1.1 用户过滤（没人用）公式一 余弦相似度 公式二 皮尔逊相关系数（减少了用户评分的影响） 2.1.2 物品过滤 基于历史数据，构建用户-物品共现矩阵（m*n） 计算共现矩阵两两向量间的相似性 获得用户历史行为数据的正反馈物品列表 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的topK物品 对于相似度进行排序，生成最终的推荐列表 2.2 矩阵分解算法通过分解协同过滤生成的共现矩阵 得到用户和物品的隐向量 2.2.1 矩阵分解的求解方法梯度下降 目标函数 2.3 特征交叉单一特征的表达性 没有特征组合起来的表达性好 且 单一特征会损失一定量的信息 2.3.1 POLY2模型暴力将特征n个 变成了$ n^2 $ 会将数据更加稀疏 增加训练复杂度 2.3.2 FM模型-隐向量特征交叉特征交叉对于线性模型可以学习到非线性特征 隐向量 就是为每个特征 学习一个隐权重向量（latent vector） 交互使用两个向量取内积就好 优势 1. 权重参数减少到nk ​ 2. 训练复杂度降低到nk级别 2.3.3 FFM模型 特征域感知概念训练过程中，需要学习n个特征在f个域上的k维隐向量，参数 n * k * f 复杂度为 k$ n^2 $ 2.4 GBDT+LR 特征工程模型化 原始特征向量x，通过树分裂 将转化的特征类似于one-hot的向量来表示原始的特征，特征组合能力特别强 但是容易产生过拟合，以及这样的过程丢失了大量特征数值信息。 2.5 MLR 深度学习开始的曙光2.5.1 MLR与LR的区别 普通的LR模型 无法拟合我们所需的曲线 但是MLR模型正常拟合出来了 2.5.2 目标公式 如果m为1 则为普通的LR模型 当m越大 模型的拟合能力越强 而同样 需要的训练样本也变得更大 （阿里巴巴 经验12） 2.5.3 优点 端到端的非线性学习能力 模型稀疏性强 "},{"title":"centos安装","date":"2022-03-13T06:47:18.013Z","url":"/2022/03/13/1.%20centos%E5%AE%89%E8%A3%85/","categories":[["undefined",""]],"content":"个人主页  1. centos安装1.1 centos安装注意 1.2 ssh的连接 "}]