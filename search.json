[{"title":"5月4号 这一周学习总结","date":"2023-05-04T13:40:16.666Z","url":"/2023/05/04/54%E5%8F%B7-%E8%BF%99%E4%B8%80%E5%91%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","categories":[["undefined",""]],"content":"目前想了想 现在是2023年5月4日21点22分 我应该在每天9点多的时候 给自己一天的学习来个总结 今天是第一天 也算是那种小日记那样子 5&#x2F;4号 对比学习 今天看了一整天的对比学习   具体来说 我想了想自己的论文 觉得缺一部分内容 我想试着添加些对比学习的内容在之中 最后选定的方法是 simsiam 然后 数据增强用随机颜色变换 加上 随机裁剪 目前看到好用的方法是 moco simclr BYOL SIMSIAM 明天去看看有监督的对比学习方法 senet 介绍 今天 又去看了看senet的解释 解释1 挤压和激励(SE) 模块用于对通道之间的相互依赖性进行建模并对特征进行重新加权通道间的信息将被选择，大大提高了模型的效率 解释2 表明重新校准特征的通道重要性确实提高了性能 这玩意就是一个重新校准通道重要性的东西 simsiam 5&#x2F;5 5&#x2F;6这两天 哇 身体不好 外加帮忙去了 对比学习总结 以下为三个范式 自监督学习 对比学习 有标签的监督学习（） 一个总结比较好的自监督对比学习综述 可以算是李沐那个视频的文字版  对比损失的性质以及温度系数的用法（多看看评论区）  损失函数设计 看明白这个论文  小插曲 和师兄的聊天 "},{"title":"wandb 项目修改教程","date":"2023-04-21T01:32:27.806Z","url":"/2023/04/21/wandb%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/","categories":[["undefined",""]],"content":"0. 前言距离上次发博客 是4&#x2F;6号 现在 过了快两周 差不多是我这篇新的论文 验证完思路 效果好 且 跟师兄讨论的一个时间点 wandb 来找找超参 之前学过一些 链接：   自己的问题是 在一个已经能跑好的模型 添加wandb sweep 来 搜索超参 这个跑好的模型存在Namespace 命名空间 来看看 代码的修改 1. 来看看官方要我们加什么 需要添加什么？ sweep config sweep id 在main函数里面init() 好 且给超参赋值 log好我们需要记录的值 开始工作 wandb.agent(sweep_id, function&#x3D;main, count&#x3D;4) 2. 原样我的train文件 有 一个类 一个方法 一个main 入口 其中的main 函数里面有 Namespace 命名空间 其实 一开始 是打算将 参数搜索的字典放在main入口里面的 但是 我 找了很多方法 就是config打印不出来结果 3. 解法看了看韩同学的代码 有可能是 pykt的代码 很牛 先交代好参数 将main入口 写成一个函数 def main(): 参数的传递 通过修改 parser 中default来修改 设置好wandb 需要log的值 yo 然后将 将执行函数 写入 wandb.agent(sweep_id, function&#x3D;main, count&#x3D;4) 4. 总结一共四步 定义config 将之前的main调用改成 main方法 修改好命名空间内的default中的值 设置好wandb需要log的值 在main调用内 写wandb.agent 最后 用nohup保护进程 "},{"title":"年度总结","date":"2023-04-06T02:35:30.235Z","url":"/2023/04/06/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/","categories":[["undefined",""]],"content":"距离自己第一次发文 有了快一年的时间 欸 完完整整100篇文章 我将我的文章分成了六个大类 其中 data 类似于数据集解读 以及数据集观察 有8篇 方法类：有9个 技巧类：类似于工具 以及 一些加速方法 9个 文章：29篇 文章撰写类：这个类别有点水 19篇 项目：包含我完成的项目以及未完成的 共两个 其实回想到去年的三月份 无无聊聊 那时候应该是在帮王老师在写论文 那时候的我处于 one-hot 到 embedding的过程还不怎么理解的那时候的我 应该是在读序列相关的文章 那时候还被逼着给老师在读知识追踪方向的文章 看了看自己那时候的caser文章总结 欸 刚刚打开看了看 是个空的 但还是记录了个日期 5.14号 那时候就在想 这么优秀的结构的文章 就没什么人愿意去读 通过梳理时间线 这一年的历程应该是 帮王老师写论文 在阅读推荐相关的文章 对于代码层面的理解 数据集的处理过程 对于比赛相关的方面的理解 视频搜索方面的项目撰写 （已经成github啦） 大数据相关知识的学习 知识追踪方面的学习 观察他人的文章的撰写方式 自己的论文成文 未来的展望我脑子现在就一个事儿 完成基于caser文章的创新（如同一个想了快一年的梦 应该要生根发芽了） 先做 其他的事儿 我先不想"},{"title":"wandb 可视化调参","date":"2023-04-06T02:17:40.868Z","url":"/2023/04/06/wandb-%E5%8F%AF%E8%A7%86%E5%8C%96%E8%B0%83%E5%8F%82%EF%BC%88%E4%BB%A3%E7%A0%81%E6%B2%A1%E8%B7%91%E9%80%9A%EF%BC%89/","categories":[["undefined",""]],"content":"主从的结构 老板和员工 三步 开始之前 配置sweep config 选择一个调优算法 俺们这边选择random算法 定义优化目标 定义超参空间 超参空间分成三种 定义剪枝策略（可选） 后言 可以通过pprint 来看看我们的config写的好不好 初始化sweep controller（老板初始化） 启动sweep agent（开始干活）需要将模型训练写成一个函数 将模型训练 最后打包成一个函数 最后 可视化平行坐标系图 通过该图 我们可以知道具体的超参数 对于模型结果的影响 参数重要性 "},{"title":"caser环境使用方法","date":"2023-04-05T14:15:15.062Z","url":"/2023/04/05/caser_readme/","categories":[["undefined",""]],"content":"文章开始 怎么切换数据集文章中提供了四个数据集 但是 我这边找了好半天找到了两个（ml1m gowalla）这两个 然后我按照数据结构 自己整了个 Foursquare 这个数据集、 这个数据集 是不能对应原论文中的效果的 针对于数据集切换 train_root test_toot 里面修改下就好了 我自己怎么整的数据集呢看看文件夹 my_work 俺自己整的方法 这个里面"},{"title":"recbole 安装","date":"2023-03-24T13:53:15.114Z","url":"/2023/03/24/recbole-%E5%AE%89%E8%A3%85/","categories":[["undefined",""]],"content":"最近发现安装方法挺难受的 官网有些的错 时间 2023&#x2F;3&#x2F;24 记一下 代码过程吧 anaconda 创建recbole环境 自己遇到的bug其实还缺两个库没装 "},{"title":"NAS 综述 （2019 Thomas Elsken）","date":"2023-03-24T02:44:34.573Z","url":"/2023/03/24/NAS-%E7%BB%BC%E8%BF%B0-%EF%BC%882019-Thomas-Elsken%EF%BC%89/","categories":[["undefined",""]],"content":"前言文章讲了三个点 搜索空间 搜索策略 性能评估 搜索空间定义了原则上可以表示哪些体系结构。结合关于非常适合于任务的体系结构的典型属性的先验知识可以减小搜索空间的大小并且简化搜索。**但是，这也引入了人类的偏见，这可能会阻止发现超出当前人类知识的新颖的架构构建块。 ** 搜索策略详细说明了如何探索搜索空间（搜索空间通常是指数级的，甚至是无界的）。它包含了经典的探索-利用权衡，因为**一方面，希望快速找到性能良好的架构，而另一方面，应避免过早收敛到次优架构区域 ** NAS的目标通常是找到能够对不可见数据实现高预测性能的体系结构。性能评估是指评估该性能的过程：最简单的选择是对数据执行体系结构的标准训练和验证，但不幸的是，这在计算上是昂贵的，并且限制了可以探索的体系结构的数量。因此，许多最近的研究集中于开发降低这些性能估计的成本的方法。 搜索空间搜索空间由下面公式参数化： 最大层数 每一层 能执行的操作（pooling conv） 关于操作的超参数 网络结构 链式网络 如左图 残差网络 如右图 L0 L8 残差添加在一起 denseNet l10 基于细胞的搜索空间设定重复的单元 为 单一的细胞或者块 来堆叠模型 优点 搜索空间的大小急剧减小，因为单元通常由比整个体系结构少得多的层组成。 通过简单地改变模型中使用的单元和过滤器的数量，从单元构建的架构可以更容易地转移或适应其他数据集。 通过重复构建块来创建体系结构已被证明是一种有用的设计 问题 一个新的设计选择，即如何选择宏观体系结构 宏观结构如何构建 使用多少个细胞 如何连接细胞 在微观下 细胞的结构是如何构建 搜索策略许多不同的搜索策略可以用来探索神经结构的空间，包括随机搜索、贝叶斯优化、进化方法、强化学习(RL)和基于梯度的方法。 贝叶斯优化无聊 没意思 RL神经体系结构的生成可以被认为是Agent的动作，动作空间与搜索空间相同。 代理的奖励是基于对训练过的体系结构在未见数据上的性能的估计（参见第4节）。 不同的RL方法在如何表示代理的策略以及如何优化策略方面有所不同： 进化进化算法进化一群模型，即一组（可能训练过的）网络； 在每一个进化步骤中，至少从种群中抽取一个模型，并作为父母通过对其施加突变来产生后代。 在NAS环境中，突变是局部操作，例如添加或删除一个层，改变一个层的超参数，添加跳过连接，以及改变训练超参数。 在训练后代后，评估它们的适应度（例如，在验证集上的表现），并将它们添加到种群中。 神经进化的方法在如何取样父母，更新种群和产生后代方面有所不同。 结论结论是RL和进化在最终测试精度方面表现同样好，进化具有更好的随时性能和发现更小的模型。 性能估算策略从头开始对每个体系结构进行评估的训练通常会产生NAS数千GPU天的计算需求 第一种 我的总结是缩小数据量 缩小训练epoch 数据子集 缩小模型规模 缩小数据 第二种 曲线外推通过一开始的曲线 来提前取消一些垃圾曲线 建议外推初始学习曲线并终止那些预测表现不佳的曲线，以加快架构搜索过程。 （剪枝？） 预测神经结构性能的主要挑战是，为了加快搜索过程，在相对较大的搜索空间中需要基于相对较少的评估来做出良好的预测。 第三种 权重遗传&#x2F;网络态射 它们不是从零开始训练模型，而是通过继承权重来进行热启动 态射：看看图！ 第四种 一次性模型&#x2F;重量分摊（GPU可行性研究）只有一次性模型才需要训练，他的权重在不同的架构中共享 这些架构只是一次性模型的子图。 One-shot Architecture Search（参见图4）将所有架构视为一个超图（One-shot Model）的不同子图，并在具有该超图共同边的架构之间共享权重 这大大加快了体系结构的性能评估，因为不需要任何训练（只需要在验证数据上评估性能），同样导致了只需要几个GPU天的方法。 一次性模型通常会产生很大的偏差，因为它严重低估了最佳架构的实际性能； 然而，它允许对体系结构进行排序，如果估计的性能与实际性能密切相关，这就足够了。 单次NAS的一个普遍缺陷是先验定义的超图将搜索空间限制在子图上。 此外，要求在架构搜索期间整个超图驻留在GPU存储器中的方法将被限制在相对较小的超图和相应的搜索空间中，因此通常与基于单元的搜索空间结合使用。 "},{"title":"nas可微方法","date":"2023-03-22T14:21:02.189Z","url":"/2023/03/22/nas%E5%8F%AF%E5%BE%AE%E6%96%B9%E6%B3%95/","categories":[["undefined",""]],"content":"CVPR这个方法跟DARTS方法很像 前言 假如说 一个CNN 一般有几个参数 卷积核大小 strip的个数 假如说 咱们上面两个超参数都有三个选项 那一共就有9种 假如说 咱们一共有20层 那么 我们的可能性就是 $$9^{20}$$ ok 大致情况说明白了 super net 来看看一个层输入一个x 输出 9个模块处理的效果 然后通过这九个模块的处理结果 加权和 就能输出z 针对于 每个层来说 可训练的参数 有 每个模块的权重参数 还有 9个加权和的权重 而针对于 层与层之间 层内的参数是不共享的 总的来看 模型训练 这样 就能确定要 我们需要的路径 还有一条路（考虑时延）latency 延迟 模块的平均时延 总的时延 实验添加损失函数的方法 总的来看 "},{"title":"wandb 可视化调参","date":"2023-03-21T14:13:41.181Z","url":"/2023/03/21/wandb-%E5%8F%AF%E8%A7%86%E5%8C%96%E8%B0%83%E5%8F%82/","categories":[["undefined",""]],"content":"主从的结构 老板和员工 三步 开始之前 配置sweep config 选择一个调优算法 俺们这边选择random算法 定义优化目标 定义超参空间 超参空间分成三种 定义剪枝策略（可选） 后言 可以通过pprint 来看看我们的config写的好不好 初始化sweep controller（老板初始化） 启动sweep agent（开始干活）需要将模型训练写成一个函数 将模型训练 最后打包成一个函数 最后 可视化平行坐标系图 通过该图 我们可以知道具体的超参数 对于模型结果的影响 参数重要性 "},{"title":"wandb 使用教程（真的好用）","date":"2023-03-20T08:11:15.651Z","url":"/2023/03/20/wandb-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%EF%BC%88%E7%9C%9F%E7%9A%84%E5%A5%BD%E7%94%A8%EF%BC%89/","categories":[["undefined",""]],"content":"0 前言相比TensorBoard，wandb具有如下主要优势： 日志上传云端永久存储，便于分享不怕丢失 可以存管代码数据集和模型的版本，随时复现。(wandb.Artifact) 可以使用交互式表格进行case分析(wandb.Table) 可以自动化模型调参。(wandb.sweep) wandb 的平台 能干啥？ 核心功能有4个 实验跟踪:experiment tracking (wandb.log) 版本管理:version management (wandb.log_artifact, wandb.save) case分析:case visualization (wandb.Table, wandb.Image) 超参调优:model optimization (wandb.sweep) 实验跟踪（experiment tracking）先总结下 欸 自己的想法 metrics记录 针对于跑过的记录 在模型中 可以添加图 来展现重要数据 在table 中 通过pin数据 来关注重要数据 然后 在之前也能展现出来 版本管理–最终成果保存 log_artifact 来保存任务关联的重要成果 注:artifact翻译为”工件”，是指软件开发中产出的最终成果 版本管理 存在于三种 数据集 模型文件 模型权重 在外面的大循环上 可以看到经典的 init，finish这样的操作 在save里面 case分析 定义辅助函数 不是必须的 开始重要的了 作为table 是可以添加 数据的 add_data 最后两步 "},{"title":"最近想写的个东西","date":"2023-03-17T13:36:00.566Z","url":"/2023/03/17/%E6%9C%80%E8%BF%91%E6%83%B3%E5%86%99%E7%9A%84%E4%B8%AA%E4%B8%9C%E8%A5%BF/","categories":[["undefined",""]],"content":"通过读取你的收藏夹 然后缓存到云盘 最后 如果说这个视频没了 你将会得到一个注意 就是这个视频没了 会给你一个提示 主要是最近 这个东西搞得挺烦的说 就是你收藏的视频 欸 东西没了 有时候 就像是 你收集好的一个东西 突然没了 就挺难受 目前能想到的应用形式 刚刚突然发现 收藏界面f12 能进去 缓存好的视频 如果发现东西没了 就可以试试 将缓存好的视频 建立个小站 然后上传上去 查一查 现在市面上的 能缓存b站视频的软件的源代码 有意思哦 "},{"title":"t-sne指南","date":"2023-03-17T12:15:46.900Z","url":"/2023/03/17/t-sne%20%E7%9A%84%E6%8C%87%E5%8D%97/","categories":[["undefined",""]],"content":"前言2-27 3-17 这个星期三发的文章给老师看 距离上次发文章快20天了 差不多这20天 自己重新把文章又写了一遍 主要是发现 文章不能写太久了 写太久的话 文章会忘掉好多东西 在 写完这篇之后 随便整理个文章框架出来 很不错 为什么会用到这个东西这篇论文是一个多任务相关的推荐 采取的交叉压缩单元 通过交叉压缩两个向量 来保证两个特征之间的相关性 即 为了考虑到相关性 我考虑是不是可以使用t-sne来展现特征两者的相关性 于是乎 就出现了这么个我学习的东西 学习到了什么？参考 链接  模型的权重文件 在模型save的时候 要注意下面这个 注意不要是save_dict() 否者模型无法验证 看了看aitm 里面 出现了 sample正负样本的方法 在本次使用的是下面这个 sample 函数真好用 但是 下次看下 是不是 sample函数是不是可以给些限制条件 来保证sample出的正负样本 sklearn 中 tsne 函数奇奇怪怪的点 说实话 真的有人看的懂这个东西？ 我当时还问了chatgpt sklearn 的中文化教程是真的差 csdn也没找到 其中 得到的解答是 perplexity 这个超参数的值 大于了 n_samples 的值 soga 立马 改了 但是 结果是真的奇怪 或者 t-sne 不太合适去展现中间的过程吧 去找找 下一步的思考 找找什么方法可以较好的展现中间结果 试一试三维的t-sne 估计又得学一会 我对于模型的权重文件的理解 应该是将四个结果展现出来 我画个图！ 最后使用三维图将结果弄出来 但我感觉 我想了想内积 来保证权重的相似性 多想想 代码整个流程"},{"title":"如何缝论文 缝完该怎么写","date":"2023-02-27T04:52:02.293Z","url":"/2023/02/27/%E5%A6%82%E4%BD%95%E7%BC%9D%E8%AE%BA%E6%96%87-%E7%BC%9D%E5%AE%8C%E8%AF%A5%E6%80%8E%E4%B9%88%E5%86%99/","categories":[["undefined",""]],"content":"有一个基准模型A，然后随便找两篇论文B，C把B、C里的思想b、c缝到A里面来A+B+C how 最差 就是写的时候，直接告诉别人，我在A的基础上用了B（别人做的B）和C 最好的 相关工作提高了A B C A 不管，A+B+C&#x3D;D完全不等同于A 我设计了一个D 里面包括了 A”,B”,C” B，最好的做法是不要完全相同，加一点点东西变成B” 我设计了一个B，仿照B来写 创新点"},{"title":"ai模块缝合实操","date":"2023-02-27T03:22:33.497Z","url":"/2023/02/27/ai%E6%A8%A1%E5%9D%97%E7%BC%9D%E5%90%88%E5%AE%9E%E6%93%8D/","categories":[["undefined",""]],"content":"a论文， A模块（）b论文 ，B模块A+B 你把你的注意力放在你要添加注意力的位置（红框框） 对于过分的细节 不需要特别把握 before x1 -》 MHA(x1) -》 x1“ after "},{"title":"答疑 数据集和创新点","date":"2023-02-26T14:05:15.552Z","url":"/2023/02/26/%E7%AD%94%E7%96%91%20%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E5%88%9B%E6%96%B0%E7%82%B9/","categories":[["undefined",""]],"content":"大小数据集问题 找不到创新点咋办"},{"title":"一文三用 小论文大论文综述区别","date":"2023-02-26T13:33:53.401Z","url":"/2023/02/26/%E4%B8%80%E6%96%87%E4%B8%89%E7%94%A8-%E5%B0%8F%E8%AE%BA%E6%96%87%E5%A4%A7%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0%E5%8C%BA%E5%88%AB/","categories":[["undefined",""]],"content":"综述 无所谓如果你要定好了方向，一定要去看几篇该方向的综述 综述讲解了你这个方向的近几年的技术的发展情况 (脉络) 新技术(以一种有逻辑的方式概述给你听)–》类似于小论文里的相关工作 高中生都可以写一篇 综述 把最近 3年的该方向的文章全部找出来，分类，概述每一篇论文 综述这东西，除非你读博，否则这东西对你而言一点用都没用 小论文 和 毕业论文的区别小论文要创新，不需要工作量，不需要你详细的解释，通篇你要解释你创新的点是什么，做出了什么成绩，这件事对你这个方向和你这个领域有什么贡献，当然，水一篇论文其实没那么多要求 (sci四区)毕业论文(硕士):需要工作量 (大量的工作量)，有创新更好，没有你也不能太差。记录你三年干了啥。背景技术路线(基础方法)你用了什么方法，比如说我做了目标检测、知识图谱、推荐系统、命名实体识别小论文要知道，你通过这四个东西的叠加完成了一件什么样的事情 毕业论文，你把四个点都写进去，然后在告诉别人 你做了个什么事情（你随便造个理由就好了），客观阐述你做了什么东西，工作量尽量大一点，多一点 小论文 7000建议，仿照（同行）写五篇 毕业论文 3-5w最好有一篇小论文，然后把每个点细节再细节 如果没有小论文，你把你一年半的工作给阐释清楚，最好再来点性能上的提升。"},{"title":"摘要和引言怎么写","date":"2023-02-26T06:41:16.340Z","url":"/2023/02/26/%E6%91%98%E8%A6%81%E5%92%8C%E5%BC%95%E8%A8%80%E6%80%8E%E4%B9%88%E5%86%99/","categories":[["undefined",""]],"content":"Abstract（摘要）相关工作的概述(你为什么要做这件事)，引言的概述(你解决什么问题，使劲吹)，方法的概述(你用什么方法解决了这个问题) ，我做的性能很好 conclude（总结）方法 你做了什么东西 你做的东西达到了什么样的效果（实验分析） 我做的很好 未来有什么能改进的地方"},{"title":"期刊分区（俺自己的）","date":"2023-02-26T02:25:21.114Z","url":"/2023/02/26/%E6%9C%9F%E5%88%8A%E5%88%86%E5%8C%BA%EF%BC%88%E4%BF%BA%E8%87%AA%E5%B7%B1%E7%9A%84%EF%BC%89/","categories":[["undefined",""]],"content":"前言两步走 确认领域有什么期刊 确认期刊难度 第一步 可以使用百度学术 以及 谷歌学术 第二步 使用  letpub去看看 中文下 小木虫 作为 小领域 你要确定好你的领域英文名字叫什么 比如说 我这边 RS KT KG 都是我们论文的缩写 未来你要投什么期刊 获取期刊难度 二区开始 时间赶的话 往三四区去投"},{"title":"洗稿 怎么洗 模型效果提升技巧","date":"2023-02-26T02:07:05.366Z","url":"/2023/02/26/%E6%B4%97%E7%A8%BF%20%E6%80%8E%E4%B9%88%E6%B4%97%20%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C%E6%8F%90%E5%8D%87%E6%8A%80%E5%B7%A7/","categories":[["undefined",""]],"content":"我们在文章里引用了一个模块a (别人的) 和一个模块b 在自己的论文方法里简单概述a就可以了 简单概述b 对模块a稍微做一点点改进(可以无用，因为你不需要做消融实验表达出来)，A（给个新名字 叫大A） 相关工作里，引言里简单提一句，我们的A基于a进行了改进 在自己的论文方法里我们设计了一个A按照a的流程去写 (中间一定是有一点点异同的) 换显卡（显卡不同 效果不同） 换种子（找结果最差的种子） "},{"title":"期刊和会议的区别 如何找创新","date":"2023-02-25T02:25:59.114Z","url":"/2023/02/25/%E6%9C%9F%E5%88%8A%E5%92%8C%E4%BC%9A%E8%AE%AE%E7%9A%84%E5%8C%BA%E5%88%AB%20%E5%A6%82%E4%BD%95%E6%89%BE%E5%88%9B%E6%96%B0/","categories":[["undefined",""]],"content":"期刊时间周期特别长 水会，ccfc即可 会议一般三个月出结果 7月份截至投稿，10月份开会 不仅时间短，而且容易 期刊一般11页+，他要你提出为什么要做这个 会议论文一般7页+，它只要你回答称做了什么创新即可，他不需要写相关工作 很多东西 你不需要写上去 水论文如何找创新水论文，如何，找 (思考) ，创新创新:开创，新颖找:已经发生过的事情，已有的论文里面相似领域找 视频描述 密集视频描述 单句视频描述 图片描述 就是把别人的东西，可以借鉴的（因为领域不同，就不一定能借鉴了，那你就换文章），原封不动的搬过来，你只需要引用就好 改吧改吧(搬过来的时候，你没有那么强的能力去原封不动的搬） 类似领域 需要有类似的问题 写论文的时候，我就模仿 b 写，b 领域有什么问题，a 领域也有什么问题呢? a 领域一定是有人做了这个的去解决a领域的问题(穿插引入 b 文章) b 领域里面有 b1 创新，还有 b2 创新，然后我把 b1+b2 一起加到我的 a 领域来，只要你换中说法，谁能看出来你是从b 领域把 b1和 b2 搬过来的 ??????即使你思考出来了，你能做出来吗? 找不到相关领域怎么办那你就在自己的领域找，我找了 a1，我放到我的基准模型上，同样的道理，那就a1 + a2&#x3D;anb，出现了什么问题，出现了 1，a1 和 2，a2 两个问题，然后我用anb解决了 道德一点，讲自己是基于这个基准模型做的，会说自己是参考a1和a2做的 真正的创新创新来源于思考，是你对这个领域有所见解后，提出了自己的看法(去解决)，遇到了该领域的问题(然后去解决) 问题有二 能不能做出来 然后怎么编故事 up举例 举了个 图片描述的例子 即 通过 图片描述 转换成 视频描述 将视频 看成一帧一帧的图片 按道理来说 十分容易修改 可以试着找作者要要代码什么的"},{"title":"期刊和会议的区别","date":"2023-02-25T02:25:18.683Z","url":"/2023/02/25/%E6%9C%9F%E5%88%8A%E5%92%8C%E4%BC%9A%E8%AE%AE%E7%9A%84%E5%8C%BA%E5%88%AB/","categories":[["undefined",""]]},{"title":"怎么水论文 水论文路线图","date":"2023-02-24T14:44:57.709Z","url":"/2023/02/24/%E6%80%8E%E4%B9%88%E6%B0%B4%E8%AE%BA%E6%96%87-%E6%B0%B4%E8%AE%BA%E6%96%87%E8%B7%AF%E7%BA%BF%E5%9B%BE/","categories":[["undefined",""]],"content":"定基准模型，如果有继承，3天 首先得找大量的论文，甚至是顶刊顶会，大概率会给代码，且有readme，代码有系统性，方便以后改动，得花好久 有了基准模型之后，尽量把这个基准模型的模型部分给看懂，至于数据集处理和训练没必要 定思想，改进的思想 找更多的论文（ccf-c） 去找灵感，不如说去尝试，不断地加入小模块进入你的基准模型，没用，调参，调参没用，换一个 然后开始，想办法编故事 三步走 定基准模型 定思想 编故事 "},{"title":"顶刊看多了 你就能发论文嘛","date":"2023-02-24T14:00:22.727Z","url":"/2023/02/24/%E9%A1%B6%E5%88%8A%E7%9C%8B%E5%A4%9A%E4%BA%86%20%E4%BD%A0%E5%B0%B1%E8%83%BD%E5%8F%91%E8%AE%BA%E6%96%87%E5%98%9B/","categories":[["undefined",""]],"content":"你只是想水论文，你的期待 你老师的期待，你发一篇二区+的论文，然后一作挂他的名字顶刊:找到问题，解决问题 (问题比较新颖) 你(不太可能)也能找到新的问题，即使找到了，你基本上也没有那能力去解决 人会犯困，找到一个东西让人不犯困，茶叶 (咖啡)，巨细 水刊: 顶刊A解决了a问题，顶刊B解决了b问题我把茶叶和咖啡混在一起，茶咖，你不讲出来，就没有人知道，把你的优点放大 水刊 看怎么缝起来 顶刊 看思路"},{"title":"水刊的技巧","date":"2023-02-24T12:57:09.696Z","url":"/2023/02/24/%E6%B0%B4%E5%88%8A%E7%9A%84%E6%8A%80%E5%B7%A7/","categories":[["undefined",""]],"content":"水刊: SCI4 区，部分三区 顶刊、顶会要看，但是别看多了 好的东西长啥样 去水刊，多去看水刊论文 看水刊能发现新大陆一个论文需要多个数据集 (两个)，绝大多数水刊只用了一个数据集一个论文需要创新，对你这个领域需要有独到的见解，先提出问题 (领域内的问题)，然后解决，绝大多数水刊，可能只是A 域用了这个，还不错，我就把A 域的拿到B 领域来一个论文性能要好，你会发现水刊的性能无法言喻，因为你和3区的论文比那都是高抬自己 水刊看的是定势(框架)，顶刊顶会(好一点的论文) 还得看 找到基准模型， 不断地尝试，把好一点的论文方法加到我的基准模型上（不一定说要一模一样，怎么简单怎么来）一般只需要修改一个models文件 尝试个三四种 可能一次次的尝试，都无用，无法写小论文（调参能解决绝大多数性能问题）但是你的工作量有了(毕业论文就不愁了） 一定要吹得狠一点(编故事，你一定是想办法编出一个问题，这个问题是你领域内的问题） 分级注意力网络(3层) 即 通过一个方法解决三个问题 （你缝补三个人的思想）每个人的思想都提出了一个问题 你自然是可以通过你提出的模型来解决这三个问题"},{"title":"tf安装指坑","date":"2023-02-24T11:56:33.177Z","url":"/2023/02/24/tf%E5%AE%89%E8%A3%85%E6%8C%87%E5%9D%91/","categories":[["undefined",""]],"content":"0. 前言本次项目是在我中了项目书重新回顾了PEBG的代码准备尝试给他跑起来 本次差不多我认真的时间应该是花了4-5天的样子 在这次坑里面 我花费了30大洋 其中20找了个人帮我看了看代码（是真的只是看了看然后给俺发了个requirement.txt） 然后我又花费了10 去找了个国内的GPT去帮我看了看代码 国内GPT真垃圾！！！ 参考的文献和链接：  本次项目  tf环境安装  这个链接基本讲明白了1.x是怎么安装  在tf运行的过程中会出现DLL文件确实 可以在这里找  dll 文件安装方法 conda环境下  dll放的位置 1. 坑 tf1.x 和 tf2的坑 目前的google在推tf2 在google根本找不到tf1.x 的安装方式 然后作者的readme 也没写个合适的requirement 项目作者的不维护 issue 不像我上个项目维护的那么好 去github去看看 一个项目 作者要是不答疑 其实这个代码其实很难跑的 tf GPU 和 CPU 建议在尝试了很多很多遍GPU的同学 试着用用CPU去跑跑 很有可能会一次跑通 不得不说 tf1.12 这个CPU的库 跑通了 我很开心！！！ 基本尝试 2.5 GPU CPU 1.5 GPU CPU 1.2 GPU CPU .dll 文件缺失 放在这个地址下 C:\\Users&lt;user&gt;\\Anaconda3\\envs&lt;env name&gt;\\Library\\bin C:\\Windows\\System32 这里 我是放这里成功的"},{"title":"git代理","date":"2023-02-24T11:56:33.175Z","url":"/2023/02/24/git%E4%BB%A3%E7%90%86/","categories":[["undefined",""]],"content":"这个问题碰到很多次了 每次安装git的时候都需要查查 然后 在软件里面设置一下就好了 等等刚刚贴了张图才意识到的问题 图中的协议为socks协议 我之前找到的教程 很可能是不符合协议 才可能设置不清不楚的 总结 协议要对的（这次犯得错误） 端口号要对的 "},{"title":"学术不端","date":"2023-02-24T11:56:33.172Z","url":"/2023/02/24/%E5%AD%A6%E6%9C%AF%E4%B8%8D%E7%AB%AF/","categories":[["undefined",""]],"content":"先看看学术不端的定义 学术不端主要指学者涉及抄袭、剽窃的不良行为，也指学者恶意的一稿多投行为。抄袭主要指抄袭者将被抄袭者的文字，不加修改地移入自己的论著，并当作自己的成果发表;剽窃主要指剽窃者将被剽窃者的文字或学术观点，经过改造后移入自己的论著，并当作自己的成果发表。抄袭是公开的，剽窃是偷偷的、暗地里的。 注意 是学者哦 "},{"title":"判断创新","date":"2023-02-24T11:56:33.169Z","url":"/2023/02/24/%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E6%98%AF%E5%88%9B%E6%96%B0/","categories":[["undefined",""]],"content":"多头注意力换头数（换超参数）不行 n&#x3D;8 换成 16 不行 比如有人用 elmo 做了词向量，然后我用bert 做词向量去做下游任务如果你是第一个，大概率可以， (水一篇水刊水文)然后就编故事这个领域有了什么问题，然后我用 bert 解决了这个问题(偏应用的领域，交叉学科)千万不能是: bert 好，我用 bert 我用自己的电脑跑，原模型性能变差了，我自己改进后，性能好，但是没有原模型好原模型: 50我拿自己电脑跑 (原模型) : 48我做了个改进，我自己电脑跑我的代码: 49毫无问题 性能好就可以了吗？最重要的是：你加去的东西，能够编个故事 新兴领域：50 你做的对这个领域有贡献，别人能够借鉴，或者说你能编个好故事 需要提升较多性能才行 水大论文，是没有问题的 老领域：99% 99.2% 改参数 也算是创新 最重要最重要的一点，编故事 这个领域有什么问题，俺通过这个解决了而不是我用了这个模块，性能变好了搞代码的时候:我用了这个模块，性能变好了 搞代码一定要有继承 复现效果更差怎么办解决方案用复现的结果一定要记住，保存证据，保存你跑出来的模型文件，然后写论文的时候记住，标明一下（是俺跑出来的） 继承有一个好的继承，我啥都有了，代码，配环境，数据，配套论文，还有他们的一些没有完成的想法我只需要做，看懂代码里的极少部分代码在别人的基础上加上一点东西即可，学术裁缝 研 4(对解码器做了改进)，研 3(这是对编码器做了改进)，把他们的拼接起来了。在别人的基础上加上一点东西即可，学术裁缝 ai 所有的方向都是坑，有什么能继承给你的，你就去继承什么?"},{"title":"没有继承且无法复现论文模型","date":"2023-02-24T11:56:33.167Z","url":"/2023/02/24/%E6%B2%A1%E6%9C%89%E6%96%87%E7%AB%A0%E7%BB%A7%E6%89%BF%E6%80%8E%E4%B9%88%E5%8A%9E/","categories":[["undefined",""]],"content":"找大量的论文，这个论文一定要附代码，如果没有附代码直接 pass 没有代码，我们手动构建（基本弄不出来） 论文里面提出来了（如果没提出来 就可以找下一篇） 去找作者要（尤其是和机构合作的论文） 去github去搜 模型一定可以调通，且复现后性能不会太差 开始做学术裁缝 现在开始做的，就是大量看论文 (不要看得太细了) ，如果觉得这篇论文的思想可以粘在你的论文上， 性能差不多，一般都可以通过调参解决（调参师） 我引用了A模型 然后改成了B模型 （改了改参数）全世界都在用bert 但是 单单bert的参数 大家肯定是大不相同的"},{"title":"感觉是最好的知识追踪入门提升论文","date":"2023-02-24T11:56:33.164Z","url":"/2023/02/24/%E6%84%9F%E8%A7%89%E6%98%AF%E6%9C%80%E5%A5%BD%E7%9A%84%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA%E5%85%A5%E9%97%A8%E6%8F%90%E5%8D%87%E8%AE%BA%E6%96%87/","categories":[["undefined",""]],"content":"说在前面论文名称： PYKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models  作者试图回答两个问题 什么是DLKT算法的合理、可靠和现实的评估过程？ 学生数据、模型设计和预测情景的不同特征如何影响模型性能 ？ 0 问题定义E &#x3D;&lt; q， {c}， r, t &gt; q 问题 c kc的集合 r 对还是不对 t 学生的响应时间戳 1 模型分类 深度顺序模型 DKT, DKT+, DKT-F,KQN KQN：使用学生知识状态编码器和技能编码器通过点积来预测学生的反应表现，通过点积预测学生反应 KQN 可以看看 很像啊 我不说是啥 很像双塔的说 内存增强模型 DKVMN 基于对抗的模型 将对抗性扰动等对抗性训练技术应用到原始学生交互序列中，以降低DLKT过拟合和有限泛化问题的风险 ATKT 基于图的模型 GKT 注意力方法 AKT,SAKT,SAINT 上述方法不具有排他性 例如SKVMN 2 数据集 Statics2011:此数据集收集自2011年秋季在卡内基梅隆大学教授的工程静力学课程[33]。 由[6,10,46]推荐，通过串联问题名和步骤名来构造唯一问题。 ASSISTments2009:此数据集由2009-2010学年从免费在线辅导辅助平台收集的数学练习组成。 在过去的十年中，该数据集被广泛使用，并已成为KT方法的标准基准[1,9,10,23,43,46]。 Assistments2015:与Assistments2009类似，此数据集是2015年从Assistments平台收集的。 此数据集是其他辅助数据集中学生人数最多的 Algebra2005:此数据集来自KDD Cup 2010 EDM挑战赛，包含13-14岁学生对代数问题的回答[32]。 它包含详细的步骤级学生响应。 独特的问题结构类似于Statics2011中使用的过程。 Bridge2006:此数据集也来自KDD杯2010 EDM挑战赛，其独特的问题构造类似于Statics2011中使用的过程。 NIPS34:此数据集来自NeurIPS 2020教育挑战赛的任务3和4。 它包含学生对多项选择诊断性数学问题的答案，并从EEDI平台收集[40]。 对于每个问题，我们选择使用主题树中的叶节点作为其KCS。 POJ：本数据集由编程练习组成，收集自北京编码实践在线平台。 该数据集最初是由Pandey和Srivastava[24]刮取的。 现实世界的预测场景步骤1 在KC响应数据上训练DLKT模型，当问题与一组KC相关联时，通过将每个问题级交互扩展为多个KC级交互，但问题由多个KC响应组成的 步骤2 首先利用学习到的DLKT模型对上述扩展的KC反应数据进行预测，然后通过聚合预测的KC掌握水平输出最终的问题水平预测。 扩展 在聚合 为什么会这个样子捏 KC 比问题相比 数据简直太少了的说 4 训练 将问题扩展成kc 首先使用学习到的DLKT模型对上述扩展的KC-response数据进行预测，然后通过聚合其kc的预测掌握水平来输出最终的问题级预测 问题标签泄露 注意看结论二 这将导致地面真相的泄漏，因为连续的kc，如kt和kt+1可能与相同的问题相关联，这被称为标签泄漏问题。（类似于 图中的k3 和 k4 先后预测 如果k3 和k4 存在相关关系，就会存在标签泄露问题） KC聚合 一步 和 多步预测(1)超前一步预测; (2)多步超前预测。 具体来说，提前一步预测任务仅预测学生在给定学生历史交互序列的最后一个问题上的反应。而多步提前预测任务预测学生的反应跨度给定学生的历史交互序列。准确的一步提前预测将极大地改进实时教育推荐系统，多步提前预测将为学习路径选择和构建提供建设性反馈，并帮助教师对未来的教材进行适应性调整。 标准化的数据处理 数据过滤 去除空值和重复值 以及 交互数量小于三的集合 四元组中缺失的也得去掉 80% 20% 其中 80% 被分成5叠 4叠训练 1叠验证 KC子序列生成用于训练和验证。 当一个问题有一个以上的KCs时，通过重复回答多次将原始问题-响应序列扩展到KC级别，每个KC对应一个KC。 将扩展后的KC水平响应序列截断为长度为m的较短子序列，其中m为预定义的最大训练序列长度。 小于m的序列将由-1 填充 默认情况下，我们选择使用LF-AVG进行KC预测融合 结论 注意机制对DLKT模型性能影响较大。首先将深度学习应用于KT问题的DKT模型仍然是优秀的。 对扩展KC序列的逐个评估会导致标签泄漏问题，从而导致性能膨胀。（注意看table2 和 table1） DLKT模型对于具有非常长的交互序列的学生表现不同 不同KC聚集方式的预测结果基本一致，“后期融合-平均”方法的预测结果略好于其他方法 在多步超前预测情况下，累计或非累计预测的选择对DLKT性能有很大影响 限制问题侧信息 : (1)问题文本内容; EKT，EERNN (2)各KC的潜在问题变化; AKT (3)问题难度等级; AKT，PEBG，MF-DAKT (4)问题之间的关系。 RKT，MF-DAKT，PEBG 学生侧信息 : (1)历史成功尝试和失败尝试; MF-DAKT (2)最近的尝试; MF-DAKT (3)学生学习能力; DKT-DSC (4)学生的个性化先验知识。 CKT KC侧信息 : (1)潜在知识表示; AKT，KQN (2) KCs之间的关系。 PEBG"},{"title":"sci写作","date":"2022-11-30T07:37:44.974Z","url":"/2022/11/30/sci%E5%86%99%E4%BD%9C/","categories":[["undefined",""]],"content":" 模型确定 结果正在跑（或已结束） 目标期刊已定，一般可以定顶刊 从目标期刊中打印3-5份最近论文定模板 第一次写没必要注意排版格式 从方法开始写 ApproachBasic module用的一些小模块，简单讲， 不要把你懂得当做别人也懂 你的方法（一般为标题）你的方法分成多个小模块 编码器-解码器架构 总的讲一下你的方法的一个架构，一个组成 编码器 a编码器 目标检测 知识图谱 b编码器 解码器 正向解码器 反向解码器 实验参数配置一些用到的东西，评价的东西（可以模仿别人写） 中文改英文 数据设置 评价指标 参数配置 性能比较和近几年最新的比较 你可以挑几个差点的顶刊顶会（近两年） 分析 消融实验（基准模型加模块）abc 去掉abc 仅去a 仅去b（如果去掉b 性能下降 那就写成去掉a+b） 仅去c 案例分析 实例分析 举例子来证明比基准模型好 摘要（Abstrac）简单描述领域 领域出现了什么问题（创新在这里），用了什么方法，方法性能提升。简述 相关工作+方法 Introduction描述领域，这个领域对现实世界有什么帮助 这个领域出现了什么问题（引用别人），尽量用可视化的例子来表达（sci第一页 右上一张图） 早期大家是怎么做的，通过进步出现了这样那样的问题 我就想着解决这个问题，我通过以下三点（我加的小模块）来解决： 方法里面的小模块 总结，我这个性能挺好的 相关工作（realted work）首先描述你这个领域的普遍做法（选）【1-10】这样 【11-12】那样 具体讲别人是怎么做的 指出缺点 总结下，受上面的启发，我怎么做的（简述） 总结我做了一件事，这件事怎么做的，有了提升 方法+实验 致谢和参考文献效果提升就可以了吗基准模型a 加了BCD 性能提升 消融实验去掉BCD 和最近几年比较（往年顶刊顶会），今年差一点的会议 要写创新 你不能说，我用了A+B+C+D 我自己加了个模块，没有引用过别人的 一定要明确你加的东西的现实意义(找到他的起源)然后对你的领域你一定要了解透彻强行扯在一起，稍微有逻辑即可(别人都看不懂)，最好是举个例子放在引言里。 如何说个好故事 继承一个基准模型，然后做了一点改进（添加一个小模块（引用一篇论文）） 复刻别人的想法 这个领域出了什么问题，为什么要加这个小模块 a领域到b领域 堆叠他人的想法我的论文引用了两三片论文的思想,堆叠在一起a领域出现了A、B、C问题，我很牛逼，我增加了一个模块，可以一次性解决三个问题我用了这个方法解决了三个问题（不要直接说堆叠abc的方法）我堆叠了ABC三篇论文的思想.……. 你知道的他不一定知道，只要他不知道，我可以选择告诉他，也可以选择不告诉他，我告诉他的都一定必须要是真的！ 你要找到领域的问题你不要说，我用了 你需要编一个领域内的问题 符合逻辑就行 实事求是去探讨你所加模块的原始功能挖掘最原始的功能（不要流于表面） 仿照他人的方法"},{"title":"服务器使用的问题","date":"2022-11-19T08:51:37.541Z","url":"/2022/11/19/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E7%9A%84%E9%97%AE%E9%A2%98/","categories":[["undefined",""]],"content":"当我们拿到了ip和密码 我们要干的活如下： 连接服务器 配置conda运行环境 上传数据集 配置conda jupyterab kernel（内核） jupyterlab使用 配置pycharm远程连接 还有一个 是我在编码过程中 总结的一些小窍门 缩小数据集 - 目的是本地的debug 以及 快速的验证 远程连接时的注意点 - 啊哈 咱忘了 以后再补！ 进程守护 - 使用jupyterlab teimial 来跑文件 多卡的优势 - cuda0 cuda1 一个机器 同时跑两个 怪快的 如果碰到了 要关闭远程连接工具 一直连接的指令 这个要和3 一起食用 要干的活： 上传数据集前两个活 就不具体说了 情况都不一样 上传数据集 使用 xshell 连接  看看这个网址 配置conda jupyterab kernel（内核） jupyterlab使用我之前写过一个怎么配置来着  感觉有点不合时宜 jupyterlab 端口映射 因为 服务器 一般来说 我们都摸不着 所以说 对于jupylab 我们需要做端口映射 我发现我的文章写的怪好的 就直接粘过来 第一步 在远程服务器开启jupyter lab服务 第二步 在自己本地的ssh命令窗口上输入以下命令，将本地端口与远程服务器的9000端口实现映射，在此将本地的6666与服务器的9000端口进行映射 &#x54;&#111;&#x6d;&#x40;&#x31;&#x31;&#46;&#x32;&#x32;&#46;&#x33;&#x38;&#x2e;&#50;&#x31;&#52; 对应远程服务器的用户名和IP地址 配置 pycharm 远程连接（大坑）真的坑特别多 也是我卡的时间最久的项目 看了看其他的文档 写的怪简单的 我这里写个详细点的 十分建议 从 你需要的文件夹 作为pycharm项目文件打开 一共分为三步 ssh确认解释器 ssh确认同步本地以及远程文件位置 确认本地以及远程 不同步的文件夹 ssh python 解释器 一直走到这一步 file -》 setting -》project xxx -》 左边的设置 -》 add -》 ssh 这里的interpreter 需要 设置成 conda 配置环境的python文件 一般在 miniconda&#x2F;envs&#x2F;xxxx你设置的环境名&#x2F;bin&#x2F;python 这里注意一个坑 running code 那底下的一行 一定要设置对应清楚（有的教程教你 这个位置乱填 简直了） finsh -》 apply 设置 远程本地文件夹 坑的地方是 你需要点击 项目总文件夹才能设置好 设置 剔除文件 这个没啥坑 一定要打的小勾勾 本人 将这个坑 称为特别的坑 在这里打勾的时候 一定要查清楚 只有一个只有一个只有一个 然后 查完了之后 你就可以打勾了 打勾的项目会存在加深的效果 记住了哈 真的是血泪教训 "},{"title":"双塔那些事儿","date":"2022-11-11T02:42:53.203Z","url":"/2022/11/11/%E5%8F%8C%E5%A1%94%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/","categories":[["undefined",""]],"content":" 推荐系统的双塔 解耦 user 和 item，部署时可分离 双塔训练训练方式三种训练方式 pointwise pairwise n对pairwise – listwise softmaxwise 样本选择正： 用户点击物品 负：没有被召回 ​ 召回但是被排序模型淘汰的 ​ 曝光但未点击的 DSSM初代双塔softmaxwise loss 更新 首先使用的是百度广告 YOUTUBE 双塔in batch sampled softmax 负采样 在模型中也并不需要 整个batch来作为负样本来计算 可以考虑1：10， 1：100 纠偏 sample-bias-corrected u，v计算出一个score之后 通过剪出这个物品被采样的log（pj）得到一个新的score pj的计算 可以使用全局被采样的概率来计算 重要经验 重要经验：正则化和温度系数 如果不采用正则化 而直接采用点积$$\\overrightarrow{a}\\cdot\\overrightarrow{b}&#x3D;|\\overrightarrow{a}||\\overrightarrow{b}|\\cos\\theta$$想让点积越大,角度越小。是我们预想的训练方向。 但是 在模型中可能会增加item向量的模 导致模型训练坍塌。 所以要使用正则化 正则化之后 数据难收敛 [-1,1] 举个例子： 模型完美的预测成功了正样本 且负采样10个 也同样完美的预测出来 但 使用正则化预测出的分数为0.42 就很拉 添加了温度系数 预测分数达到了0.999 就很强 所以需要添加 温度系数 负样本构造总结 – 召回阶段很重要 召回元素怎么找 一个用户 和所有物品算相似度 效果一定好 但是时间慢了 annoy - 只支持CPU 用的多 将物品的向量空间 通过树的形式 分割出来，确保每个树 有定量的样本 Faiss - 啥都支持 但是 用的少 复现踩坑 未来发展"},{"title":"lite transformer讲解","date":"2022-11-09T13:55:38.304Z","url":"/2022/11/09/lite-transformer%E8%AE%B2%E8%A7%A3/","categories":[["undefined",""]],"content":"传统transformer 传统 input 通常包含 数据的长度N 数据的维度d 模型参数量与性能对比 d下降的情况下，会导致attention单元 上下文捕捉关系不足，导致效果差 d不变的情况下，会导致参数量变多，上下文捕获好，效果好 作者的想法 d不变，参数量给他变少一点 LSRA (长短距离注意力) 左侧为传统transformer结构 右侧为专门处理局部关系的卷积分支 LSRA 模块遵循两分支设计。左侧注意力分支负责捕获全局上下文，右侧卷积分支则建模局部上下文。研究者没有将整个输入馈送到两个分支，而是将其沿通道维度分为两部分，然后由后面的 FFN 层进行混合。这种做法将整体计算量减少了 50% "},{"title":"多任务学习 他方总结","date":"2022-11-08T13:28:21.991Z","url":"/2022/11/08/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0-%E4%BB%96%E6%96%B9%E6%80%BB%E7%BB%93/","categories":[["undefined",""]],"content":"2022&#x2F;11&#x2F;8 链接： 一文梳理多任务学习(MMoE&#x2F;PLE&#x2F;DUPN&#x2F;ESSM等) 链接： 这个老师讲的也很好的 多任务为什么有效 隐式数据增强：每个任务都有自己的样本，使用多任务学习的话，模型的样本量会提升很多。而且数据都会有噪声，如果单学A任务，模型会把A数据的噪声也学进去，如果是多任务学习，模型因为要求B任务也要学习好，就会忽视掉A任务的噪声，同理，模型学A的时候也会忽视掉B任务的噪声，因此多任务学习可以学到一个更精确的嵌入表达。 注意力聚焦：如果任务的数据噪声非常多，数据很少且非常高维，模型对相关特征和非相关特征就无法区分。多任务学习可以帮助模型聚焦到有用的特征上，因为不同任务都会反应特征与任务的相关性。 特征信息窃取：有些特征在任务B中容易学习，在任务A中较难学习，主要原因是任务A与这些特征的交互更为复杂，且对于任务A来说其他特征可能会阻碍部分特征的学习，因此通过多任务学习，模型可以高效的学习每一个重要的特征。 表达偏差：MTL使模型学到所有任务都偏好的向量表示。这也将有助于该模型推广到未来的新任务，因为假设空间对于足够多的训练任务表现良好，对于学习新任务也表现良好。 正则化：对于一个任务而言，其他任务的学习都会对该任务有正则化效果。 任务关系的隐式建模shared-bottom 一般用于 两个任务适不适合做多任务！ 做一个验证 一般来说 这个模型会出现 负迁移现象 MMoE（图b为OMoE） 如下图所示，模型(a)最常见，共享了底层网络，上面分别接不同任务的全连接层。模型(b)认为不同的专家可以从相同的输入中提取出不同的特征，由一个Gate(类似) attention结构，把专家提取出的特征筛选出各个task最相关的特征，最后分别接不同任务的全连接层。MMOE的思想就是对于不同任务，需要不同专家提取出的信息，因此每个任务都需要一个独立的gate。 mmoe 在使用上会存在跷跷板效应（即 一个效果提升 一个效果下降） PLE即使通过MMoE这种方式减轻负迁移现象，跷跷板现象仍然是广泛存在的(跷跷板现象指多任务之间相关性不强时，信息共享就会影响模型效果，会出现一个任务泛化性变强，另一个变弱的现象）。PLE的本质是MMOE的改进版本，有些expert是任务专属，有些expert是共享的，如下图CGC架构，对于任务A而言，通过A的gate把A的expert和共享的expert进行融合，去学习A。 总结 任务关系的显示建模ESSM 解决的问题： cvr 正样本数量稀疏 cvr 模型训练样本空间和线上测试样本空间。不符合独立同分布。 how： cvr 与ctr 的embedding共享，使得cvr网络可以在未点击曝光样本中学习 通过引入ctcvr任务，将ctcvr与ctr任务定义在全样本空间，来辅组cvr在全样本空间建模 ESSM2更长的序列 AITM业务模型：信用卡有5个状态 五个状态（impression-&gt;click-&gt;application-&gt;approval-&gt;activation），链路很长。样本非常不均衡，即正样本越到后面越少，因此，前面任务的信息如何传递到后面任务，是本文研究的一个话题。 塔之间的传递 不在只是使用乘法！ 论文提出了一个AIT（Adaptive Information Transfer）模块进行信息的传递，此外，在任务与任务之间，论文还增加了一个辅助损失函数 其中 qt 是当前任务塔的输出， zt−1 是前面一个任务塔t-1的AIT模块的输出。所以变量z地位可以理解为循环式神经网络隐藏层的地位。 gt−1 是一个函数（网络层），他的物理意义是决定两个任务之间什么信息可以流动。 损失函数 （生产环境下！） 如果yt-1≥yt， Llc(θ)将输出一个正惩罚项，否则输出0。 图！ MetaHeacMetaHeac摒弃底部bottom共享，tower分离的模式，认为tower层的信息也是可以共享的，比如有的item不会被点击那么也不会被转化，MetaHeac采用Gate机制同时控制bottom（Expert）和tower（Critic），并且采用元学习（meta learning）来学到任务通用的参数，再通过Gate机制选择任务特异的特征和预测结果的组合。 多loss优化策略优化策略 主要是 在不同任务之间 提供单任务的loss权重 即$$L(t)&#x3D; \\sum {}{i}w{i}(t)L_{i}(t)$$确认 Loss 前面权重t是啥样子 GradNorm UWL"},{"title":"10月14日 面对的问题","date":"2022-10-30T08:05:02.377Z","url":"/2022/10/30/10%E6%9C%8814%E6%97%A5-%E9%9D%A2%E5%AF%B9%E7%9A%84%E9%97%AE%E9%A2%98/","categories":[["undefined",""]],"content":"看了看 好久都没写文章了 我就写一写 我最近学习到的技能 远程jupyter lab 第一步 在远程服务器开启jupyter lab服务 第二步 在自己本地的ssh命令窗口上输入以下命令，将本地端口与远程服务器的9000端口实现映射，在此将本地的6666与服务器的9000端口进行映射 &#84;&#111;&#109;&#x40;&#x31;&#x31;&#46;&#50;&#x32;&#46;&#51;&#x38;&#46;&#50;&#x31;&#x34; 对应远程服务器的用户名和IP地址 python 环境配置 setup.py requirement.py 问题 NameError: name ‘_C‘ is not defined解决方法：pip install Cython，重启kernel No module named ‘typing_extensions‘ 问题解决 pip install typing_extensions 解决CUDA error: no kernel image is available for execution on the device 3080 太年轻了 报错信息中的CUDA capability sm_86意思是算力8.6，即该RTX型号的显卡算力是8.6，但是当前的PyTorch依赖的CUDA版本支持的算力只有3.7、5.0、6.0、6.1、7.0、7.5。 "},{"title":"阅读DKVMN时产生的问题","date":"2022-10-30T08:05:02.290Z","url":"/2022/10/30/%E9%98%85%E8%AF%BBDKVMN%E6%97%B6%E4%BA%A7%E7%94%9F%E7%9A%84%E9%97%AE%E9%A2%98/","categories":[["undefined",""]],"content":"DKVMN 具有维持用户概念状态和概念更新的能力 对于俺的项目书来说 作用十分的大 在阅读模型代码文件时 产生了一些问题 在这里写一下 pytorch维度广播如果一个Pytorch运算支持广播的话，那么就意味着传给这个运算的参数会被自动扩张成相同的size，在不复制数据的情况下就能进行运算，整个过程可以做到避免无用的复制，达到更高效的运算。广播机制实际上是在运算过程中，去处理两个形状不同向量的一种手段。pytorch中的广播机制和numpy中的广播机制一样, 因为都是数组的广播机制。 如果两个数组的shape不同，就会触发广播机制 1）程序会自动执行操作使得A.shape&#x3D;&#x3D;B.shape；2）对应位置进行相加运算，结果的shape是：A.shape和B.shape对应位置的最大值，比如：A.shape&#x3D;(1,9,4),B.shape&#x3D;(15,1,4),那么A+B的shape是(15,9,4) 条件 两个张量都至少有一个维度 从右往左顺序看两个张量的每一个维度，x和y每个对应着的两个维度都需要能够匹配上 2.条件细致的说一下 1 维度可以和任何维度进行匹配 维度可以随意产生1来进行维度的对应 看看上面对应的例子 从右往左看 是一一对应的 那就符合广播机制 chuck它是将tensor按dim（行或列）分割成chunk_num个tensor块，返回的是一个元组。 torch.chunk(tensor,chunk数，维度）chunks&#x3D;n 代表切分成几个小块dim&#x3D; n代表要在哪个维度一进行操作 torch.getorch.ge(a,b)比较a，b的大小，a为张量，b可以为和a相同形状的张量，也可以为一个常数。 mask_select一般配个torch.ge使用 torch.t()求转置矩阵 mul matmul mmmul 是矩阵对应位相乘 a，b的维度必须满足广播才行 mm 是矩阵相乘 最正常的情况 matmul &#x3D; mm"},{"title":"ctr 综述","date":"2022-09-29T12:16:33.562Z","url":"/2022/09/29/ctr-%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"说些废话点击率预估(CTR)在各种个性化在线服务中扮演着重要的角色，包括：计算广告、推荐系统和Web搜索等。 根据时间来分 可以稍微看看这个图！ 2007年 以前 都是 LR+人工特征工程为主 到了2010年 以 隐向量为代表的FM 模型，解决了人工组合特征的困扰 而在2014 年 facebook提出的 GBDT + LR 提出的一种树模型特点构建组合特征的思路 2015年以后，借助非线性自动组合特征能力的深度模型，开始成为业界的主流 从经典的DNN 到 结合浅层的Wide&amp;Deep 用于CTR预估模型的深度模型在这些年百花盛开。 各种交叉特征建模方法层出不穷，attention机制也帮助更好的适应业务，提升模型的解释性。 而核心问题离不开解决数据高维稀疏难题，自动化组合特征，模型可解释性等问题。 主要看了什么深入 FFM 美团技术团队：深入FFM原理与实践 1. 前言 – CTR推荐是啥ctr是啥CTR指在搜索引擎中输入关键词后进行搜索，然后按竞价等因素把相关的网页按顺序进行排列出来，然后用户会选择自己感兴趣的网站点击进去；把一个网站所有搜索出来的次数作为总次数，把用户点击并进入网站的次数占总次数的比例叫点击率。 计算公式 计算公式为CTR&#x3D;实际点击次数&#x2F;展示量，即 Click &#x2F; Show content。 CTR：点击通过率，Click-Through-Rate (点击通过比率) ctr预估推荐 预测点击率(CTR）的算法被称为CTR预估算法，CTR预估的一系列算法经常用于推荐系统的排序阶段，其实有很多机器学习的方法或者深度学习的方法都可以作为CTR预估的算法，前提是这些算法的输出是一个0~1的概率值，并以概率作为分类的依据。 2. 问题定义以及评价ctr推荐并不相同与ctr预估。其目标存在差距。 CTR预估起源于计算广告，因为关系到真金白银的定价问题，因此要求预估出来的CTR必须“绝对准确”。这是因为，假如给一个用户准备了A&#x2F;B&#x2F;C三个广告，那么无论预测CTR是0.9、0.8、0.6，还是0.5、0.4、0.3都不影响三个广告的展现顺序，但是向客户的收费却有天壤之别。 但是推荐系统只要求“相对准确”。假如ABC换成了三篇文章，只要能够将用户最喜欢的A排在第1位，次喜欢的B排在第2位，无论我们预测的CTR是0.9、0.8、0.6，还是0.5、0.4、0.3，用户都能接受。 2.1 定义其输入是用户的点击和未点击的数据。通过分析，输出一个相对排序的item 进行推荐 2.2 数据集数据集以点击数据集为主 同样的 我看到的数据集类型基本都是广告这个类型 一定比的数据集有Criteo，azavu 其他的有 kdd12，ml-1m，iPinYou等 dataset website filed records&#x2F;k Criteo  广告点击 45,850 azavu  广告点击 40,428 iPinYou  广告点击 15,367 kdd2010  教育评分 ml-1m  电影评分 1000 2.3 常用评估指标一定会比较的 AUC logloss 其他的 RelaImpr Rmse RIG AUC  是一个模型的评价指标，只能适用于二分类模型的评价。 AUC（Area Under Curve）被定义为ROC曲线下与坐标轴围成的面积， 其中，ROC曲线全称为受试者工作特征曲线（receiver operating characteristic curve） 二元分类模型的预测结果有四个结局。即以下四种。 ROC曲线的横坐标是伪阳性率（也叫假正类率，False Positive Rate），纵坐标是真阳性率（真正类率，True Positive Rate），相应的还有真阴性率（真负类率，True Negative Rate）和伪阴性率（假负类率，False Negative Rate）。这四类指标的计算方法如下： 其中坐标轴的定义： （1）伪阳性率（FPR）：判定为正例却不是真正例的概率，即真负例中判为正例的概率 （2）真阳性率（TPR）：判定为正例也是真正例的概率，即真正例中判为正例的概率（也即正例召回率） （3）伪阴性率（FNR）：判定为负例却不是真负例的概率，即真正例中判为负例的概率。 （4）真阴性率（TNR）：判定为负例也是真负例的概率，即真负例中判为负例的概率。 关于曲线的绘制 得看这个  步骤大致分为三步： 按照属于“正样本”的概率将所有样本排序 将分类阈值设为最大，即把所有样例均预测为反例，所以得到此时的p和r均为0 依次将样本从高到低的样本score值作为阈值 logloss 二分类时 logloss 就是交叉熵 RelaImpr $$Relalmpr &#x3D; (\\frac {AUC(meadured\\quad model)-0.5}{AUC(base\\quad model)-0.5}-1)\\times 100%$$ RelaImpr代表相对于based model的相对改进指标，对于随机猜测的话，值AUC是0.5 （阿里DIN） RMSE 均方根误差 $$RMSE &#x3D; \\sqrt\\frac{\\sum_{i&#x3D;1}^{N}(Predicted_i - Actual_i)^{2}}{N}$$均方根误差RMSE和标准差的计算公式也是高度近似的：标准差是用来衡量一组数自身的离散程度，而均方根误差是用来衡量观测值（真值）与预测值之间的偏差 RIG 相对信息增益(Relative Information Gain) 信息熵公式为：$$h(X) &#x3D; - \\sum_{i&#x3D;1}^{n} p_i\\log p_i$$在条件A一直的情况下，X的相对熵为：$$h(X|A) &#x3D; \\sum_{i&#x3D;1}^{n} p_i(x_i|A)\\log p_i(x_i|A)$$熵与条件熵的差值为信息增益：$$g(X,A) &#x3D; H(X) - H(X|A)$$信息增益值的大小是相对数据集而言的，并没有绝对的意义，在分类问题比较困难的时候，也就是训练数据集的经验熵比较大的时候，信息增益的值比较大，反之，信息增益值会偏小。使用相对信息增益(信息增益比)可以对这一个问题进行校正，相对信息增益(Relative Information Gain):$$RIG &#x3D;\\frac {H(X) - H(X|A)}{H(X)}$$RIG指标不仅和模型的质量有关，还和数据集的分布情况有关；因此千万注意不可以使用RIG来对比不同数据集上生成的模型，但可以用来对比相同数据集上不同模型的质量差异。 2.4 模型大类 模型大类 提出时间 原理 优势 局限性 LR 2010年以前 使用线性权重组合每个单一的特征 具有效率高、易于快速部署、快速挖掘有效特征的优点 不具备特征交互的能力，交叉特征可能比单一特征具有更重要的影响 FM 2010 对Poly2中的权重矩阵W做矩阵分解，为每个特征学一个k为的向量表示。两个向量的内积表示特征对的重要性 FM可以捕获特征交互，同时可以在稀疏场景下有效的学习 FM忽略了这样一个事实: 当一个特性与来自其他域(Field)的特性交互时，它的行为可能会有所不同。 DNN 2015 利用深度网络，提高模型的交叉特征建模能力 偏重于挖掘高阶，增强了模型的表达能力 对低阶交叉特征信息利用不足，可解释性差 wide&amp;deep 2016 通过联合训练的方式，同时获得线性模型（Wide）的记忆能力和深度模型（Deep）的泛化能力，提高模型的预测准确性和多样性 具有线性和深度两个特性 模型复杂 训练时间长 3. 模型3.1 LR模型 LR LR( Logistic Regression ) 是机器学习中一种线性分类模型， 由于其简单、高效、易并行的特点，在实际生产环境中被广泛地应用。 使用线性权重组合每一个单一特征 同样的在实际广告场景下的广告点击率预估问题。对于 给定年龄、性别、教育程度、兴趣类目等用户侧特征，上下文特征，以及广告id，商品类型等广告侧特征。预测用户对于特征的点击概率。针对特征的做法是对各类特征进行one-hot编码，针对连续数值特征，会按照区间分段的方法将其离散化再进行one-hot编码。 对于LR的模型来说，需要提供成熟的特征工程工作，而对应的模型结构较为简单。对于其模型来说没有使用到组合特征，是一大问题。$$\\hat{y}(x) &#x3D; w_o + \\sum_{i&#x3D;1}^{n} w_ix_i$$ poly2 模型有效的解决了LR的组合特征的问题，需要o（m2）的参数空间。 特征xi和xj的两两组合用xixj表示，可见只有当xi和xj都为非零值时，组合特征（或称交叉项）在多项式模型中才有意义。在真实业务场景下，样本特征通常高维稀疏，而组合特征xixj的稀疏程度则更为严重。在这种极度稀疏的情形下，模型很难准确地学习出参数wij 。 因为特征数据十分稀疏，模型在训练样本中从未见过有效的交叉项xixj，无法学习这个交叉项和label之间的关系。而参数无法准确学习，必然会严重影响模型效果。$$\\hat{y}{Poly2}(x) &#x3D; w_0 + \\sum{i&#x3D;1}^{n} w_ix_i +\\sum_{i&#x3D;1}^{n}\\sum_{j&#x3D;i+1}^{n}w_{ij}x_ix_j$$ GBDT+LR 这个模型的主要思路是，先训练一个GBDT模型，然后利用训练好的GBDT对输入样本进行特征变换，最后把变换得到的特征向量作为LR模型的输入。 模型名 论文名 优势 缺陷 年份 LR 简单、高效、易并行的特点，在实际生产环境中被广泛地应用 缺少组合特征 2007 poly2 相较于LR 添加了 组合特征 稀疏特征向量会变得更加稀疏，无法收敛。权重参数由n直接上升到 n2，增加了训练复杂度 2010 GBDT+LR Practical Lessons from Predicting Clicks on Ads at Facebook, KDD 2014 自动化对输入样本进行特征变化，相比仅使用LR或Tree模型，loss降低了3% 1）树模型在节点分裂时需要遍历特征集，通常不适用于海量离散ID类特征场景 2）引入级联模型误差，无法做end-to-end学习 2014 3.2 FM模型 FM 解决了POLY2模型的缺陷，FM诞生。最主要的区别就是使用隐向量对交叉特征的处理。对每一个对应的向量学习隐权重向量，在交叉特征是，使用两个特征隐向量的内积作为交叉特征的权重。 $$\\hat{y}{FM}(x) &#x3D; w_0 + \\sum{i&#x3D;1}^{n} w_ix_i +\\sum_{i&#x3D;1}^{n}\\sum_{j&#x3D;i+1}^{n}&lt;v_i,v_j&gt;x_ix_j$$ 直观上看，模型的计算复杂度是O(kn^2)，但经过如下对交叉项的优化，复杂度可以降低到O(kn)。 经过化简，FM模型的最终表达式为$$\\hat{y}{FM}(x) &#x3D; w_0 + \\sum{i&#x3D;1}^{n} w_ix_i + \\frac{1}{2}\\sum_{f&#x3D;1}^{k}[(\\sum_{j&#x3D;1}^{p}v_{j,f}x_j)^2-\\sum_{j&#x3D;1}^{p}v_{j,f}^2x_j^2]$$ FFM 通过将特征划分为不同的领域， 将不同特征域的差异信息显式引入到模型中，台湾大学阮毓钦（YuChin Juan）等人提出了域感知的改进版FM，称为FFM（Field-aware Factorization Machines）模型。 FFM是在FM的基础上引入了“场”的概念而形成的新模型。在FM中计算特征xi与其他特征交互影响时，使用的都是同一个隐向量Vi。而在FFM中会按照事先规则分为多个场（Field），特征xi属于特定的场f。每个特征被映射为多个隐向量vi1，….vif，每个隐向量对应一个场。当两个特征Xi，Xj组合时，用对方对应的场对应的隐向量做内积。 优势：FFM可以捕获特征交互，考虑Field信息 缺点：参数量过多，在实际生产系统中，FFM的参数是不能接受的。 假设隐向量的长度为k，特征fields数量为f，FFM的参数空间大小为fkn+n+1。由于二次项不能化简，FFM的训练和预测时间复杂度为O(kn^2)。 带来了巨大的内存消耗，以及过拟合的风险。 BiFFM FFM的二次项参数量是FM的fields数量倍，实际应用中面临内存消耗大的困境。为此，任职于新浪微博的张俊林博士提出了新的解决方案，称为双线性FFM模型[4]，模型方程如下$$\\hat{y}{BiFFM}(x) &#x3D; w_0 + \\sum{i&#x3D;1}^{n} w_ix_i +\\sum_{i&#x3D;1}^{n}\\sum_{j&#x3D;i+1}^{n}v_iWv_jx_j$$ 双线性FFM的二次项和FM类似，仍然是将所有特征映射到同一隐空间。但不同于直接对隐向量做点积，为了增强模型的表达能力，双线性FFM在FM基础上增加了k×k参数矩阵W。 所有特征共享一个矩阵w 参数量为k^2 一个Filed 一个矩阵wi 参数量为f*k^2 一个filed组合一个Wij 参数量为f^2*k^2 下表是双线性FFM在Criteo和Avazu两个公开数据集上的对比实验，结果表明，在内存消耗远小于FFM的情况下，双线性FFM效果接近于FFM，第三种类型的效果优于第一、第二种类型。一般情况下，在加入LayerNorm后，效果还有一定的提升。 FwFM（加权field型因子分解机） 通过FM,FFM的基础上做的工作，主要的思想：特征交互的重要性不同，因此要赋予不同的特征交互不同的权重。$$\\hat{y}{FwFMs}(x) &#x3D; w_0 + \\sum{i&#x3D;1}^{n} w_ix_i +\\sum_{i&#x3D;1}^{n}\\sum_{j&#x3D;i+1}^{n}x_ix_j&lt;v_i,v_j&gt;r_{F(i),F(j)}$$ 模型表达还是十分简洁，从复杂度分析，比FFM简介很多。相较于FM多了n*（n-1）&#x2F;2的参数量 但是在效果上 几乎接近于FFM，但在模型训练的难度上 还是比FFM简单 FvFM（向量 Field 型因子分解机） 为了解决FwFM在特征交互权重上表达能力不够的问题。通过Field matrix的方式来计算特征交互重要性。 过程主要分成三步 FvFM是一个比FwFM自由度更加高，但是比FmFM自由度低的模型。此时的交互矩阵为每个维度有学一个权重 FmFM（ 矩阵 Field 型因子分解机） 把交互矩阵进行了扩展 3.3 DNN如果说FM模型的更新，止步于filed层。在DNN这边就有了一个新的概念，embedding层。 embedding是用一个低维稠密的向量表示一个对象。方法起始于自然语言处理领域词向量生成问题的研究。 embedding技术对于深度学习推荐系统主要重要在于三个方面 推荐场景中大量使用one–hot编码对类别，id型特征进行编码，导致样本特征向量极其稀疏。几乎所有深度学习模型会使用embedding负责将高维稀疏特征向量转换成低维稠密向量 embedding本身就是极其重要的特征向量。相较于MF等传统方法产生的特征向量，embedding的表达能力更强，embedding几乎可以引入任何信息进行编码，使其本身包含大量有效信息 embedding对物品、用户相似度的计算是常用的推荐系统召回层技术。在局部敏感哈希等快速最近邻搜索等技术应用于推荐系统后，embedding更适用于对备选物品进行快速“初筛”，过滤出几百到几千量级的物品进行精排 FNN 主要解决embedding层收敛速度慢的问题。 FNN使用FM模型训练好的各特征隐向量初始化Embedding层的参数,相当于在初始化神经网络参数时引入了先验信息,这样一来神经网络训练的起点更接近于目标的最优点,加速了神经网络的收敛 在结构上来看，FNN就是FM+MLP 优点 引入DNN对特征进行更高阶的组合，减少特征工程，在一定程度上增强了FM的学习能力 缺点 两阶段训练模式，在应用过程中不方便，且模型能力受限于FM表征能力的上限 FNN专注于高阶组合特征，但是却没有对低阶特征进行建模 PNN PNN在embedding层和MLP全连接隐层之间增加了一个乘积层（product layer），用于更直接的建模两两特征的交互作用关系。乘积层包含z向量和p向量两部分，z向量由常数“1”向量和特征embedding相乘得到，因此z向量部分实际相当于特征embedding的直接拼接。p向量部分是PNN的核心，它是两两特征embedding进行“乘法”操作后的产物。 在使用不同乘法操作，将会带来不同的变体模型。IPNN( Inner Product-based Neural Network ),OPNN( Outer Product-based Neural Network )。从实验结果上看 PNN模型，包括IPNN、OPNN、PNN*（将inner product和outer product进行拼接），效果都要优于FNN。另外也能看到，基于深度神经网络的模型，效果普遍优于LR、FM线性模型，这说明非线性高阶特征建模有很重要的提升作用。 DeepCrossing 模型结构： Stacking层负责将各特征embedding拼接成一个向量，随后使用级联的多个残差单元来代替传统MLP。引入残差单元是DeepCrossing的主要改进点，好处是可以使用更深的网络层数，建模更高阶的特征，增强模型的表达能力。 FNN、PNN和DeepCrossing模型都是在经典DND框架下演化而来的，它们都有效的提高了交叉特征的建模能力。 但是对于低阶特征信息利用不足 3.4 wide&amp;deep框架对推荐系统的排序模型而言，定性地说，我们通常追求模型的记忆能力和泛化能力。一方面我们希望模型能准确记忆不同特征组合对预测目标的影响，以便依照已有的用户画像以及用户过去发生的行为使系统获得精准推荐能力，准确触达用户兴趣。另一方面，希望模型拥有一定的泛化能力，对未见过的或极少出现的特征组合，同样给出良好的预测结果，提高推荐内容的多样性。我们知道，线性模型以浅层形式直接学习稀疏组合特征权重，对训练数据中出现过的组合特征具有很好的记忆能力。而深度模型，稀疏特征被映射成低维稠密embedding向量，随后在深层全连接网络中获得充分交互，对具体的特征组合的“记忆”能力会减弱，但换来了更好的泛化效果。 基于这样的想法。 Google工程师在2016年提出了一种线性模型和深度模型的联合学习框架——Wide&amp;Deep模型，期望通过联合训练的方式，同时获得线性模型（Wide）的记忆能力和深度模型（Deep）的泛化能力，提高模型的预测准确性和多样性。 wide端改进 DeepFM模型 Wide&amp;Deep模型中Wide部分和Deep部分的特征输入是不同的，Wide部分的输入还需要依赖人工特征工程来挑选有效特征，为了减少特征工程依赖，华为工程师在2017年提出了DeepFM模型[12]。DeepFM仍然由Wide部分和Deep部分构成，不过Wide部分使用了FM来构建，利用FM自动学习二阶交叉特征的能力，代替需要人工交叉特征的LR。另外，Wide部分和Deep部分的底层输入特征以及特征embedding向量完全共享，能实现端到端学习。 Deep&amp;Cross（DCN）模型 除了用FM代替LR以外，2017年发表的Deep&amp;Cross Network（DCN）模型，提出了一种新的Cross网络来建模Wide部分，可以用更高效的方式实现更高阶的交叉。 deep端改进 NFM&#x2F;AFM模型 对Deep部分的改进，Neural Factorization Machines（NFM）模型[14]是一个比较有效的方案，由Xiangnan He等人于2017年提出。NFM作者认为，经典DNN框架中的concat层只是把各特征embedding拼接成一个向量，并没有在底层利用好特征交互信息，只能依赖后面的MLP全连接层来建模特征之间的相互关系。但由于深度网络有梯度消失、梯度爆炸和过拟合等问题，DNN并不是很好优化，所以最终会导致模型效果有损失。所以NFM的主要改进点是，引入Bi-Interaction Pooling层，替代经典DNN的concat层，在底层增加足够的特征交互信息后，再馈入到MLP网络做进一步的高阶非线性建模。 引入新的子网络 xDeepFM xDeepFM模型，全称eXtreme Deep Factorization Machine，是微软研究人员在Deep&amp;Cross（DCN）模型基础上进行研究和改进后提出的。xDeepFM的主要创新点是，在传统Wide部分和Deep部分之外，提出了一种新的特征交叉子网络，Compressed Interaction Network（CIN），用于显式地以向量级vector-wise方式建模高阶交叉特征。 3.5 引入注意力机制相比黑盒模型，注意力机制增强了可解释性，并且这些注意力权重是在翻译时根据输入动态计算的，模型的自适应能力也获得提升。基于这些优点，注意力机制在广告&#x2F;推荐系统的排序模型上也有很多的探索和应用。 AFM AFM模型可以认为是NFM模型的后续。在NFM模型中，不同域的特征embedding向量经过特征交叉池化层的交叉，将各交叉特征向量进行“加和”，输出最后由多层神经网络组成的输出层。 问题就在于 sum pooling操作，相当于将所有交叉特征赋予相同的权重，不考虑不同特征对于结果的影响程度，实际上消解了大量有用的信息。 而模型通过引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络实现的。 图的前三部分 都和FM是一样的。而后面的部分，则是AFM创新所在，就是attention net AutoInt 2018年提出的AutoInt模型是一个使用多头自注意力机制增强模型解释性，同时又具备高阶交叉建模能力的模型 FiBiNET FiBiNET，是结合特征重要性和双线性特征交互的CTR预估模型，由新浪微博机器学习团队发表在RecSys19。 其新颖的点是加入了senet layer 和 Bilinear-Interaction Layer SENET SENET Layer的主要作用是学习不同特征的重要度，对不同特征向量进行加权。即该Layer的输入为特征组embedding矩阵 E &#x3D; [e1, e2, …, em]，Layer内部学习出每组特征的重要度 A &#x3D; [a1, a2, …, am]，最后输出的“SENET-Like Embeddings”，V &#x3D; A * E &#x3D; [a1e1, a2e2, …, am*em]。SENET Layer本质上也是Attention机制 Bilinear-Interaction Layer： 原文作者认为传统的内积或哈达玛积形式难以有效对稀疏数据进行交叉特征建模，因此提出了一种双线性特征交叉方式（这种形式前文介绍双线性FFM时也有提到）， DIN Deep Interest Network（DIN）[21]是阿里妈妈广告算法团队在2017年提出的，DIN是一个工业应用性很强的方案，特别是在电商领域。 其中是为了解决传统DNN模型会将这些用户行为embedding向量通过固定的形式（sum&#x2F;max&#x2F;mean等）pooling成一个定长向量作为用户兴趣表示，所有用户的兴趣表示都被映射在一个相同的固定空间，而由于空间维度（即向量维度）的限制，用户兴趣的多样性特点无法得到充足的表达。 DIN对这个问题的解决方案是“局部激活”（Local Activation），即根据候选广告来自适应的为历史行为向量分配激活权重，以attention-based pooling的方式将其合并作为用户在该候选广告下的兴趣表示，使得与候选广告更相关的历史行为在用户兴趣表示中占更主导的作用。 核心模块Activation Unit 该论文的注意力模块是一大创新点，输入out product中候选物向量和用户历史行为向量进行交互（DIN模型b很清楚的解释了谁和谁交互），交互方式可以选择向量点乘、向量加或向量减（两个向量进行减法可以表示两者差异，结果小说明两个物品相似，加法相当于pooling，乘法表示两者之间相似度），文章中使用的是向量减，使得损失信息最少。交互是为了引入显性支持，利于后续建模，将交互后的向量与原始两个向量进行拼接，输入到多层感知机中，最后通过一个全连接层（可以看成与拼接向量同维度的一个向量进行点乘然后相加）得到这个拼接向量对应的权重，也就是这个用户历史行为向量的注意力得分。 DIEN DIEN，全称Deep Interest Evolution Network[22]，也是来自阿里DIN的提出团队，是DIN的进化版，旨在挖掘用户行为背后的更高抽象语义的用户兴趣演化规律。DIEN的两个主要创新点： 兴趣抽取层（Interest Extractor Layer），利用GRU序列建模，从用户历史行为中抽取兴趣状态表示h(t)，并且引入辅助loss利用下一时刻的行为信息帮助指导当前时刻h(t)的学习，更准确的建模用户兴趣向量。 兴趣演化层（Interest Evolving Layer），引入AUGRU（GRU with attentional update gate）建模用户兴趣的演化过程，在AUGRU的隐状态更新中引入目标广告和兴趣向量的attention因子，目的是使最终输出给DNN网络的兴趣终态h’(T)和目标广告有更高的相关度。 4. benchmarkBarsCTR: Open Benchmarking for Click-Through Rate Prediction  实验介绍 数据集 数据集 示例 特征总类 特征数 正比例 Criteo 46M 39 5.55M 26% Avazu 40M 24 8.37M 17% 数据拆分 训练：验证：测试&#x3D;8：1：1 评价指标 AUC 和 Logloss 实验结果 "},{"title":"知识追踪综述","date":"2022-09-25T13:21:32.486Z","url":"/2022/09/25/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"0 说点废话（讲一下怎么找到这个文章的）2022年7月16日 在床上百无聊耐的看视频 突然看到这样的东西  嗯嗯 我就去这个网站试了试  查了查 我留下的二课题 知识追踪（cool） 发现了一篇 2022-4-1 号的 中文的一篇综述 就很开心 0.1 废话之后根据 实验室要求 9\\21 俺 又根据一篇新的中文综述 来弥补这次文章的更新 文章名： Review of Knowledge Tracing Model for Intelligent Education doi号：  这次 更新 bkt更新 数据集更新 加 模型原型 GKT 模型解释 1. 前言-知识追踪是啥​ KT算法将学生的知识掌握程度随着时间的推移建模预测，从而能够准确地预测学生在未来互动中的表现据此有针对性地为学生订制不同的学习路线，提升学习效率.学生通过在线学习平台进行学习交互，形成答题行为时间序列，KT算法通过对学习者和序列联合建模，预测其对于新知识的认知概率分布，进一步推理出学习者的技能和认知水平。 2. KT问题定义以及比较常用缩写： KT knowledge tracing KC 是 知识点、概念、技能或者项目等通用术语 2.1 定义 2.2 数据集ASSISTments Data是KT领域最为经典的数据集，其中ASSISTments2009数据集 是绝大多数KT模型的标准数据集；EdNet［8］ 发布于2019年，是KT领域最新的数据集，提供了超过1亿条 学习者交互记录；Synathetic是 DKT模型所附带的数据集，包括了超过 20万条学习记录信息；其他数据集包括：Junyi15，algebra 2006 2007及Statics2011等. dataset Website Field Records&#x2F;k ASSISTments2009  Math 325 ASSISTments2012  Math 2541 ASSISTments2015  Math 683 ASSISTments2017  Math 942 KDD Cup2010  607 Ednet  English 131 441 Junyi15  Math 2 500 algebra 2006 2007  Math 180 Synathetic-5  Math 200 Statics2011  Engineering 361 slepemapy.cz  10087 2.3 常用评估指标 2.4 模型大类 模型大类 提出时间 原理 优势 局限性 适用场景 BKT 1994 HMM 模型简单，具有可靠的教学可解释性 依赖教育专家标注矩阵以及简化假设 适用于根据先验知识状态自动给每个学生推荐题目的场景，需要先得到先验分布 FAM 2006 logistic 模型简单，增加练习-kc的q矩阵，具有可解释性 依赖专家标注矩阵，需要手工输入特征 适用于从历史数据中学习一般参数对学生建模进而预测作答表现的场景，需要教育专家标注的 Q矩阵 DKT 2015 RNN&#x2F;Lstm 性能好，无需专家对KC进行编码 模型复杂，训练规模较大，不具备可解释性 适用于不需要解释学生知识状态、只需给出学生学习结果情况的场景,如智能组卷场景 DKVMN 2017 MANN 网络相对简单，提高模型记忆能力 参数多 训练规模大 适用于学生日常练习记录交互日 志，在练习-KC 单一映射的场景下快速建立学生知识状态掌握情况 GKT 2019 GNN 建模底层 KC 之间的关系 实际教学中 KC 划分粒度不一致直接影响学生知识状态评估性能 适用于练习-KC 之间存在多重复杂关系的场景，对学生知识状态掌握情况的细节要求较高，具备一定的可解释性 KT过程分析基于logistic回归方法项目反应理论（IRT）项目反应理论（IRT）该理论假设学习者的学习能力不随时间和实践变化。（该理论认为 只有答会高水平的题目 才能证明学生是高水平的） 项目反应理论[29] 是一种现代心理测量理论，其中 “项目”（item）指的是学生试卷中的题目，“项目反应” （item response）为学生在具体题目上的作答情况。 简言之，IRT 就是建立在学生能力和作答正确率的关系上，影响学生在项目上作答结果的主要因素有两个：其一是学生本身的能力水平；其二是试题项目的测量学属性，如项目难度、区分度和猜测性等 举个例子： 考生答对的题目难度是判断考生能力的标准。某考生答对10道难度为1的题目，获得的能力值依然是1，另一考生答对1道难度为8的题目，能力值则为8 其中rasch模型 使用难度描述 输入模型的问题 其中，θ 代表学习者的学习能力；b 代表问题的难度 .Rasch 模型在可解释性、问题区分性等方面性能优越，GHOSH等通过在深度模型中使用Rasch编码，提高了DKT的可解释性，取得了卓越的预测性能. 深度技术下 IRT模型 模型名 论文名 干啥 年份 Deep ⁃IRT Deep ⁃ IRT：make deep learning based knowledge tracing explainable using item response theory 它是 IRT 模型与 DKVMN 模型的结合 . 2022 EKPT Learning or forgetting？A dynamic approach for tracking the knowledge proficiency of student 提出知识熟练度追踪（KPT）模型和练习关联的知识熟练度（EKPT）模型，应用于知识估计、分数预 测和诊断结果可视化三个重要任务. 2020 KTMs Knowledge tracing machines：factorization machines for knowledge tracing 综合IRT，AFM，PFA等模型，提出了知识追踪机（KTMs）框架，KTMs利用所有特征的稀疏权值集，对学习者答题结果的概率进行建模. 2019 基于知识追踪的因子分析因子分析模型与 IRT模型相似，但是它们会通过学习知识水平的参数估计学生正确回答题目的概率。学习因子分析（learning factors analysis，LFA）源于学习曲线，是一种半自动化的方法，改进由统计 模型、人类专业知识与组合检索组成的认知模型。学习曲线证明了错误率与练习次数呈幂次关系，但 没有考虑到学生数量的庞大性，以及一个题目可能包含多种 KC。因此，LFA适应并扩展了学习曲线，假设不同学生的学习率相同 LFA模型的标准形式可参见式 性能因子分析（performance factors analysis，PFA）是 LFA 的一种改进，LFA 对练习时间很敏感，但是忽略了学生的正确和错误回答。PFA 假设每个学生都 是一个独特的个体，学生的学习不是仅仅通过相同的练习频率积累，每个学生也会通过其自身正确或错误的尝试来学习。 PFA 在LFA的基础上 进行了调整 Vie 等人采用因子分解机（factorization machines，FM）将其分类形式用于学生建模，并提出了知识追踪机（knowledge tracing machines, KTM），使用 FM 来交互每个特征进行预测。该方法将有关题 目与学生的侧面信息编码到参数模型中，即使观察到的学生数据是稀疏的情况下，也可以快速准确地 估计学生的知识状态。对观察到答题正确与否的二进制输出概率进行建模，对于每个题目回答正确的概率进行验证。但是，KTM 存在冷启动问题以及依 赖 KC的重复学习，对于一些不经常练习的 KC，KTM可能退化为 IRT。为了解决这些问题，Lai等人[35] 提出 了一种循环知识追踪机（recurrent knowledge tracing machine，RKTM）来改进 KTM。该方法根据学生的知识状态，在时间上丰富了 KTM 和难度、学生能力、技能以及学生技能实践历史的编码，利用其处理知 识 状 态 的 RNN 结 构 捕 捉 KC 之 间 的 关 系 ，以 便RKTM 可以找到相似的KC 并收集其相似信息以提供更精准的预测。 模型 方法概述 局限性 发表年份 LFA 源于学习曲线的一种半自动化的方法 对练习时间具有敏感性 2006 PFA LFA的改进方法，考虑题目的正误反应数量的学习累积 不能处理知识点之间的内在依赖性 2009 IRT 基于 IRT理论，为学生能力和题目难度建立参数模型 学生的能力水平在学习过程中是固定的 2018 KTM 利用 FMs将传统的 Logistic模型推广到更高的维度 冷启动问题，不能准确代表之前的学习序列 2019 RKTM 引入学生知识状态，与当前学习场景交互 特征提取困难，参数较多，增加了学习复杂度 2021 学习认知机制和遗忘机制认知机制WANG等提出了一种通用的神经认知诊断框架，摒弃人工特征，将神经网络集成到复杂的非线性交互模型中，解决认知诊断问题，并且结合CNN，提出了Neural CDM+模型，通过自动提取系统中的知识点信息，补充知识点相关度矩阵，避免了主观性甚至错误. 遗忘机制DKT模型使用RNN一定程度上实现了对记忆过程模拟，但是仍然没有真正意义上模拟人类思维习惯. 模型 论文名 方法 年份 LPKT Learning Process-consistent Knowledge Tracing 加了个遗忘层 主要是sigmoid 2021 DKVMN Dynamic key ⁃ value memory networks for knowledge tracing 过类似于计算机内存管理的方式，建立知识记忆遗忘矩阵，在模型可解释性上取得了很大的进步 2017 CKT Context-aware attentive knowledge tracing 基于Transformer的模型框架 上引入了注意力衰减机制，模拟全局遗忘行为，从而取得了较好的模型效果 2020 KT方法BKT 特点 标准BKT模型建模过程中将知识点设置为“永不忘记”，并且假设一个题目只对应一个知识点（按道理来说 一个题目是对应多个知识点的） 在其中有几种概率 2.1 P ( L)是初始知识状态下学生掌握相关知识点的概率 2.2 P (T )为经过练习后学生掌握目标知识点的概率 2.3 P (G)表示学生猜对答案的概率（有趣） 2.4 P ( S)为学生掌握知识点但做错题目的概率 结合个性化的扩展模型由于 BKT 模型并没有考虑到学生背景知识的个体化估计、先验知识的参数化等 局限性：依赖于简化的假设， 如每道题目仅涉及一 个 KC，学习过程中不存在遗忘情况等 模型 论文 方法 年份 modeling individalization in a bayesian networks inplementation of knowledge tracing 为每个学生设计不同初始背景的知识状态 2010 the impact on individalizing student models on necessary practice opportunities 设计学生导向模型，提高个体性差异 2012 Individualized Bayesian knowledge tracing models 将模型参数划分为知识部分和学生部分提高模型性能 2013 Traditional knowledge tracing models for clustered students 基于聚类学生进行贝叶斯知识追踪 2020 结合知识相关性BKT 对每个 KC 进行单独建模，导致模型对习题库以及 KC 与习题关联模型的依赖性较强，无法捕捉到不同 KC 之间的相关性。如果知识模型粒度太粗或太细，都会使 BKT 模型难以准确评估学生的知识状态；如果学生连续处理几道与同一个 KC 相关的类似习题，可能会对该学生后面的表现产生积极影响 局限性：需要设置阈值，而不 同类型 KC 需要不一 样的阈值范围以及设置依据 模型 论文 方法 年份 Using similarity to the previous problem to improve Bayesian knowledge tracing 使用 DBN表示 KC拓扑结构 2014 Structured knowledge tracing models for student assessment on coursera 考虑到题目相似性的 BKT-ST模型 2014 Dynamic Bayesian networks for student modeling 基于知识状态的层次性和时间特性进行建模 2016 Spectral Bayesian knowledge tracing 利用 DBK在单个模型中联合考虑不同的 KC 2017 结合节点状态BKT 是以一种二进制变量概率分布追踪并更新学生对某个 KC 潜在掌握情况的一阶 HMM，该模型 假设一个理想的无噪声环境且参数具有简并性。这显然是与实际情况不符的 局限性： 参数较多，计算量大，复杂度高 模型 论文 方法 年份 Extending knowledge tracing to allow partial credit: using continuous versus binary nodes 采用 0到 1的连续型表示法，细化学生的知识状态 2013 Proceedings of the 8th International Conference on Educational Data Mining, 用 3-gram代替二元节点状态的 Spectral BKT模型 2015 three learning states Bayesian knowledge tracing model 采用三支决策的思想改进二进制节点状态 2018 现实结合局限性：先验概率的确定存在主观性，简单的模型很难纳入实际情况的复杂性 模型 论文 方法 年份 introducing item difficulty to the knowledge tracing model 将题目的难度系数引入 BKT 2011 Affect and inference in Bayesian knowledge tracing with a robot tutor 融入学生的情感状态 2015 Dynamic knowledge tracing through data driven recency weights 引入学生答题情况近期率权重的 MS-BKT模型，细化学生知识状态 2020 融合行为和遗忘因素的贝叶 斯知识追踪模型研究 融合了学生的学习行为与遗忘因素 2021 DKT改进 可解释性问题改进 深度学习模型不具备类似传统模型的可解释性，很难弄清楚隐藏状态是如何代表学生的知识状态的，无法从隐藏 状态确定学生的知识掌握水平 On the interpretability of deep learning based models for knowledge tracing[J]. arXiv: 2101.11335, 2021. 缺少学习特征问题改正 DKT 模型的输入仅仅是练习标签的 one-hot 编码，排除了许多其他丰富 的信息和特性，如练习内容、学生尝试答题的次数以及答题持续时间等，而是将所有学生的表现平均化， 因此被认为不足以进行适应性学习 基于RNN的KT整体来讲，基于RNN结构的追踪模型在性能和可用性方面大幅度超越了传统模型，但是在解释性上略显不足. 模型 论文 方法 年份 DKT-DSC Deep knowledge tracing and dynamic student classification for knowledge tracing 通过在每个时间间隔内将学生分组，预测学生的学习效果 2018 .。。 Incorporating features learned by an enhanced deep knowledge tracing model for stem&#x2F;non-stem job prediction 采用DKT进行知识状态预测，证明了DKT模型在实际工作中的有效性. 2019 EERNN Exercise⁃enhanced sequential modeling for student performance prediction 通过追踪学生的练习记录和相应练习的文本内容，提出了一个通用的练习增强循环神经网络（EERNN）框架 2018 基于注意力的DKT通过注意力机制，可以在过去的交互序列中寻找到与当前问题相关的重信息，从而做出更为准确的预测，并且证明了基于 Transformer的模型比基于 RNN的模型在运算速度上快了一个数量级. 模型 论文 效果 年份 transformer A self ⁃ attentive model for knowledge tracing 必然伴随着对过去相关练习交互的回忆 2019 双向transformer Towards an appropriate query，key，and value computation for knowledge tracing 将练习序列和回答序列分别进行编码，从而寻找到了更为合适的 Query 2020 Saint+ Saint+：integrating temporal features for EdNet correctness prediction 将经过时间、滞后时间两个特征编码与学生答题响应的编码进行结合，从而增强了 模型的预测精度. 2021 基于hawkes过程的DKT Hawkes过程则假设过去事件会在一定程度上提高未来事件发生的概率，并且这种影响会随着时间指数衰减，这种思想比较符合认知遗忘规律下的学习者能力. 模型 论文 效果 年份 LSTM The neural Hawkes process：a neurally self⁃modulating multivariate point process 利用Hawkes过程对长短期记忆（LSTM）节点的时间效应（遗忘效应）进行衰减处理.KT领域的学习者交互过程可以被看作是一系列的连续事件流，但是泊松过程假定事件相互独立，并不符合多知识点状态下学习者交互的逻辑 2017 Hawkes Process Temporal cross⁃effects in knowledge tracing 定事件相互独立，并不符合多知识点状态下学习者交互的逻辑忘效应）进行衰减处理.KT领域的学习者交互过程可以被看作是一系列的连续事件流，但是泊松过程假 深入研究了不同知识点之间的时间交叉效应，并且提高了深度模型的可解释性 2021 GKT知识关系– 使用图来表示知识关系（知识与知识之间具有复杂的逻辑关系） 局限性：计算密集型，易受数据集大小的限制，由于 KC 划分 粒度不一致，可能会直接 影响学生知识状态的评估 性能 模型名 论文 方法 年 GKT Graph-based knowledge tracing: modeling student proficiency using graph neural network 利用 GNN构建 KC关系图 2019 GIKT GIKT: a graph- based interaction model for knowledge tracing 利用 GCN提取练习-KC关系图中包含的高阶关系信息 2020 HGKT HGKT: introducing problem schema with hierarchical exercise graph for knowledge tracing 结合练习之间的层次关系，建模练习学习依赖性 2020 JKT JKT: a joint graph convolutional network based deep knowledge tracing 联合图卷积网络提取隐藏在“练习-KC”图中的深层隐式信息 2021 DGMN Deep graph memory networks for forgetting-robust knowledge tracing 利用外部记忆结构的知识状态动态构建潜在 KC 及其 关系图，同时考虑遗忘行为 2021 Peer ⁃inspired student performance prediction in interactive online question pools with graph neural network 在 R-GCN 的基础上，利用学生互动过程，构建了“学生—互动—问题”网络，提出了 R2GCN 模型 2020 动态键值网络由于 DKT 模型以隐藏状态代表学生对 KC 的掌握情况，无法详细输出学生对每个 KC的掌握程度， 并且 LSTM 将所有记忆存储在一个隐藏向量中，这使得 LSTM 很难准确地记录拥有数百个时间步长的序列。 Zhang 等人 借鉴 MANN，结合 BKT 和DKT 的优点提出 DKVMN 模型，该方法允许网络保留多个隐藏状态向量，分别进行读写。 模型名 方法 论文 年份 DKVMN 借鉴 MANN，利用静态、动态外部矩阵分别读写学生知识状态 Dynamic key- value memory networks for knowledge tracing 2017 DKVMN-CA 改进 DKVMN，支持人工标注概念树 Concept- aware deep knowledge tracing and exercise recommendation in an online learning system 2019 LPKT 采用 DKVMN，结合学生的知识现状，完善模型的遗忘机制 Knowledge tracking model based on learning process 2020 DKVMN-LA 引入学生学习能力与行为特征的多功能知识追踪算法 Dynamic key-value memory networks with rich features for knowledge tracing 2021 注意力机制针对于 前四种kt方法是针对网络结构的一种既定改变，引入注意力机制是一种特殊的网络结构。 模型名 理由 方法 年份 EERNNA 首次考虑到练习文本的特征 双向lstm 提供嵌入文本特征，嵌入练习文本特征建模学生的学习过程 2018 SAKT 首次将 Transformer模型应用于知识追踪领域 基于自注意力机制建模学生的交互历史，减少无关练习对目标练习的影响 2019 SAINT 认为SAKT 模型的注意力层太浅且没有对 Q、K 与 V 进行充分发掘建模 改进 SAKT，基于深度自注意层建模练习和学生回答之间的关系 2020 AKT 完全依赖注意力 建模题目与回答的上下文感知，表示提取学生的猜测与失误特征 2020 RKT 引入了一个包含上下文信息的关系感知自注意力层，同时保持了自注意力机制的简单性和灵活性。 利用上下文信息来增强自注意力机制，采用对指数衰减核函数建模学生遗忘行为 2020 EKTA 改进 EERNNA，追踪学生对特定 KC的掌握程度 2021 MF-DAKT 使用预训练方法来合并练习关系和难度水平信息丰富了模型的题目表示 DAKT 从不同角度捕捉因子和因子相互作用中包含的信息 2021 ATKT DNN存在过拟合风险，导致泛化能力有限 利用高效注意力-LSTM 自适应聚合先前知识隐藏状态的信息，通过 AK增强模型的泛化能力 2021 benchmark 展望本文作者对比讨论了目前主流的KT模型，分析了主流模型的优缺点.目前的研究主要针对知识点与题目间的关系进行建模，很少有研究从模型效果评价指标、学习潜力预测、深度记忆过程模拟等方面进行知识状态追踪和预测，同时也较少有对多知识点关系建模方法进行知识状态追踪的研究.通过分析KT领域目前主流的模型，梳理出KT领域未来的发展方向，从数据表征、认知建模、建模方法、解释及反馈方面对KT领域进行展望. 1）数据处理及数据表征.KT模型在运用输入数据方面越来越需要预处理、预训练操作.预训练模型在序列任务上表现出了良好的性能，采用可解释性较强的算法预处理输入数据变得越来越重要.比如使用Rasch编码预处理输入数据后，再进行注意力运算和模型预测，在模型性能和可解释性方面都取得了很好的效果 .在数据特征方面，引入学习者生物特征、更加丰富的习题特征都是未来重要的突破方向，KT模型应该向更高维度、更普适、更泛化的方向发展，如何对学习者的非结构性学习数据进行追踪也是重要的发展方向. 2）认知建模 .认知诊断和 KT分别应用于学习者静态数据分析和动态数据分析，但 KT模型内不应缺乏对学习者认知能力的建模.对于问题维度、知识点维度的建模不足以拟合学习者的知识状态变化，应在此基础上进一步对认知维度进行建模，从而在更高的维度上追踪学习者的状态变化情况. 3）模型方法及可解释性 .自从 DKT被提出以来，KT领域内的模型基本以深度模型为主，但越来越多的工作表明 DKT无法做到真正的动态自适应 KT.基于 RNN 的模型在数据拟合能力上逐步被以注意 力机制为核心的Transformer类模型超越，未来KT领域建模方法应该在注意力方向、图谱方向进一步发展.人脑记忆的形成过程中，人自身的注意力是重要的一环，这也是基于注意力机制模型结合遗忘建模取得不错效果的关键原因.知识图谱作为非结构化知识表征的重要手段，在KT领域有更进一步的潜力， 并且对于认知能力研究也可以加入图谱技术，从而在可解释性KT方向取得突破."},{"title":"毕业条件 以及 未来的奋斗方向","date":"2022-09-23T14:38:29.780Z","url":"/2022/09/23/%E4%BC%9A%E8%AE%AE%E6%80%BB%E7%BB%93/","categories":[["undefined",""]],"content":"5&#x2F;28日 有幸 有两个同学读博士 9&#x2F;30 - 12&#x2F;17 咱也不晓得是个好事还是个坏事 今天的帮老师 真的是 把我震惊到了 还有教育机器人 不会说招我是来干这个的吧 吐槽完成 开始码字 徐老师的总结 问题化学习 基础 发文章 新文章 掌握 创新点 思路 把握好 个人战斗 和 团队工作 top顶会 博客 论文 中文文献也得看 模型-代码复现-idea 做好文献整理（从几个方向进行总结 给文章打好标签） 项目申请 申请项目 注意申请内容 记住后期结题 毕业答辩 注意格式 两章算法 一章系统 小论文 多发小论文 代码和模型（问的多） 2-3篇 实验做得好 文章差不多一个星期能写好 工作方向突感感觉 自己就只有一年的时间 来准备自己的工作计划了 挺焦急的 现在是秋招提前批 注意胆子大 加油！"},{"title":"zhihurec 推荐数据集","date":"2022-09-14T12:52:38.125Z","url":"/2022/09/14/zhihurec-%E6%8E%A8%E8%8D%90%E6%95%B0%E6%8D%AE%E9%9B%86/","categories":[["undefined",""]],"content":"说些废话推荐数据集 跟知识分享相关的 这个数据集是 知乎10天内收集约100万次交互 798k用户 165k问题 等关键字组成 一般用于 top-N推荐、顺序推荐和上下文感知推荐 除推荐以外 用户性别预测、最有价值的回答者识别和高质量的答案识别 目前的 缺乏推荐系统用户交互日志 前言论文名：A Large-Scale Rich Context Query and RecommendationDataset in Online Knowledge-Sharing 链接： 论文链接  代码链接  文章介绍链接：（知乎） 相关知识： 数据集探究 数据集 按静动态划分 静态 评分 收藏夹 动态 点击（积极） 印象（消极） 印象 是 我最难理解的一个量 现在理解了一丢丢 好像就是 目录中 没有点击的问题对 其数据集分为 印象 查询 用户配置文件 答案 问题 作者配置文件 数据集种类 100M 20M 1M 三种 能做的任务topN，序列推荐，上下文感知，最有价值的回答，高质量回答识别"},{"title":"github的使用 以及 hexo这个玩意","date":"2022-09-11T14:40:50.976Z","url":"/2022/09/11/%E5%88%B7github%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/","categories":[["undefined",""]],"content":"   0. 前言今天碰到的东西 很奇怪 主要是对于昨天对于自己的项目 没有push上去 但弄到现在 发现 自己的博客日期跟个鬼一样 这个 等会再看看 博客解决三个问题 重新更换电脑的git帐号设置 hexo的重新安装 github仓库的版本回滚 1. git 帐号设置对于这样的东西  这个链接是 修改全局用户名以及邮箱地址 这里 需要同 这里的名字和邮箱相同 然后 这边 需要添加一下 ssh的添加  2. hexo重新安装 这个讲的真不错 3. 版本回滚如果一个仓库 进行了多次的commit 当你想回到某一个版本的时候 你就可以看看这个 说实话 解决方法找了好多 "},{"title":"数据数仓hive 入门","date":"2022-09-11T14:40:50.966Z","url":"/2022/09/11/%E6%95%B0%E6%8D%AE%E6%95%B0%E4%BB%93hive-%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":" OLTP -&gt; ETL （抽取 转换 加载）-&gt; OLAP 数仓数仓的主要特征 面向主题 主题是一个抽象的概念，是较高层次上数据综合、归类并进行分析利用的抽象。 集成性 主题相关的数据通常会分布在多个操作型系统中，彼此分散、独立、异构。需要集成到数仓主题下。 非易失性 也叫非易变性。数据仓库是分析数据的平台，而不是创造数据的平台。 时变性 数据仓库的数据需要随着时间更新，以适应决策的需要。 hive开始Apache Hive是一款建立在Hadoop之上的开源数据仓库系统，可以将存储在Hadoop文件中的结构化、半结构化数据文件映射为一张数据库表，基于表提供了一种类似SQL的查询模型，称为Hive查询语言（HQL），用于访问和分析存储在Hadoop文件中的大型数据集。Hive核心是将HQL转换为MapReduce程序，然后将程序提交到Hadoop群集执行。 为什么使用操作接口采用类SQL语法，提供快速开发的能力(简单、容易上手)避免直接写MapReduce，减少开发人员的学习成本支持自定义函数，功能扩展很方便，背靠Hadoop，擅长存储分析海量数据集 hive 与 hadoop之间的关系Hive利用HDFS存储数据，利用MapReduce查询分析数据。Hive的最大的魅力在于用户专注于编写HQL，Hive帮您转换成为MapReduce程序完成对数据的分析。 底层猜想 映射关系（文件和表之间的对应关系） 映射在数学上称为一种对应关系 在 hive中 映射指的是对 mysql文件 映射成表的功能 因此 映射信息专业的应该被称为 元数据信息（metadata） sql语法的解析 用户写完sql之后，hive需要针对sql进行语法校验，并且根据记录的元数据信息解读sql背后的含义，制定执行计划。并且把执行计划转换成MapReduce程序来具体执行，把执行的结果封装返回给用户。 架构 用户接口 包括CLI、JDBC&#x2F;ODBC、WebGUI。其中，CLI(command line interface)为shell命令行;Hive中的Thrift服务器允许外部客户端通过网络与Hive进行交互，类似于JDBC或ODBC协议。WebGUI是通过浏览器访问Hive。 元数据存储 通常是存储在关系数据库如mysql&#x2F;derby中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性(是否为外部表等），表的数据所在目录等。 驱动程序 完成 HOQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储住HDPS 中，开仕随后有执行引擎调用执行。 执行引擎 Hive本身并不直接处理数据文件。而是通过执行引擎处理。当下Hive支持MapReduce、Tez、Spark3种执行引擎。 hive 元数据元数据 为 描述数据的数据 hive metadata 即为hive元数据 包含用Hive创建的database、table、表的位置、类型、属性，字段顺序类型等元信息。 元数据存储在关系型数据库中。如hive内置的Derby、或者第三方如MySQL等。 metastore 即元数据服务 metastore 服务的作用时管理metadata元数据 metastore 配置方式 metastore远程模式 远程模式注意的点 启动主要是 结构的奇怪 在hive 安装的服务器上 首先启动metastore服务 然后启动hiveserver2服务 metastore服务启动 前端启动 前台启动，进程会一直占据终端，ctrl + c结束进程，服务关闭。可以根据需求添加参数开启debug日志，获取详细日志信息，便于排错。 &#x2F;export&#x2F;server&#x2F;apache-hive-3.1.2-bin&#x2F;bin&#x2F;hive –server metastore 后台启动 nohup &#x2F;export&#x2F;server&#x2F;apache-hive-3.1.2-bin&#x2F;bin&#x2F;hive –service metastore &amp; hiveserver2服务启动hive很奇怪 必须启动完 第一代客户端之后 才能启动二代客户端 命令 nohup &#x2F;export&#x2F;server&#x2F;apache-hive-3.1.2-bin&#x2F;bin&#x2F;hive –service hiveserver2 &amp; 连接&#x2F;export&#x2F;server&#x2F;apache-hive-3.1.2-bin&#x2F;bin&#x2F;hive node3 连接 node1 启动的服务 "},{"title":"RKT  一个使用题目文本计算相似度的算法","date":"2022-09-11T14:40:50.886Z","url":"/2022/09/11/RKT%20%20%E4%B8%80%E4%B8%AA%E4%BD%BF%E7%94%A8%E9%A2%98%E7%9B%AE%E6%96%87%E6%9C%AC%E8%AE%A1%E7%AE%97%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E7%AE%97%E6%B3%95/","categories":[["undefined",""]],"content":"说些废话对于这个项目的实现 前言论文名：RKT:Relation-Aware Self-Attention for Knowledge Tracing 链接： 论文链接  代码链接 （文章公开了一个包含题目描述的数据集） 文章介绍链接：（知乎） 1. 相关背景从本质出发，这篇文章 对于 遗忘效应 以及 题目相关性计算上提出了较新的方式 1.1 核心思想 遗忘效益 随着时间的流逝 time的变化 会随着时间 不断的流逝 对于公式的体现 图！ 题目相关性 题目的词向量转换 图！ 通过余弦相似度 来计算 对应的 文本的相似性 作答数据 相关性计算 图！ 训练关系矩阵计算 图！ 2. 实证分析3. 问题描述正常的知识追踪方法 4. 方法（基本为论文方法部分）我们已经得到了训练的A矩阵，以及上述的遗忘的阶段计算出来的 RT 图！ RT 为 训练题目相关的向量（类似于 我求了一个序列 对于最后答案的贡献度） 数据输入图！ E 为 题目编码的向量 rj 为 长度为d 题目正确与否的扩充 p 为 位置编码 5. 实验6. 总结7. 自己的想法"},{"title":"大数据学习","date":"2022-09-11T14:40:50.866Z","url":"/2022/09/11/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/","categories":[["undefined",""]],"content":"2022&#x2F;8&#x2F;30日 开始  课程的学习 预计花费时间两周 数据分析基本步骤 明确分析目的和思路思路是使分析框架体系化，比如先分析什么，后分析什么，使各分析点之间具有逻辑联系，保证分析维度的完整性，分析结果的有效性以及正确性，需要数据分析方法论进行支撑； PEST分析法 数据收集 互联网公开数据业务数据日志数据爬虫数据 数据处理 数据预处理 数据清洗 数据转化 数据提取 数据计算 数据分析 数据分析方法 数据分析软件 数据展现 数据可视化 分析结果图表展示 报告撰写 数据分析报告是对整个数据分析过程的一个总结与呈现把数据分析的起因、过程、结果及建议完整地呈现出来，供决策者参考需要有明确的结论，最好有建议或解决方案 分布式与集群共同点：多台机器 分布式： 多台机器每台机器上部署不同组件 集群：多台机器每台机器上部署相同组件 vimware第一件事 是调整虚拟机 与 本机 是在同一个网段下面 vm中 是 编辑-》虚拟网络编辑器-》更改设置 win下 是 网络连接 -》vm8 -》tcpipv4 -》设置网段 挂起 类似软件冻结 再次开机恢复原来状态 快照 可以将虚拟机 恢复到我们之前设定好的状态 （几乎是一瞬间回复） 说实话 这个功能今天才知道 算是一个大杀器 ssh 公钥私钥成双成对 都是服务器生成的 服务器将公钥给客户端 客户端使用公钥进行加密 最后服务器将加密数据用私钥进行解密 就完成了这个过程 linux 文件系统 操作系统中负责管理和存储文件信息的软件机构称为文件管理系统，简称文件系统; 文件系统的结构通常叫做目录树结构，从斜杠&#x2F;根目录开始; Linux号称“万物皆文件”，意味着针对Linux的操作，大多数时间是在针对Linux文件系统操作。 vim编辑器 Apache Hadoophadoop集群包括两个 HDFS YARN 两个集群 逻辑上分离 物理上在一起 两个集群 是标准的主从架构 hadoop 安装包目录结构 结构 配置文件 第一类 java路径 和 运行的进程 第二类 核心模块的配置 （hdfs mapreduce yarn） 外加 核心模块设置 第三类 表示 小弟从角色运行在什么机器上 将hadoop添加配置到环境变量 大G小o 来到最后一行进行编辑 然后 source &#x2F;etc&#x2F;profile 弄好配置文件 namenode format 初始化操作（只需要做一次） hdfs namenode -format 本质上是初始化工作 进行hdfs清理和准备工作 成功的样子！ hadoop开启 开启脚本 开启成功检测方法 web ui开启HDFS 集群 yarn集群 HDFS 命令行 web ui 操作 hadoop重要组件-hdfsHDFS - hadoop分布式文件系统 一个分布式存储系统核心属性 分布式存储 元数据记录 分块存储 副本机制（冗余存储） HDFS 设计场景 是 大文件 大数据场景 文件系统协议 命令行常用指令 HDFS 角色划分 namenode datanode secondnamenode namenode的秘书 一般是给namenode分担压力 但是 不能替代namenode 写数据流程三个核心概念 pipeline管道 ack应答响应 副本存储 MapReduceMapReduce-分布式计算框架MapReduce的思想核心是“先分再合，分而治之”。所谓“分而治之”就是把一个复杂的问题，按照一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，分别找出各部分的结果，然后把各部分的结果组成整个问题的最终结果。这种思想来源于日常生活与工作时的经验。即使是发布过论文实现分布式计算的谷歌也只是实现了这种思想，而不是自己原创。 MAP 负责拆分 即把复杂的任务分解为若干个“简单的子任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。 Reduce 负责 合并 对map阶段结果进行全局汇总 在当下 MR框架已经退居二线 局限性 实时计算性能差，主要用于离线作业 不能进行流式计算 流式计算的特点是数据源源不断得计算，并且数据是动态的 MapReduce 实例进程MRAppMaster:负责整个MR程序的过程调度及状态协调MapTask :负责map阶段的整个数据处理流程ReduceTask:负责reduce阶段的整个数据处理流程 阶段组成 一个编程模型中组成 1个map 1个reduce 只有map 数据类型 mapreduce 程序运行 map阶段执行过程 reduce阶段执行过程 shuffle机制 map端shuffle reduce端shuffle shuffle机制弊端 Shuffle是MapReduce程序的核心与精髓，是MapReduce的灵魂所在。 Shuffle也是MapReduce被诟病最多的地方所在。MapReduce相比较于Spark、Flink计算引擎慢的原因，Shuffle机制有很大的关系。 Shuffle中频繁涉及到数据在内存、磁盘之间的多次往复。 YARN - 通用资源管理系统和调度平台 资源管理系统:集群的硬件资源，和程序运行相关，比如内存CPU等。 调度平台:多个程序同时申请计算资源如何分配，调度的规则（算法）。 通用∶不仅仅支持MapReduce程序，理论上支持各种计算程序。YARN不关心你干什么，只关心你要资源，在有的情况下给你，用完之后还我。 可以把Hladoop YARN理解为相当于一个分布式的操作系统平台，而MapReduce等计算程序则相当于运行于操作系统之上的应用程序，YARN为这些程序提供运算所需的资源(内存、CPU等）。 Hadoop能有今天这个地位，YARN可以说是功不可没。因为有了YARN，更多计算框架可以接入到HDFS中，而不单单是MapReduce，正是因为YARN的包容，使得其他计算框架能专注于计算性能的提升。 HDFS可能不是最优秀的大数据存储系统，但却是应用最广泛的大数据存储系统，YARN功不可没。 YARN 三大组件 RM（ResourceManager） YARN集群中的主角色，决定系统中所有应用程序之间资源分配的最终权限，即最终仲裁者。接收用户的作业提交，并通过NM分配、管理各个机器上的计算资源。 NM (NodeManager) YARN中的从角色，一台机器上一个，负责管理本机器上的计算资源。根据RM命令，启动Container容器、监视容器的资源使用情况。并且向RM主角色汇报资源使用情况。 AM (ApplicationMaster ( App Mstr ) ) 用户提交的每个应用程序均包含一个AM。应用程序内的“老大”，负责程序内部各阶段的资源申请，监督程序的执行情况。 程序交互流程 核心交互流程 MR作业提交 Client -&gt; RM 资源申请 App Mstr -&gt; RM MR 作业状态汇报 Container ( Map|Reduce Task ) –&gt;Container ( MrAppMaster ) 节点状态汇报 NM -&gt; RM 用户向YARN 提交了一个应用程序后 YARN分两个阶段运行该程序 第一个阶段： 客户端申请资源启动本次程序 AM 第二个阶段： AM根据本次程序内部具体情况，为它申请资源，并监控它整个运行过程 分步讲解 用户通过客户端向YARN中ResourceManager提交应用程序（比如hadoop jar提交MR程序）; ResourceManager为该应用程序分配第一个Container(容器），并与对应的NodeManager通信，要求它在这个Container中启动这个应用程序的ApplicationMaster。 ApplicationMaster启动成功之后，首先向ResourceManager注册并保持通信，这样用户可以直接通过ResourceManage查看应用程序的运行状态（处理了百分之几）; AM为本次程序内部的各个Task任务向RM申请资源，并监控它的运行状态; 一旦ApplicationMaster 申请到资源后，便与对应的NodeManager通信，要求它启动任务 NodeManager为任务设置好运行环境后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务 各个任务通过某个 RPC 协议向 ApplicationMaster汇报自己的状态和进度，以让 ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。 应用程序运行完成后，ApplicationMaster向 ResourceManager 注销并关闭自己 scheduler 资源调度器调度策略"},{"title":"结构化比赛入门","date":"2022-08-09T02:04:56.423Z","url":"/2022/08/09/%E7%BB%93%E6%9E%84%E5%8C%96%E6%AF%94%E8%B5%9B%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"说些废话9.9 买不到吃亏 买不到上当 加油吧 通过第一个课 感知到了 这个比赛的快乐 哈哈哈！ 前言比赛名：RecSys Challenge 2022 链接： 代码链接  文章介绍链接：（知乎）pandas操作 相关知识： code印象较深的pandas操作 "},{"title":"sine 论文阅读","date":"2022-08-06T02:28:39.021Z","url":"/2022/08/06/sine-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","categories":[["undefined",""]],"content":"说些废话目前 论文分析要求 很重 而且 我心很杂 需要一套班子 前言论文名：SINE Sparse-Interest Network for Sequential Recommendation 链接： 论文链接  代码链接  文章介绍链接：（知乎）  相关知识： 序列推荐 概念原型 工业界 1. 相关背景拟解决的问题：推荐系统的召回阶段仅用一个user embedding无法表征用户丰富的意图。目前通常利用聚类的方法生成多兴趣embedidng的方法，但现实场景的item categories太多 ，聚类不现实。 1）如何有效地利用概念原型对大量的item在细粒度上进行聚类是一项重要的任务。 2）一个人通常只与一组稀疏的概念进行交互，需要准确从全量概念原型中抽出一部分 1.1 核心思想 通过在一个大的核心概念池中 提取对应序列的核心的几个兴趣点 准确从概念原型中抽出一部分对用户意图进行表示 2. 实证分析 如SASRec仅用单一的emb建模用户丰富的兴趣容易受到recent行为影响 3. 问题描述序列推荐 4. 方法（基本为论文方法部分）实验模型 稀疏兴趣 多兴趣融合 5. 实验5.1 消融实验 结论： 多头在小数据集结果好 多兴趣嵌入比单兴趣好 5.2 超参数设计 6. 总结文章参照显式方法进行的兴趣构建 将SINE与现有方法进行比较，这些方法侧重于在推荐召回阶段提取用户的多兴趣emb。粗略地将它们分为两类，并在下面分析了它们的区别。 隐式方法：这种方法依赖强大的神经网络来隐式聚类历史行为并提取不同的兴趣。例如，MIND [24] 利用 Capsule 网络 [34] 自适应地将用户的行为聚合到兴趣嵌入向量中。 SASRec [21] 采用多头自注意力机制 [43] 为用户输出多个表示。与这些方法相比，我们的模型属于一种显式方法，该方法基于潜在的概念原型从用户的行为序列中显式地检测意图。 显式方法：维护一组概念原型，以明确确定用户行为序列中item的意图。 MCPRN [44] 是最近的一项代表性工作，用于从Session中提取多个兴趣以完成序列推荐。 DisenRec [29] 利用潜在原型来帮助学习用于推荐的解耦表示。与它们相比，文章也遵循显式方法，但SINE可扩展到大规模数据集。具体来说，前人的工作要求不同兴趣emb的数量等于概念原型的数量。然而，潜在概念的数量取决于应用程序，并且在工业推荐系统中可以很容易地扩展到数百甚至数千，这阻碍了它们在实践中的应用。相比之下，SINE能够自动从大型概念池中推断出一组稀疏的首选意图。 7. 自己的想法现在的我 看过源码 我算是基本没什么想法"},{"title":"CauseRec 反事实生成的序列推荐","date":"2022-08-04T02:08:37.612Z","url":"/2022/08/04/CauseRec-%E5%8F%8D%E4%BA%8B%E5%AE%9E%E7%94%9F%E6%88%90%E7%9A%84%E5%BA%8F%E5%88%97%E6%8E%A8%E8%8D%90/","categories":[["undefined",""]],"content":"说些废话2022&#x2F;8&#x2F;4 在8&#x2F;2日 在写完core这篇文章的时候 发现 文章的一个思路非常像这篇文中的反事实生成这个部分 还有 我其实写完了sine的文章 但是 学校服务器不让用 就离谱 然后发现 这个东西 我以前度过 说实话 没有整理出来 我东西都忘光了 说实话 现在回头看 自己还是看的不怎么清晰 前言论文名：CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation 链接： 论文链接  代码链接  文章介绍链接：（知乎） 相关知识： 1. 相关背景由于记录的用户交互的噪声和稀疏性，仅对观测行为序列进行建模可能最终导致系统脆弱和不稳定。 1.1 核心思想CauseRec通过替换原始概念序列中的可有可无和不可或缺的概念，从反事实数据分布中有条件地采样用户概念序列。利用从合成用户序列获得的用户表示，CauseRec通过对比反事实和观察数据来执行对比用户表示学习。 对比学习设计了模型无关和非侵入性框架，帮助任何基线模型以端到端的方式学习更有效的用户表示。通过将原始用户表示与反事实正样本和反事实负样本进行对比，这种表示更加准确和稳健 反事实建议 专注于去噪用户表示学习，并考虑了回顾问题，即“如果我们干预观察到的行为序列，用户表示会是什么？”。从技术上讲，我们提出了几个基于识别不可或缺&#x2F;可有可无概念的反事实转换，并设计了几个对比目标，用于学习准确和稳健的用户表示 2. 实证分析 这个实证 是根据作者经验得到 在推荐系统中，用户通常仅与有限数量的项目进行交互，而在大型实时系统中，项目库很容易达到1亿个。因此，仅对既稀疏又有噪声的观测行为序列进行建模可能最终导致不太令人满意的脆弱系统。 3. 问题描述序列推荐 从顺序推荐的观点来看，数据集可以表示为D&#x3D;{（xu，t，yu，t）}u&#x3D;1,2，…，N，t&#x3D;1,3，…，Tu，其中xu，t&#x3D;{yu，1:（t−1） }表示用户在第t个行为yu，t之前的历史行为，并按时间顺序排列，Tu表示用户u的行为数。顺序推荐的目标是预测下一个项目yu，t，给定历史行为xu，t，其可以表示为建模所有可能项目的概率 同样的 因为要偶然删除子序列 所以 输入被定成了（关键是 有时候还得提取子序列） (xu,t,yu,t ) 4. 方法（基本为论文方法部分）注意，文章提出的框架CauseRec的所有变体都是在基础模型上展开的。 4.1 整体架构模型的本质是 回答追溯问题 具体来说，我们首先确定历史行为序列中不可或缺的概念。一个不可或缺的概念表示一个行为序列的子集，可以共同表示用户兴趣的一个有意义的方面。可有可无的概念表示在表示感兴趣的方面不太重要的噪声子集 4.2 归纳偏置（类似于先验） 通常情况下，我们不知道具体上帝函数的情况，但我们猜测它类似于一个比较具体的函数。这种基于先验知识对目标模型的判断就是归纳偏置（inductive bias）。归纳偏置所做的事情，是将无限可能的目标函数约束在一个有限的假设类别之中，这样，模型的学习才成为可能。 4.3 概念不可或缺&#x2F;可有可无的划分 Item-level Concepts Interest-level Concepts 反事实变换 旨在通过替换原始用户序列的一部分概念来构建分布外out-of-distribution的用户序列。这里的用户序列可以是众所周知的item序列，也可以是兴趣level的概念序列。基于全局总览中描述的归纳偏差，建议以 rrep 的速率替换已识别的必不可少&#x2F;可有可无的概念，以分别构建反事实的negative&#x2F;positive用户序列。直接删除不可或缺&#x2F;可有可无的概念似乎也是可行的，但替换具有不影响整体序列长度和概念的相对位置的优点。具体来说，文章维护一个先进先出队列作为每个level的概念memory，将从当前mini-batch中提取的全量概念加入队列，并使用dequeue的概念作为替代。（这部分没说清楚可能要看代码） 用户embedding生成 目标函数 除了最大化似然概率的目标函数外，文章引入几个对比学习的目标函数来学习更精确的更鲁棒的用户表示。 反事实和真实序列之间的对比 一个健壮的用户表示应该对序列中可有可无的概念不那么敏感。 因此，从反事实序列中学习到的具有必不可少的概念转换的用户表示应该远离原始用户表示。 类似，准确的表示应该更多地信任不可或缺的概念。 因此，从具有可有可无的概念转换的反事实序列中学习的用户表示应该直观地更接近原始用户表示。因此，文章使用三元组边际损失来衡量样本之间的相对相似性： 不过这里想强调的是，在 测试&#x2F;服务阶段，生成 user embedding 只用到了原始行为序列，不需要计算 proposal scores 以及 反事实用户序列生成。强调这个原因在于，文章计算 proposal scores 需要用到 target item，而 target item 在测试和服务阶段是看不到的。具体来说，CauseRec-H 和 CauseRec-In 在测试阶段完全一样，而CauseRec-Item 和 Base Model 的差别也是在训练阶段。 5. 实验验证：1）CauseRec对比SOTA的序列推荐模型性能提升了多少；2）消融实验和超参数对实验指标的影响；3）用户表示如何从反事实数据建模和对比表示学习中受益 效果 文章验证了 CauseRec 的三种架构，包括 CauseRec-Item (CauseItem)、CauseRec-Interest (CauseIn) 和 CauseRec-Hierarchical (CauseH)，观察到这些架构在各种baseline和三个不同指标上的明显改进。值得注意的是，CauseRec-H 对比之前的 SOTA ComiRec-SA&#x2F;DR 在 Amazon Books 数据集上提高了 +.0299（相对 22.1%）的 NDCG@50 和在关于 Gowalla 上提高了+.0179（相对 8.64%）的 Recall@20。 ComiRec 主要通过对给定用户的多个兴趣进行建模来产生最佳性能。然而，对嘈杂的历史行为进行建模可能会导致可能无法准确代表用户的多样化，最终导致较差的结果。 GRU4Rec 在 Gowalla 数据集上与 ComiRec 取得了相当好的结果。 GRU4Rec 可以有效地对行为序列中item之间的顺序依赖性进行建模。但是，由于严格的逐步编码过程，它可能更容易受到噪声的影响。相比之下，CauseRec 架构通过将用户表示远离反事实的negative的用户表示并将其拉近反事实的positive的用户表示来对抗用户行为中的噪音。此外，这些结果证明了 CauseRec 通过对反事实数据进行建模来应对分布外out-of-distribution的用户序列的泛化能力。 在三种 CauseRec 架构中，CauseRec-Item 是一种模型无关设计，这意味着它可以应用于任何其他顺序推荐系统，而无需对原始用户编码器进行任何修改，并且仅在训练阶段起作用，而不会牺牲推理效率（训练阶段才需要替换掉用户行为序列的一部分，测试阶段完全不需要。）CauseRec-Interest 通过将可能与某一兴趣相关的item（例如巧克力和蛋糕属于糖果）组合成一个整体概念来构建兴趣级别的概念。与 CauseRec-Item 相比，CauseRec-Interest 具有减少概念冗余和建模item之间高阶关系的优点，从而改进 CauseRec-Item。为了结合 CauseRec Interest 和 CauseRec-Item 的优点，CauseRec-Hierarchical 在反事实转换中同时考虑了兴趣级别和item级别的概念。 CauseRec-H 取得了最好的结果，这表明item级概念上的反事实替换仍然产生了一些独特的优势，例如对细粒度偏好进行建模。如人们通常不会喜欢所有的甜食，而更喜欢蛋糕而不是巧克力。（item-level能区分蛋糕还是巧克力，而兴趣level将二者都融合成甜食概念。） 消融实验 CauseRec-Item 具有以下优点：易于实现（与模型无关）、服务高效（测试阶段不需要做任何的修改）和有效。文章通过消融实验、更好地理解 CauseRec Item 中的不同模块： 6. 总结文章对反事实数据分布进行建模，以应对推荐系统中的用户交互的稀疏性和噪声行为。文章提出的CauseRec 有条件地对反事实的positive和negative用户序列进行采样，并对可有可无&#x2F;不可缺少的概念进行转换。文章提出了多种结构（-item、-interest、-hierarchical）来面对细粒度的item-level概念和抽象的兴趣level概念。此外，文章设计了几个对比目标来对比反事实与现实序列数据，以学习准确和鲁棒的用户表示。CauseRec-Item 具有模型无关的优点，即仅在训练中起作用而不影响evaluate&#x2F;inference效率。大量实验证明 CauseRec 的优势是设计简单且性能有效。 7. 自己的想法之前没看懂 现在觉得这个工作 很复杂 根本不想看下去 对对对 对于 项目的距离是没有一个明细的 （通过替换正负的项目 来实现 是作者的一个先验） 但是 这个操作是不是太过复杂 （理解还好） "},{"title":"core 在一致表示空间内基于会话的简单有效的推荐","date":"2022-08-02T03:11:21.347Z","url":"/2022/08/02/core-%E5%9C%A8%E4%B8%80%E8%87%B4%E8%A1%A8%E7%A4%BA%E7%A9%BA%E9%97%B4%E5%86%85%E5%9F%BA%E4%BA%8E%E4%BC%9A%E8%AF%9D%E7%9A%84%E7%AE%80%E5%8D%95%E6%9C%89%E6%95%88%E7%9A%84%E6%8E%A8%E8%8D%90/","categories":[["undefined",""]],"content":"说些废话看到题目 目的解决我的两个疑问 什么是一直表示空间 简单有效 前言论文名：CORE: Simple and Effective Session-based Recommendationwithin Consistent Representation Space 链接： 论文链接  代码链接  文章介绍链接：（知乎）无 相关知识： 会话推荐 推荐系统 1. 相关背景非线性编码器学习的会话嵌入通常与项目嵌入不在同一表示空间中，这导致在推荐项目时出现不一致的预测问题 1.1 核心思想设计了一种表示一致性编码器，将输入项嵌入的线性组合作为会话嵌入，保证会话和项位于相同的表示空间 使用了一个稳健的距离测量 来使 相近的在一块 其他的偏离 1.2 挑战 考虑到会话和项目嵌入共享一致的表示空间，如何设计更合适的编码器，以便我们能够利用深度非线性神经网络的巨大能力 一旦表示空间统一，项目嵌入直接涉及分数计算和模型优化，如何测量嵌入之间的距离以避免项目嵌入的过度拟合 2. 实证分析 看 实证分析的右侧 匿名用户 点击a 最后 甚至会跑到item b 那里去（离大谱） 事实证明 一致空间 从某种程度来说 也增加了可解释性 3. 问题描述会话推荐 4. 方法（基本为论文方法部分） 输入 有三 会话 下一个item 其他的item 通过session对话 计算出 每一个session的权重 然后 加和 其训练过程 就是 让 我们 训练出来的一致表示下的item 与 next item 靠经 与 其他地 other item 拉远（这个好好看一下）我看过一篇类似的文章 4.1 DNN mean pooling平均池化 transformer通过transformer 来 进行权重的捕获4.2 用于解码的鲁棒距离测量 由于会话被编码为项目嵌入的线性组合，并通过测量嵌入空间中到项目的距离来解码，因此项目嵌入直接涉及嵌入之间的距离计算，导致过度拟合的高风险。因此，我们寻求一种稳健的方法来测量统一表示空间中的距离，以防止过度拟合。 说实话 有点看不太懂证明 论文名字：Improved Deep Metric Learning with Multi-class N-pair Loss Objective 看论文 貌似是一个优化方法 本文使用了三种方法来 避免过拟合 使用了一个可控的超参数 来替换 固定裕度2 dropout 很平常 受对比学习启发 使用了 cos 计算测量距离 24 where h′ denotes the item embeddings with dropout. 5. 实验作者做了两个消融实验来证明自己的效果好 将REC编码器（Representation-Consistent Encoding）替换为sasrec编码器 将RDM（Robust Distance Measuring for Decoding）替换成 传统点积 又做了一个添加模块的实验 真不错啊 ]() 作者 用图来表示空间一致性 超参数设计 6. 总结在本文中，我们提出了一种简单有效的一致表示空间中基于会话的推荐框架CORE，该框架在整个编码和解码过程中统一了表示空间，以克服不一致预测问题。与在项目嵌入上堆叠多个非线性层不同，我们建议只对项目嵌入应用加权和，将一致表示空间中的会话编码为项目。此外，我们从多个方面提出了稳健的距离测量技术，以防止所提出框架中项目嵌入的过度拟合。在五个公共数据集上的大量实验表明了所提出方法的有效性和效率，以及所提出的技术如何帮助现有方法。在未来的工作中，我们将考虑从理论和经验两方面研究所提出的表示一致性编码器的表达能力。此外，我们将探讨如何在拟议的框架中引入边特征和有用的归纳偏差 7. 自己的想法本文的方法简单且有效 对其十分感兴趣的是 一个先有鸡还是先有蛋的问题 即 先想到这个方法 然后跑个实验 给他取了一个漂亮的名字 然后讲了个故事。 还是说 一开始就想到了表示空间一致这个问题 然后 去跑了这个实验 发现有效。 这个真的是个问题。 sine 中 提取 拉取相近兴趣 拉远 不相关 兴趣 是不是可以思考一下 我之前 看到过一篇序列推荐 是 把序列不重要的item替换 然后 做对比学习(其中拉近拉远的操作)就跟这个差不多 我等会去看看 "},{"title":"图推荐系统综述","date":"2022-08-01T09:12:20.467Z","url":"/2022/08/01/%E5%9B%BE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"说些废话图推荐是目前的项目需要 最近一个小伙给我又发了一篇新的 文章 需要好好读一下 2022&#x2F;8&#x2F;1 因为下午不能游泳 我心情很不开心 前言论文名：Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions 链接： 论文链接  代码链接  文章介绍链接：（知乎） 目前没有 相关知识： 1. 相关背景 现有的方法 谱模型 空间模型 动机 高阶连通性 数据结构特征 增强的监督信号 挑战（） 图构造（ 适当地构造成图，节点表示元素，边表示关系 ） 嵌入传播&#x2F;聚合 模型优化（ 包括优化目标、损失函数、数据采样等 ） 计算效率及有效部署（ 由于GNN的嵌入传播操作引入了大量计算，图神经网络在推荐系统中的有效部署是另一个关键挑战 ） 2. 标准系统及我为什么需要图数据图！ 2.1 结构数据数据形式 从在线平台收集的数据有多种形式，包括用户项目交互（评级、点击、购买等）、用户档案（性别、年龄、收入等）、项目属性（品牌、类别、价格等）等 传统的推荐系统无法利用这些多形式的数据，通常只关注一个或几个特定的数据源，由于忽略了许多信息，这导致了次优性能。通过将所有数据表示为图上的节点和边，GNN提供了一种利用可用数据的统一方法。同时，GNN在学习表示方面表现出强大的能力，因此可以获得对用户、项目和其他特征的高质量嵌入，这对推荐性能至关重要 2.2 高阶连通性在传统方法中，由于训练数据主要是仅包含直接连接项的交互记录，因此只能隐式捕获协同过滤效果。换句话说，只考虑一阶连通性。缺少高阶连通性可能会在很大程度上损害推荐性能。相反，基于GNN的模型可以有效地捕捉高阶连通性。 2.3 监控信号（解决数据的稀疏问题）监督信号在收集的数据中通常是稀疏的，而基于GNN的模型可以在表示学习过程中利用半监督信号来缓解这一问题。以电子商务平台为例；与其他行为相比，目标行为purchase相当稀少。因此，仅使用目标行为的推荐系统可能会获得较差的性能。通过在图上编码半监督信号，基于GNN的模型可以有效地结合多种非目标行为，例如搜索和添加到购物车，这可以显著提高推荐性能 3. 挑战3.1 图构建数据输入为观察到的用户项交互数据，输出为缺失用户项交互的预测。因此，可以构造一个以用户&#x2F;项目为节点、交互为边的二部图。此外，CF任务转向图上的用户项链接预测。 节点 图神经网络学习的主要目标之一是为节点分配表示。这导致节点的定义在很大程度上决定了GNN模型的规模，其中大多数参数由 layer-0嵌入占据。注意，边缘嵌入通常不考虑或基于节点嵌入计算。另一方面，确定是否区分不同类型的节点也是一个具有挑战性的问题。例如，在协同过滤任务中，可以对用户节点和项目节点进行不同的建模，也可以将其视为同一类节点。另一个挑战点是处理具体的输入，例如一些数字特征，如项目价格，这些特征总是连续的数字。为了在图中表示这些特征，一种可能的解决方案是将其离散化为分类特征，然后可以将其表示为节点 边边的定义在进一步传播和聚合以及模型优化中高度影响图的质量。在一些琐碎的任务中，推荐系统的数据输入可以被视为一种关系数据，例如用户-项目交互或用户-用户-社会关系。在一些复杂任务中，其他关系也可以表示为边。例如，在bundle推荐中，bundle由几个项组成。连接束和项目的边缘可以反映隶属关系。好的边设计在构造图时应该充分考虑图的密度。过于密集的图意味着存在度数极高的节点。这将使嵌入传播由大量邻居进行。这将进一步使传播的嵌入不可区分和无用。为了处理过于密集的边，对图进行采样、过滤或剪枝是很有希望的解决方案。当然，过于稀疏的图也会导致嵌入传播的效用较差，因为传播将仅在一小部分节点上进行。 3.2 网络设计（传播和聚合的设计）使GNN不同于传统的图学习方法的是传播层。对于传播，如何选择路径是建立推荐系统高阶相似性模型的关键。此外，传播也可以是参数化的，为不同的节点分配不同的权重。 在传播中，也有各种聚合函数的选择，包括均值池、LSTM、max、min等。由于在所有推荐任务或不同数据集中没有一个选项可以表现最好，因此设计一个特定且适当的选项至关重要。此外，传播&#x2F;聚集的不同选择严重影响计算效率。例如，均值池在基于GNN的推荐模型中被广泛使用，因为它可以高效地计算，特别是对于包含高度节点的图，例如非常流行的项目（可以连接大量用户）。此外，可以堆叠传播&#x2F;聚合层，以帮助节点访问更高跳数的邻居。太浅的层使高阶图结构无法很好地建模，太深的层使节点嵌入过度平滑。这两种情况中的任何一种都会导致推荐性能较差。 总结一下 网路设计分为 传播和聚合 可以堆叠传播&#x2F;聚合层 来帮助节点访问更高的邻居层 3.3 模型优化为了优化基于图神经网络的推荐模型，推荐系统中的传统损失函数总是转向图学习损失。例如，优化中的对数损耗可以视为逐点链路预测损耗。类似地，BPR损耗[126]通常用于图上的链路预测任务。另一个方面是数据采样。在基于GNN的推荐中，要对正项目或负项目进行采样，采样方式在很大程度上取决于图结构。例如，在社交推荐中，在图上执行随机游走可以生成弱正项目（例如朋友互动的项目）。此外，有时，基于GNN的推荐可能涉及多个任务，例如不同类型边缘上的链路预测任务。那么在这种情况下，如何平衡每项任务并使它们相互促进是一个挑战。 3.4 计算效率为了保证基于GNN的推荐模型的应用价值，应认真考虑其计算效率。与传统的非GNN推荐方法（如NCF或FM）相比，GNN模型的计算成本要高得多。特别是对于谱GNN模型，如GCN，每个GCN层都涉及复杂的矩阵运算。随着GCN层的多层堆叠，计算成本进一步增加。因此，PinSage等空间GNN模型更容易在大规模工业应用中实现。通过在邻域之间采样或剪枝图结构，只要我们能够承受推荐性能的下降，就可以始终保持效率。 4. 论文总结分阶段的顶会 阶段 模型名 论文名 年 会议 Matching GCMC Graph convolutional matrix completion 18 Matching PinSage Graph convolutional neural networks for web-scale recommender systems 18 Matching NGCF Neural graph collaborative filtering 19 Matching LightGCN Lightgcn: Simplifying and powering graph convolution network for recommendation 20 Ranking Fi-GNN Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction. 19 Ranking PUP Incorporating Price into Recommendation with Graph Convolutional Networks 20 Ranking L0-SIGN Detecting Beneficial Feature Interactions for Recommender Systems 21 Ranking DG-ENN Beyond clicks: Modeling multi-relational item graph for session-based target behavior prediction 21 Re-ranking IRGPR Personalized Re-ranking with Item Relationships for E-commerce 20 5. 总结6. 自己的想法"},{"title":"saint+ saint的时间正确使用版本","date":"2022-08-01T02:11:58.897Z","url":"/2022/08/01/saint+-saint%E7%9A%84%E6%97%B6%E9%97%B4%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8%E7%89%88%E6%9C%AC/","categories":[["undefined",""]],"content":"说些废话前面不是说过 saint的 变种 因为时间属性没有使用正确 导致 其效果甚至都不如原来的本体 这个估计就是时间正确的适用版本 这篇文章 transformer 下 时间属性怎么使用 人家的改进方法 前言论文名： 链接： 论文链接  代码链接  文章介绍链接：（知乎） 已经运行的环境：   相关知识： transformer 知识追踪 saint 1. 相关背景saint没有很好的将时间参数引入 embedding 中 1.1 核心思想其中的时间分为两种 elapsed time 做题的时候所花费的时间 提出了两种编码方式 连续嵌入 （公式） et 是一个时间（int） w 是一个可以学习的参数 范畴嵌入为每个整数秒分配唯一的潜在向量。我们将最大经过时间设置为300秒，超过该时间的任何时间都限制为300秒。 lag time 滞后时间是交互之间的时间间隔，是影响学生学习过程中出现的复杂现象的重要因素。例如，随着时间的推移，学生往往会忘记所学的内容 连续嵌入 （公式） it 是一个时间（int） w 是一个可以学习的参数 范畴嵌入其细粒度为分钟 0, 1, 2, 3, 4, 5, 10, 20, 30, . . . , 1440.从结果上面来看 一共分为了150类 （看看人家的embedding） 2. 实证分析时间分布 一样存在长尾问题（划分的理由）细粒度划分的理由 3. 问题描述各一个序列 题目 加 响应 然后回答序列的最后一个答案 4. 方法（基本为论文方法部分） 论文 的 整体方法 大致 和 saint 差不多 具体可以看看实验是怎么run出来的 时间种类的划分 取得最好的效果 （这里的时间 是放在 解码器之中） 对于时间的消融实验 对于添加时间特征在哪里好的实验 5. 总结此外，通过将时间特征合并到解码器输入中获得了最佳结果，验证了分别处理练习信息和学生反应信息适合于知识跟踪的假设。未来工作的途径包括1）不仅对学生的问题解决记录进行建模，而且对各种学习活动进行建模，例如观看讲座和学习每个练习的解释；2）探索知识跟踪模型的体系结构，而不是分别处理练习信息和学生反应信息的基于转换器的编码器-解码器模型。 6. 自己的想法 对方的消融实验做得十分不错 对于时间的划分 可以通过 观察时间的分布 来确定离散量的分布 对于 lag time 的 使用 是不是 显得有点草率 看看 能不能像lpkt一样用出来 "},{"title":"VKT 视觉知识追踪","date":"2022-07-29T12:14:13.301Z","url":"/2022/07/29/VKT-%E8%A7%86%E8%A7%89%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/","categories":[["undefined",""]],"content":"说些废话这个文章 是 我在看 知识追踪 的 paperwithcode 时候 偶然 发现一篇文章 前言论文名：Visual Knowledge Tracing 链接： 论文链接  代码链接  文章介绍链接：（知乎） 数据集下载： 相关知识： 视觉分类 人类类别表示 度量学习 联合估计 1. 相关背景​ 人类学习者成功的关键是我们能够从我们周围的世界中提取信息丰富和可概括的表示，以及我们在相对稀疏的反馈下更新这些表示的能力。 ​ 其目的是解决三个疑问。 ​ （i）使人类学习的表征如此有效的属性是什么， ​ （ii）这些表征是如何学习的 ​ （iii）我们可以预测人类在学习过程中的分类行为吗？ ​ 最终的目的是了解人类为什么学习如此有效 **我们的目标是估计人类学习者使用的图像分类函数，该函数已提供一系列图像和相应的地面真值类标签作为训练数据 ** 1.1 相关工作 度量学习： 该工作试图从稀疏的人类注释中学习人类对齐的视觉表示 提出了 更具挑战性的视觉知识跟踪设置，其中假设学习者在学习过程中是非平稳的，即他们用于执行手头的分类任务的视觉特征可能会随着时间而变化 提出了一种基于递归神经网络的视觉知识跟踪方法。经过训练后，我们的模型能够预测训练期间未观察到的人类学习者的分类行为。所提出的模型利用以前学习者反应的历史、图像和基本真理类标签来预测他们未来的反应。 度量学习 我们可以在特定的任务通过选择合适的特征并手动构建距离函数。然而这种方法会需要很大的人工投入，也可能对数据的改变非常不鲁棒。度量学习作为一个理想的替代，可以根据不同的任务来自主学习出针对某个特定任务的度量距离函数。 如果注释者正在学习感兴趣的视觉概念，则违反了这一假设。在这项工作中，我们解决了这种非平稳设置，并表明通过这样做，我们可以更准确地预测真实人类学习者的视觉分类行为。 人类类别表征人类会根据任务的不同 使用多种不同的类别学习系统 目前的共识是，根据手头任务的具体性质，人类可能会使用多种不同的类别学习系统[5,3]。例如，在基于规则的任务中，最优策略可能很容易表达，因此可以通过一组规则进行有效编码。然而，在实践中，感知任务（如细粒度视觉分类）可能更难用这种方式表示[8]。 在更具挑战性的环境中，在学习分类任务的过程中，我们的学习者不是静态的。 知识追踪洒洒水 机器教学机器教育人类（机器飞升） 机器教学算法通过生成向新手学习者展示的教学示例序列来解决教学问题，以提高他们完成给定任务的能力。 1.2 核心思想1.3 贡献（i）一个新的视觉知识跟踪模型，该模型联合估计了非平稳人类学习者使用的视觉特征和每时间步分类函数。（ii）从参与学习具有挑战性的视觉分类任务的人那里收集的三个基准评估数据集的一组新注释。（iii）详细比较了这些数据集上的几种视觉知识跟踪方法。 2. 实证分析没有 3. 问题描述具体来说，在每个训练时间步，向学习者k呈现图像x，他们提供其响应r，并以正确的类标签y的形式给出反馈。 相反，为了克服这种有限的信息设置，我们跨多个学习者训练模型φ，允许该模型发现所有学习者共享的知识状态和学习规则。 4. 方法（基本为论文方法部分）跟踪模型φ 特征提取 f CNN 分类函数ψ softmax 特点 特征提取器在实验的时间间隔内保持不变（针对所有学习者使用相同的底层提取器，且随时间间隔内保持不变） 并且他们只是在对不同视觉特征的相对重要性上有所不同 但是 学习者使用的分类函数 不保持静态 分类器 简单静态分类器 我们探索的第一个模型是最简单的。在这里，我们假设所有学习者使用相同的分类器，该分类器不会随时间变化。在此设置中，ψ是一个多类线性分类器，具有权重矩阵w和每类偏差b该模型类似于传统的度量学习方法，它不试图捕捉与个体偏差或时间变化相关的任何注释者特定差异。在训练时，我们只需为所有学习者估计一组参数。该模型不考虑响应历史 时间敏感追踪模型 静态跟踪模型的一个明显局限性是，它没有考虑到学习者可能会随着时间的推移而变化的事实，即他们可能在早期的新分类任务中表现得更差，但随着时间的流逝，随着示例图像序列及其相关的基本真理类标签的显示，他们可能会有所改善。一种更先进的模型捕捉到了这种时间演变，即每个时间步长都有不同的分类器， 在每一个时间步，使用不同的偏差和权重 w b i.e. wt 不等于 wt−1. ### 跟踪模型 以前的问题即单个学习者可能从不同的能力水平开始，并根据他们所获得的信息以不同的方式更新其内部知识状态。[34]表明，递归网络可用于跟踪参与学习数学测验问题的人类学习者的技能习得。 作者使用了 DKT 作为基础 设计了一个新的基于视觉的DKT模型 直接响应模型特征提取 CNN 分类函数 lstm该模型假设学习者在时间t的知识状态由他们之前看到的图像和他们过去的分类响应定义。递归模型可以通过调节学习者的隐藏状态为其生成独特的转换。在这种情况下，在共享特征提取器将图像转换为特征向量后，模型根据学习者的隐藏状态通过一系列非线性变换修改特征向量。最后一个线性层将特征向量转换为预测响应。注意，该模型还以当前查询图像z&#x3D;f（x）和相应的真值类标签y为条件 分类器预测模型与之前的直接预测递归模型不同，我们现在明确表示单个学习者使用的独立的分类函数。 5. 总结在这项工作中，我们探索了视觉知识跟踪的问题，即预测人类学习者使用的内部、可能随时间变化的图像分类功能的任务。为此，我们提出了一系列复杂度从基本静态线性分类器到递归模型的模型，这些模型在预测学习者未来行为时考虑了学习者先前的反应历史。我们从参与视觉学习任务的人那里收集了三个具有挑战性的视觉分类任务的新注释，以便对这些不同模型的性能进行基准测试。 6. 自己的想法 找出开放性问题 知识追踪来说 有个学习的权重分配。相近的权重相对较高，离的较远的权重分配的较低 这个是一个特别新的工作 论文出来估计才两个星期 需要好好阅读 "},{"title":"论文分析的板子","date":"2022-07-29T08:07:34.453Z","url":"/2022/07/29/%E8%AE%BA%E6%96%87%E5%88%86%E6%9E%90%E7%9A%84%E6%9D%BF%E5%AD%90/","categories":[["undefined",""]],"content":"说些废话目前 论文分析要求 很重 而且 我心很杂 需要一套班子 前言论文名： 链接： 论文链接 代码链接 文章介绍链接：（知乎） 相关知识： 1. 相关背景1.1 核心思想2. 实证分析3. 问题描述4. 方法（基本为论文方法部分）5. 实验6. 总结7. 自己的想法"},{"title":"saint论文 扫盲","date":"2022-07-29T07:42:33.970Z","url":"/2022/07/29/saint%E8%AE%BA%E6%96%87-%E6%89%AB%E7%9B%B2/","categories":[["undefined",""]],"content":"前言2022&#x2F;7&#x2F;29 作为 知识追踪的sota saint+ 前身 主要的作用就是体现出 transformer 变种 在 知识追踪的体现 链接： 论文地址： github：  （不太可信）  参考链接： 相关知识： Pre-LN  transformer 1. 相关背景目前的transformer 模型有两个限制 transformer 深度不够 qvk的交互有问题 1.1 核心思想 将 运动序列 和 响应序列 分别应用于编码器和解码器 2. 实证分析（有的有 有的没有）这篇没有分析 3. 问题描述 4. 方法（基本为论文方法部分）参数使用 练习ID：将潜在向量分配给每个练习唯一的ID 练习类别：每个练习属于领域主题的一个类别。为每个类别分配一个潜在向量 位置：输入序列中练习或响应的位置（第一、第二、…）表示为位置嵌入向量。位置嵌入在运动序列和反应序列中共享 响应：将潜在向量分配给学生响应ri的每个可能值（0或1） 已用时间：学生以秒为单位的响应时间被舍入为整数值。将潜在向量分配给0到300（包括0和300）之间的每个整数。任何超过300秒的时间都被限制为300秒 时间戳：记录学生收到每个练习的绝对时间的月、日和小时。为月、日和小时的每个可能组合分配唯一的潜在向量 论文 使用了三种embedding 作为数据的输入 多头这里使用的是上掩码的权重操作，目的是避免看到未来的信息 Pre-LN 把Transformer架构中传统的Add&amp;Norm做layer normalization的方式叫做Post-LN，并针对Post-LN，模型提出了Pre-LN，即把layer normalization加在残差连接之前 编码器M &#x3D; SkipConct(Multihead(LayerNorm(Qin,Kin,Vin)))O &#x3D; SkipConct(FFN(LayerNorm(M))) 这里的o 是 encoder 最后的一个输出 在每一层 ln和残差连接都有使用 解码器M1 &#x3D; SkipConct(Multihead(LayerNorm(Qin,Kin,Vin)))M2 &#x3D; SkipConct(Multihead(LayerNorm(M1,O,O)))L &#x3D; SkipConct(FFN(LayerNorm(M2))) O是编码器的最终输出。译码器中第一层的Qin、Kin和Vin都是Re 我的变体 ltmti区别是 输入解码器序列 变成了 I 不是 e 使用了下掩码 （ 目的是为了让模型关注 most recent (i-1) 来预测。会考虑不同长度的历史信息进行预测，可以当作一种增强 ） UTMTI UTMTI模型遵循与SAINT相同的架构，仅在输入序列的选择上有所不同。 SSAKT 作者报告，当注意力块被多次叠加时，AUC降低。SSAKT通过在将练习作为查询提供之前在练习上应用自我注意力来解决这个问题。运动自注意力块和运动交互注意力块的输出进入相应的以下块，作为其注意力层的输入 5. 总结SOTA 6. 自己的想法 在信息的堆叠 不一定能将效果变得更好（UTMTI和saint） 作者堆变体的样子 像是 能发好文章的姿势 可以 像lpkt那种 试着将时间信息加入其中 "},{"title":"EdNet 数据集","date":"2022-07-29T02:24:52.342Z","url":"/2022/07/29/EdNet-%E6%95%B0%E6%8D%AE%E9%9B%86/","categories":[["undefined",""]],"content":"前言作为知识追踪的先锋，这个数据集 就像 刚进监狱的吴亦凡哥哥 一样 大规模分层 又大又圆 一个配备人工智能教学系统的多平台自学解决方案。EdNet包含2年多来收集的784309名学生的131417236次互动，是迄今为止发布的最大的公共IES数据集 看看 与其他数据集比较 EdNet具有层次结构，将学生行为分为4个不同的抽象层次 第一章 介绍student： 784309 交互： 131417236 图！ 学生使用santa的可能场景。在学生购买了50天的通行证后，他们解决了一个LC问题。当他们解决问题时，他们的所有动作，包括音频播放和选择消除都被记录下来 特性 大规模 EdNet由2017年以来从78 4309名Santa学生收集的总计131441538个互动组成。每个学生在使用Santa时平均产生441.20个互动。基于这些交互作用，EdNet使研究人员能够访问大规模真实世界的IES数据。此外，Santa提供了总计13169个问题和1021个讲座，标记了293种技能，每个问题和讲座分别消耗了95294926次和601805次。据我们所知，就学生总数、互动和互动类型而言，这是可供公众使用的最大的教育数据集 多样性 行为比较多， 数据的丰富性使研究人员能够从不同角度分析学生。例如，购买日志可能有助于分析学生对学习过程的参与程度。 层次结构 为了以一致和有组织的方式提供各种类型的数据，EdNet在四个不同的数据集中提供数据，分别命名为KT1、KT2、KT3和KT4。 多平台 安卓 ios 网络 不同的数据集捆绑是共享一篇文章、图片或听力材料的问题集合。例如，ID为q2319、q2320和q2321的问题可能共享相同的阅读文章。 KT1一个 问题 回答的对 (q1, r1), (q2, r2), · · · , (qt, rt) 可以用于 知识追踪 KT2 解决的问题 问题-回答序列格式的一个主要限制是，它是学生活动的一个非常简明的摘要。例如，在决定一个答案并提交最终答案之前，学生可以在两个答案中的一个选项之间进行选择。这可能表明他们已经将答案缩小到两个选项中的一个，但不确定这两个选项中哪一个是正确的。现代IESs能够记录此类详细信息，但问题响应格式无法有效表示此类情况，限制了使用EdNet-KT1进行的分析 解释 item-id b开头 为捆绑 q开头 为 问题 其他的 自己看 能看明白 EdNet-KT2是EdNet中最简单的基于动作的数据集，由与问题解决活动相关的动作组成 KT3在Santa，学生可以参加除解决问题外的各种学习活动。这包括阅读专家评论或观看系统提供的讲座。EdNet-KT3整合了有关这些学习活动的信息。这些信息可以用来推断学习活动对每个学生的知识状态的影响。例如，可以分析每个学生学习某些专家评论的时间，并观察其对不同学习行为和表现的影响 可以算 这个记录了一个 宏观上的学习过程 e 开头 阅读 解释 I 开头 观看 课程 KT4在微观上 能够 体现出 先选了什么 在修改的一个过程 很强 EdNet-KT4中的示例学生数据。学生购买物品后，他们解决了LC问题q878。记录了他们播放和暂停音频的时间戳。他们还去掉了“a”，选择了“c”作为答案。 我能干什么知识追踪KT1 EdNet-KT1的大规模数据允许该模型通过深度注意力层捕捉学生互动之间的复杂关系。 移动学习环境中的学习会话退出预测 这个是 示例论文 KT4 标签缺少教育问题-预训练任务 使用EdNet-KT4作为训练数据集，评估建模在考试分数和复习正确性预测方面显示了最先进的结果，优于自然语言处理社区开发的学习学习学习项目内容表示的预训练方法 强化学习强化学习（RL）是一种突出的方法[11,6,22,18,17,12]。在RL的背景下，训练策略（例如辅导策略）以最大化奖励函数，该函数评估代理（导师）随着时间的推移的整体教育效果。 KT1 通过历史 来 结果问题的响应 KT4 可以执行更详细的操作，例如讲师匹配，可以用同样的方法模拟产品购买或答案选择消除。每种选择都权衡了简单性和保真度 结论本文介绍了EdNet，一个由多平台服务提供商收集的大规模教育数据集。EdNet包含每个用户活动的高分辨率记录，到目前为止，它比教育领域的任何其他公共数据集都大得多。EdNet的层次结构允许研究人员从不同的抽象层次处理AIEd中的不同任务 "},{"title":"大数据知识了解","date":"2022-07-15T12:34:28.033Z","url":"/2022/07/15/bigdata/","categories":[["undefined",""]],"content":"大数据技术概括 数据分析工具与算法框架 接口适用 只需要了解少部分的sql语言就好 数据生命周期 数据源 预处理（采集分类录入） 储存与索引（储存索引整合） 处理（统计分析 数据挖掘） 决策（可视化 决策） 知识 大数据 技术 批处理（对时间不敏感的） 交互式分析（强调用户参与交互过程） 流式计算（快速反应） 图计算（顶点 边）交互式分析通过多条件联合查询 满足时间，准确性，成本，处理能力要求流式计算满足少量条件下，快速返回结果 快速响应，结果精确，同时服务大量用户计算引擎 spark flink impata presto tidb kylin 数据分析工具 和 算法库 大数据技术发展路径 "},{"title":"留学的问题","date":"2022-07-14T03:56:39.515Z","url":"/2022/07/14/%E7%95%99%E5%AD%A6%E5%B0%8F%E9%97%AE%E9%A2%98/","categories":[["undefined",""]],"content":"如何找到相对应的信息 微博 小红书（消息来源）如何找到相对应的要求 通过学校的官网 会有一个对应学生的list 1.1 list 主要会分为985 211 一本 二本学生有其相对应的要求（学历越高 要求越低） 1.2 list 内会有一个成绩平均分（还有个加权平均分）如果没有明确标注 选择对自己有利的成绩进行提交你是如何找到机构的 如果判断机构的好坏 选择对应的老师 而不是去选择机构 主要是通过设计一些问题 看看机构老师回答的怎么样 看风评一个机构能帮你做什么择校 申请 材料翻译 收offer 个人申请与简历 你学英语是怎么学的 得准备多长时间自学10个月 然后 参加线下班21天 花费2w4 同机构的情况 一样 要找到好的老师 而不是一个机构（小黄鱼 用的很不错）雅思的口语分数很难考 得准备相对较长的时间（口语根据老哥的说法是 只有一个在一个小机构 学习了很长的时间 才拿到的6.5 ）英语中口语 写作较难但相对来说 听力 阅读较简单 老哥最后的成绩为 6 6 5.5 5 最后为了求稳 报了个语言班 技巧 有的学校不仅仅看雅思 也看 多邻国和朗思（线上朗思相较简单） 有一同学准备了7天 拿到了对应的分数 选择看市场供需 有时候 好的选择有奇效 出去学习 只是过简历关 但 整体的硬实力 还是得靠自己 如果 想平平淡淡 没什么高的技术追求 可以去试试 国企和联通这样的企业花费 2w1 中介费用 2w4 英语学习 0.44w 两次的雅思考试费用 总体花销5w5 感谢老哥 "},{"title":"文章的idea到产出的完整环节","date":"2022-07-12T03:38:51.691Z","url":"/2022/07/12/idea/","categories":[["undefined",""]],"content":"idea 相关领域顶会（想法产生）会议是新的 领域大佬（自己领域的大佬） 组会交流（迁移，集成，多模态）– 知识迁移，模仿是最快的发论文方式（集成 boosting starting besting）领域的baseline调通 上一届师兄成果 准备工作 论文5篇 至少一个综述 1.1 挖掘新想法 nlp  1.2 aminer 研究方面：动机 问题 贡献点 最后是方式 2.1 谷歌学术 dblp web of science arxiv 溯源网：aminer 工程方面：代码 数据集 实验设置（超参数 sota） key point 对自己研究领域有初步认识 idea好坏 创新性 直观解释 数学分析 可行性和可验证性 怎么写 论文题目很重要 题目长度 不能太长 缩写和学术用语 题目最后要商讨 文章结构 discussion 可写可不写 最好不写 先word 在latex 到时候换模板 好转换title 摘要 问题 动机 方法 方法展开 实验 关键词 应用背景（应用类型文章） 背景 现实背景 方法背景（主要存在的问题） 类似方法背景（不用与优势） 提出的方法的概述 贡献点 相关工作 研究问题的相关工作 方法 问题定义（定义方式和符号） 总体架构-算法伪代码（图）一般要有 加分！ 算法流程图 实验流程 实验 数据集 评估标准 baseline 实验设置 比较论文的复现结果（如果差）打上鑫号 结果分析 图表是定性分析 投稿流程 杂谈 假如说会议六月份 一月份准备 实验要两三个月 论文一个星期就能做好 提前一个月吧事情都准备好 格式调整 需要一个月 期刊 会议有个摘要截止日期 （会议需要一个月准备）文章都写好这个样子 "},{"title":"基于知识细粒度查询设计方案(使用手册)","date":"2022-07-02T12:17:08.173Z","url":"/2022/07/02/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E7%BB%86%E7%B2%92%E5%BA%A6%E6%9F%A5%E8%AF%A2%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/","categories":[["undefined",""]],"content":"作者：谢昊 班级：计算机应用技术 1. 使用说明本系统涉及的环境有点多，本设计将从爬虫获取，知识点提取，知识点展示来说明。 2. 爬虫获取项目爬虫功能是一个代码开源的软件来完成对应的功能 软件设计清晰 且帮助文档完善 下载网址为： 3. 知识点提取这个是整个项目的难点和重点 知识点提取 是基于mooc视频中的视频提取 举个例子 在 python程序语言设计中RGB色彩体系这个课中，存在下列两个小的知识点。但是视频并没有对其进行区分。（没有划分出知识点的起始结束时间） 对于该功能的讲解 打算分成三个部分来说明 3.1 项目介绍 项目的文件夹三个 为pic text video 分别存取 图片 文字 视频的信息 重要的py文件有三个 video_pic ,pic_word, json_mysql 分别完成 下面介绍的 3.2 3.3 3.4 的功能 3.2 视频关键帧提取视频的关键帧提取使用的是 基于ffmpeg的pyav库来进行存取 输入：视频 输出：视频关键帧图片（名称含有视频关键帧时间） 关键代码： 3.2.1 使用方式项目文件为 video_pic 将我们下载好的视频总集 放入video文件夹内 然后修改好 我们要提取的文件夹名称 run 就生成了对应视频总集文件的png 你需要修改的位置 运行的情况 3.3 图片信息提取图片信息的提取，使用了百度提供的ocr服务，基本手敲了300行代码 这个为项目的难点和核心点 输入：视频关键帧图片 输出：含有课程名称，课件名，知识点（topic），知识点内容（content），起始时间，终止时间的json文件 工作流程： 先将一个课件名内的图片按时间戳顺序整理 ocr遍历课件内的图片 图片ocr处理 得到图片文字内容 ocr会返回含有文字的条数 如果条数小于3 则跳过 文字进行垃圾词清理 判断文字条数是不是为 0 为0跳过 和上一个content进行比较 如果存在相同的字段 则不存 若有不同 则添加字段(topic相同) 当topic 发生改变时候 将存下上一次topic改变的时间，和这一次topic的修改时间记为起始时间和结束时间 记下原topic的名称，和content 存为json存取 当遍历到最后一个图片时 将存下上一次topic改变的时间，和这一次topic的修改时间记为起始时间和结束时间 记下原topic的名称，和content 存为json存取 3.3.1 使用方式项目文件: pic_word 通过百度智能云 获得对应的api_key 和 secret_key  填入自己要获取课程知识点的课程名称 3.4 json数据转存通过读取json数据 生成能够插入数据库的.sql文件 输入：知识点json文件 输出：基于课程的sql文件 3.4.1 使用方式 在使用的时候 需要填入对应的className 和sqlroot 想要生成的sql文件名称 4. 数据库展示本次的数据库 展示使用的是 jeecgboot来完成我们的数据库展示的功能 其具体的开发开发文档： 比较好的视频文档： 展示功能完成了 基于 classname topic main的查询功能 完成了基础的增删改 且能够导入导出exel文档来进行查阅"},{"title":"模型调参","date":"2022-06-26T10:33:20.696Z","url":"/2022/06/26/%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/","categories":[["undefined",""]],"content":"看的是 李沐的视频   前言 开始一个好的基线 改一个值 重新训练模型 重复很多次 去获得直觉 什么超参数很重要 模型对于超参数 的敏感程度（Adam比SGD 调参要简单很多） 超参数的范围在哪里 结果会好 做好笔记 需要认真的管理 （训练日志和超参数）execl 或着 word tensorboardweight&amp;bias 重现很难（硬件，库，代码，随机性） 自动调参 HPO 超参数优化 更加泛化一点 NAS HPO的小一点版本 专注于神经网络 HPO 哈哈哈搜索空间 算法1 黑盒算法遍历！ 找到最好的 也是最傻的算法 算法2 多准确度（加速）目前再用的算法 小数据集 缩小模型规模 层 通道数 快点停止 搜索策略（黑盒子）第一个暴力穷举（贵） 第二个随机取（有效） 写代码很简单 第三个 贝叶斯 （李沐说这个大研究方向 咱不研究 咱不用） 搜索策略 （多准确度）深度学习较多sh算法 节省花销对于多超参数 随机选取n个参数 去训练m个epoch 算法过程 类似一种 递归的思想在其中 选取n&#x2F;2的超参数进行训练 训练m次epoch 选取n&#x2F;4的超参数训练 训练2m次epoch m n 的选取 取决于 你的预算花销 hyperband 用的较多多跑几个sh算法 在每次跑完后 减小n 增大m 每次都是用不同的m 和 n 影响是 对于 m n的选取 没那么大的影响 Neural Architecture Search (NAS)神经结构搜索 这里只关心 神经网络的参数 强化学习 nas代价非常贵 模型优美 one-shot方法学习模型架构 外加超参数 因为 这个东西看起来就很大 所以 关注点在于 只关心架构之间的排名关系 用一个近似的指标 ： 只训练很少的epoch 来验证 one-shot方法 – 可微架构搜索 多种候选层方法 在每一层 在layer层 中 所有的候选层方法 都会有个输出 对于这个输出 添加一个大于0 且 相加为1 的权重系数相乘 最后 学习 得到 在候选层 中权重最大的给保留下来 通过学习 来判断最好的那一条路径 使用DARTS 这个方法 可以在三个GPU天 完成sota效果 简单实用 effientNet调参不多 cnn优化 更多层 更多输出通道 输入图像更大 目前研究方向 可解释性 调整参数 能使用在边缘设备都能跑 整个流程更加的自动化 "},{"title":"研究的艺术","date":"2022-06-26T02:28:16.575Z","url":"/2022/06/26/%E7%A0%94%E7%A9%B6%E7%9A%84%E8%89%BA%E6%9C%AF/","categories":[["undefined",""]],"content":"第一部分 研究读者知道什么 想要知道什么 研究是什么东西 收集信息（提出解决方案） 回答一个疑问 解决一个问题 为什么要写作 会记住 帮助理解 测试你的想法 通过写作来看你的想法是不是对的 为什么要用论文格式对于 对方的方便 以及更好的理解 第二部分 和读者进行连接写作是一个想象中的对话作者自己定义的角色 和 给读者假定一个角色 而且 角色定好 就没有变了 理解作者角色 我找到一些有趣的信息 给你们看看 我找到一个解决方案 在实际问题上 我找到一个重要问题的答案 对应读者角色 娱乐我 （追求新奇的信息） 帮助我解决实际问题（直接跳干货位置） 帮助我更好的理解（大部分的研究者） 怎么去问问题 怎么去找答案 你的研究 是否让人感兴趣 预测读者的回应 我的观点是不是跟所有人的观点是对立的 会不会做标准的一些问句 来反对解决方案 是不是特别关心我是怎么一步一步的解决问题 第三部分 话题到问题针对领域 选择一个针对的话题。话题是一个途径去让你问问题，问题回答得好 别人就会有兴趣 整个领域的人都会有感兴趣。 针对话题问问题 问题的答案 别人会觉得比较重要 答案能在一定程度上改变领域 question or problem question不一定会带来问题 question是一个找到问题的途径 在学界 我们主要是解决problem 而不是question 兴趣去找话题 怎么把效果做出来（提出算法 之前不行 现在很行） 做大 （数据更大 规模更大） 做便宜 做安全点 话题 缩小 到能研究的东西 话题 转换成 论点 针对话题 去问问题(找到解决方法 更能针对的去读文献) so what！当你对一个问题感兴趣的时候 你应该去问一个更难的问题 能不能带来好处 能不能推动领域发展 能不能启发工作 如果不做 整个领域会不会有损失 你最后的工作要别人值得去读，如果一个工作对自己和对别人来说 不知道有什么价值。可以考虑放弃这个工作。 如何问一个更难的问题（so what） 列出话题 话题加一个间接的问题（indirect question） 含有w词 who what when how where 评估问题的重要性 找到的问题 是领域关心的问题 （这时候就不需要在意东西的意义在什么位置）理工科领域 如果能很好解决痛点 一般是有意义 想一想事情的意义 question to problem（疑问到解决问题）通过三步来判断疑问是不是一个值得解决的问题 topic 回答topic 的 question 找到意义 这样 就能很好的找到了一个解决问题的problem 一直去问so what（面向读者的角度） 直到 觉得什么时候 可以停 找到好的研究问题（很难） 找人帮忙 读的时候去找问题 写作中找 第四部分 怎么讲好一个故事（论证）假设研究正常展开，开展到一半或者一大半。我的读者有兴趣去看。如何让我的读者信我的东西或者说使大家相信我的新方法。通过讲一个故事，使读者信我们的东西。 故事 和 信息搜索故事 是 给我们收集信息 给定一个方向（做实验） 而同样的 故事 也能回答读者可以预测的问题，研究还剩下多少东西能做 而同样 在故事发生变更的情况下，可能需要重新去做实验，在不同的超参数或者不同的情况，甚至需要更换数据集。 一边做实验一边讲故事，早一点讲故事给想出来。 最后的来说 跟读者做一个合作探索，达到相同的一个认知水平，方法不一定是最好的。但一定要激发读者的认知，发现一个更好的想法。在方法中可以在做改进 how 足够多的原因和论据 要在读者的角度去问（为什么我要相信这个事情）和 so what 这两个问题 要在做研究的过程中 不断地去问这两个问题 做好一个论证首先要提出一个论点，用原因和证据来支撑论点。有时候你需要承认和回复一些别的观点，最后你要提供一下推理一些逻辑的原则。 写作是一个假想的对话，使得在跟真人对话之前，把所有的可能性，别人攻击你的地方以及缺失的理由，论点，论据全部给补充起来。 论点：我们核心的理由是什么东西，很多时候文章最后就是一个核心的论点，当然论点还有别的子论点支撑； 理由：为什么我们的论点是对的； 论据：一些数据点或者别人的工作； 承认和回复：对于别的一个观点的一个说明； 保证：这个逻辑是怎么样过来的，如果读者不理解的话，应该把它说出来，解释一下我们的理由为什么能解释我们的结论好； 核心要干的事情（支撑论点） 怎么用原因和论据啊来支撑你的论点： 用原因来支撑的话，一般会有一个因为这一个词在这个地方 在一般的情况下，很少只用一个理由来支撑你的论点，很有可能会用多个理由，而特别的是说，其实理由的本身它也是一个论点； 什么时候可以结束说明理由： 理由必须是在论据之上的，结论是基于好的原因，理由又是基于好的论据； 所谓的论据就包括了，做实验得出来的一些实验的结果，或者前面的值得信任那些工作里面的一些论点，这个是现实存在；理由很多时候更多是一个思维的逻辑，是存在你脑海中的； 一旦我们成功的把论点通过理由和论据支撑住了之后，被大家认可之后，我们的论点也会成为别人工作的一个论据。 我们一个论点需要有原因来支持，这些原因又是在基于我们的论据上面的，所以我们要保证说原因能够合理的解释我们的论点，反过来讲每一个原因也需要有他的论据来支撑才是合理存在的； 使用承认和回复 作者提前的预测读者可能会提的一些反对意见，然后把答案写在这里，这样子在读者读的过程中心中产生问题的时候，作者就在下面就把这些问题给回答掉了。 其实回答本身不是最难的问题，最难的是说在写的时候要假设他们这些问题的存在，就是你要想到你的读者可能会提这样子的问题。 核心是说我们得尽量的去考虑到很周全，去想象你的读者会问这样子的问题； 我们的论点可能有一些反对的意见，或者不同的解释、不同的看法，这样子我们需要去承认这些东西的存在，并且给予回复 用理由去支撑论文时，读者可能会看不出它们之间的关系 在读者看不明白的时候，需要补充说明他们之间的联系，不然读者是不会买账的； 这个补充说明一般是一些通用的原则，就是一些大家都能接触的东西，然后把它作为一个通用的原则。在这个原则之下，能够特立出我们的推理逻辑； 当原因和论点之间隔得比较远的时候，需要给出一些推理的保证，来使得读者能清楚的认识到我们的原因和论点是怎么样联系起来的。 实际上我们正在写的时候，整个逻辑可能是比较复杂的，可能就想说一个很简单的东西也可以变得比较复杂。 想把一个文章写的有理有据、滴水不漏是一件很难的事情 要把argument弄得“厚一点”：在支撑论点的时候，正正反反啊都要多讲一点，因为我们的目的是要通过这些比较厚实的论述，让读者能够相信我们所说的内容； 关于声明声明就是对研究问题的答案，就是把答案浓缩成一句话变成声明，然后整个文章呢主要是围绕去支撑这个声明； 声明要考虑那么下面这3个问题： 我在做一个什么样类别的声明：因为不同类别的声明，导致可能要支撑他的这些证据是不一样的； 声明够不够具体：因为对于比较空洞的声明，大家读起来会觉得比较空洞； 声明啊够不够重要：读者觉不觉得有必要去花一篇文章去支撑我这样子的声明； 我们要解决一个有价值的问题，然后我的解决的方法本身应该也要是有价值的 声明需要是具体的而且是重要的 怎么样把声明变得更重要一些 对一个大家感兴趣的话题提供新的证据； 不仅仅把数据展示出来而是要用数据去回答一个大家有争论、不那么确定的问题的答案； 如何让论点更加可信 怎么样把论点变得更加可信一点：​ 如果想让别人信我们说的话，最好不要把话说的特别的满；​ 可以承认一些局限性的条件​ 要去想的限制条件的时候，是从读者角度来出发的。从他们的角度来讲，去想想我们的理由也好我们的论据也好，在哪些地方更加薄弱一点，把这些薄弱的地方作为限制条件给出来之话，那么对整个的可信度就会增加 使用一些降低语气确信度的词，使得论点显得没那么的强硬： 如果讲的特别自信的话，读者会觉得更加的难以置信一些； 如果用了大量这样子的不确定的词汇呢，整个文章可能会显得比较弱，别人会觉得你可能自己也不是那么的确信。 尽量要避免这些词汇：all、no one、every、always、never 第五部分 理由、论证读者怎么看待理由和论证 读者首先会去看我们论述的核心（论点和它的一些支撑的论据）； 对于支撑来说，读者会先去看我们提过的那一些理由，然后去看一下它是不是有道理； 如果理由靠谱的话，读者会去看它的整个逻辑，然后把它排好序，读者会顺着这个思路往下看，看看这个逻辑是不是过得去； 如果这些理由看上去还不错的话，那么接下来他去会去看论据，论据是整个论述的一个基石，论据理应是不容质疑的，但是如果读者不相信你的论据的话，那么他也就不会相信我们的理由。 核心是说，我们有一个论点，然后通过理由，这个是能够架在我们的论据上面，理由是能够撑住你的论点的，如果中间任何一个部分没有做好的话，那么导致我们的论点是支撑不起来，就会导致大家不会信你 如果要去收集整个论述要怎么办 首先应该给读者提供一些合理的理由； 然后这些理由需要在一个清晰的有逻辑的顺序之下； 最后所有这些理由必须要是基于论据的，而且读者是可以接受这些论据的 怎么区分 论据和理由 不是由我们来决定什么是理由什么是论据，而是读者来决定的； 需要把整个论据写的非常的脚踏实地 如何评估证据好坏 一个好的证据必须是准确的、精确的、足够的、有代表性的和权威的； 必须要很准确的来报告证据，作为证据来讲一般来自于两种可能性：从自己收集而来的（比如说做实验采集到了证据）；来自于前面人的一个工作； 所以不管是哪一种都要很准确的报告你的数据怎么来的： 对第一种来讲，要说整个实验是怎么做的，流程是什么样子，然后这数据是怎么样采集的，这需要我们能够准确的描述这个流程，而且大家是认可我们的这个方法的； 如果论据是来自于别人方法的话，那么把证据从别人的报告搬到我们这里来的时候，要足够准确，至少是ctrl C + ctrl V过来的，不要把一些数字搞错了 需要证据准确精确就说不要使用这种很模棱两可的词，而要使用一些比较精确的语言；所谓的模棱两可的词就包括了some most many almost often usually frequently generally这都是一些比较模糊的词，我们要尽量得避免它 需要足够的且有代表性文章举了个例子 即 莎士比亚 一定是 憎恨女性 （因为在麦克白和哈姆雷特里面 将女性描述成虚伪的代表） 出现了两个问题 例子的不全面 这个例子不够有代表性 不能体现作者的意图 证据权威性好的会议引用好的文章 不要使用一些差很多的文章 （最好不用维基百科 这样的内容） 读者不会相信 总结 对于论点怎么样用理由和论据来支撑； 理由通常有多个而且是要合理的，而且需要用合适的顺序把它组织起来； 证据是要需要强有力的而且是读者要认可的，包括你的报告是准确的； 然后这数据的精度是合适的，全面的有代表性的以及是权威的； 第六部分 承认和回应 需要去回应读者心中的那些不同的看法； 写文章的时候要去预测、承认、回应这些读者在读你文章过程中间产生出来的一些问题、反对意见和一些另外的解决方法； 难点是说，我们的写作是一个假想的对话，我们并不知道读者在真的看到我们这么写的时候的反馈，所以在这个时候，我们需要去想象读者的情况和我们要怎么样回应； 在本章节作者会告诉我们，可以从两个方法来想象读者可能会怎么样提供一些不一样的看法，然后也告诉大家怎么样去承认和回复这样子的看法； 读者通常有两种方法去挑战我们说的东西： 内在的完备性，也就是说他们会挑战论点是不是讲的很清楚，然后理由是不是相关的，以及说论据的质量是怎么样的； 外部的完备性：包括了 是不是有一个别的方法来重新来讲我们的问题，或者是说 是不是有一些我们自己没有注意到论据，还有是说 是不是有别人的工作也写过相似的一些话题，但是他们提供了不一样的意见，我们并没有引用他们； 如何找到没有发现的问题（很难） 这是一个很难的事情，比如已经花了很多时间去想象各种理由、各种证据的好坏，如果读者还是有一些不一样的问题的话，那肯定是没有想到的，当时如果已经花了很长时间的话，那再花更多时间可能也不一定能想到；所以要去找别人来看一看，因为别人看的话，反正他也不知道你怎么想的，所以他没有先验的偏见，也许他可以提供一些不一样的看法； 所以在做研究的时候，时不时找人（不管是同学、师兄师姐、师弟师妹或者是导师或者是同事）能够让他们来帮忙看一看现在有的东西也是非常有用的； 如果还是找不到怎么办，作者给大家提供了一些问题可以去问自己，就当自己是读者，从这些方向去看，看看是不是能够找出什么漏洞出来： 这为什么是一个问题（问题应该是指我们所研究的东西）？因为有时候问题可能不是真正存在，只是我们构造出来的或者你想象出来的啊 问题是不是已经很好地定义；（有时候 定义清晰 就很解决问题） 看看自己的解决方案： 看一下自己这些原因和论据 能不能提供一些别的种类的论据； 有很多数字但可能希望一些关于真实案例的情况，或者说只有一些真实的案例但是并没有统计上的一些数字； 所有的论据可以去看它的一个质量（是否准确、精度是不是高、是不是有代表性、是不是权威的）；最难的是说，需要更多的证据； 我们可以用这些问题来客观的去看你现在有的所有的东西，从而发现你的论述中间薄弱的一些地方，在找到论述里面的薄弱点之后，接下来要去决定我们应该把哪一些拿出来写。 如果你承认了太多不一样的观点的话，会导致你的文章很长，别人会觉得说你说了那么多有的没的，那么你的自己的方案在哪里；如果你承认太少，别人会觉得说你可能想地不够深入，你的置信度就没那么高，所以我们需要有一个合适的一个平衡点； 你需要更多的证据 可能会遇到一些问题是无法回答的： 要纠正一个思想是说我们的文章解决了某一个问题的所有，在绝大部分的情况下，我们的研究工作，只是回答了一个小的问题的一部分，所以肯定会漏想了一些别的部分，以及说跟这个问题更大的一些问题的东西，可能是无法回答的； 有时候承认自己无法回答别的问题也很正常了，大家也是这么理解的。对于无知来讲，知道自己无知的无知和不知道自己无知的无知，后者当然更加无知一点，所以宁可自己做前者。 第七部分 推理的保证五点建议 让我们看看准则或者公理是不是合适的： 它是不是有道理的，因为我们对一个这样的担保，通常不会用原因去证明它是合理的，所以你至少让读者觉得他是一个合理的； 这个是不是它覆盖面不用特别广，因为如果一个担保想覆盖更多通用的情况下，就会显得它更加的薄一点，可能反例就更多一点。很多时候我们只要这个担保，能够足够覆盖到论点和原因就行了。 有没有别的一些更好的担保，有时候我们在数学里面做公理的时候，当我们要选一个最好的一个公理而不是选一些公理下面的一些东西； 对于我们的这个领域（大家的研究文章都是发在某个领域上面）是不是合适的，就这个领域的人能不能接受这样子的观点； 我们得覆盖住论点和原因； 什么时候需要用到担保 读者是在领域之外的时候（写教科书的时候经常会用它），大量的新来读者不清楚里面的一些隐藏逻辑，把它给大家讲出来是非常有用的，不然读者可能会看不懂。如果说发一篇文章，必须要是自然杂志的话，它的读者相对来说比较广的时候，也尽量要把这个领域相关的一些东西给大家写出来，让大家明白我们的这一个逻辑； 如果我们使用的原则对这个领域的读者来说，比较新或者是有争议的时候，也应该把它讲一讲； 当我们的论点特别有争议性可能读者觉得很难接受的时候，那么在大家都会接受的情况下在前面说一些准则（如果他接受第一句，然后再过渡到第二句的时候）就显得没那么难接受一些 我们选择去把这样子担保说出来时候，意味着是说我们其实关心读者，生怕他不懂我在说什么，所以把前面这一些逻辑给他们交代的更清楚； "},{"title":"虚拟机和mobasterm连接","date":"2022-06-24T13:53:11.268Z","url":"/2022/06/24/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%92%8Cmobasterm%E8%BF%9E%E6%8E%A5/","categories":[["undefined",""]],"content":"未来可能这个教程会被反复的翻阅 我纠结这个东西纠结了好久 现在写一下 首先感谢：   这里贴一个centos 安装 ssh 教程  0. 前言这个连接 主要看两个东西 第一个看 虚拟机 即看是不是在同一网段内 第二个 看 虚拟机的ssh服务是不是安装着 我在安装ssh时候出现了错误 感谢：  解决方式： ok 这时候 应该就能快乐连接了"},{"title":"python遇到安装不了的包的解决whl包的安装","date":"2022-06-22T05:03:17.638Z","url":"/2022/06/22/python%E9%81%87%E5%88%B0%E5%AE%89%E8%A3%85%E4%B8%8D%E4%BA%86%E7%9A%84%E5%8C%85%E7%9A%84%E8%A7%A3%E5%86%B3/","categories":[["undefined",""]],"content":"感谢： 操作流程  包编译好的文件 2022&#x2F;6&#x2F;22 今天试了试 paddle ocr 包 说实话 安装的的过程有点慢吞吞 于是乎出现了这个样子的错误 错误的大致描述是 c 没有编译好这个包 嗯！ 1. 破局假如 你的python环境是 3.9 你就在这个网站 找人家编好的文件 然后下载完成后按照：文件位置+文件名的格式，直接pip install，例如我的安装的是： 然后 这个包就转好了！ 2. 感悟 有问题 多看看论坛 github不好使的时候，用用gitee pytorch不好使的时候 用用paddle "},{"title":"GRU4Rec 推荐论文","date":"2022-06-12T03:15:52.399Z","url":"/2022/06/12/gru4rec-%E6%8E%A8%E8%8D%90%E8%AE%BA%E6%96%87/","categories":[["undefined",""]],"content":"这是16年出的一篇基于session时序建模的召回模型paper，在当时大部分模型只考虑用户最后点击行为，而忽略用户历史点击行为。虽然用户最后一次点击行为与用户下一次点击的item相关度很高，但是用户历史点击行为一方面丰富了用户画像，另一方面用户的兴趣是多峰的，不一定最后一次点击是最相关的。 模型到现在看的话 实在是简单了一些 重要的是得看他的亮点 1. 亮点说实话 第一个两点我看了半天（可能是上午的吃的太少 抑或是 钱给的太少） 1.1 亮点1 训练时session重组 好笑的是 我怎么忘了这个东西 本文的训练过程是 通过i1,1 来预测 i1.2 所以训练序列要比 会话序列 要短一个 假设一个batch有三条数据，代表不同的session。大家看到session1有4条数据，session2有3条数据，session3有6条，真正作为input的话，每个session中item序列长度只有3，2，5，next item是作为groud truth。记录下batch最短的input 长度为2。首先组装 作为第一个batch的input，output是 ，然后组装第二个batch，input： ，output： ，这时候session2已经处理完了，但是session1，session2还没有处理完，这时候用session4来替换session2，切换session的时候要重新初始化下hidden state。这时候第三个batch的input变成了 ，output是 。到第四个batch的时候session1也处理完了，使用session5来替换。这样下来就能支持多个session并行处理，极大的提升了训练速度。 注意为了保证Session内的连续性和Session间的独立性： 1.2 亮点2 negative sampling作者解释这种负采样方法，首先提出了一个假设，那就是用户不太可能将没见过的东西标记为负样本。如果一个物品很流行，而用户却没有在next-item位置进行点击，那么用户大概率在该时序点不喜欢这个物品。 基于这样的假设和观察，作者认为，应该基于物品的流行程度进行负采样，那么作者就提出了一个巧妙的快速负采样方法，那就是一个batch中的其他序列的next-item作为负样本。如下图: 序列的next-item的gt是1, 5, 8对于1来讲 5，8是负样本。和随机抽样相比，个人觉得优势有两个: 一个是基于pop的采样，可解释性更强，另一个是对于batch内需要计算的item是一致的，那么可以将batch内所有序列的next-item预测，统一成一个矩阵运算，大大降低显存占用 同一个batch 时序 其他的mini-batch 为负样本 表示 用户在这个时间内 不喜欢该样本 2. 作者的发现 多layer不一定好 one-hot的编码更好 每一步输入前面所有的信息和前一步信息性能差不多 提高GRU的宽度有帮助 作者的发现表明 GRU 足够可以编码 item 信息(one-hot更好)，单个layer就可以达到很好的效果，对序列的建模能力比较好。"},{"title":"模型过拟合处理","date":"2022-06-09T08:51:28.478Z","url":"/2022/06/09/%E8%BF%87%E6%8B%9F%E5%90%88%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"0. 前言这篇文章想写好久了，一直拖着。其一开始 在阅读 SASrec的时候 发现为了加入注意力机制 给了三个处理过拟合的方法 为此 想写一个文章 来整理一下 这三个技术的作用 1. dropout感谢： 1.1 出现的原因在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。 过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。 综上所述，训练深度神经网络的时候，总是会遇到两大缺点： （1）容易过拟合 （2）费时 Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。 1.2 啥是dropoutDropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。 Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征 1.3 why？（1）取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。 （2）减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。 （3）Dropout类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝 2. 残差网络(Residual Network)，残差连接(skip-connect)感谢： 2.1 动机深度神经网络的退化问题 以及 梯度弥散&#x2F;爆炸 问题 残差网络很好地解决了深度神经网络的退化问题，并在ImageNet和CIFAR-10等图像任务上取得了非常好的结果，同等层数的前提下残差网络也收敛得更快。这使得前馈神经网络可以采用更深的设计。除此之外，去除个别神经网络层，残差网络的表现不会受到显著影响 3. 归一化 BN 和 LN区别  还是区别  这个应该是最能让人清楚的bn 和 ln 区别的文章  归一化的整体理解  norm pytorch 怎么用 3.1 作用 Norm起作用的本质是它平滑了Loss，保持了梯度下降过程中的稳定。 3.2 layer normalization Latch Normalizaiton在NLP中的直观图中，是对一个btz中的同一句话中每个字进行归一化，即图中红色箭头方向，对该方向这一桶计算均值和方差后，计算归一化；以此对整个btz进行归一化。 3.3 Batch Normalizaiton Batch normalizaiton在NLP中的直观图中，是对一个btz中的每句话同一个位置的字进行归一化，即图中红色箭头方向，对这一桶计算均值和方差后，计算归一化；以此对整个btz进行归一化。 BN缺点* Btz太小会影响。对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；* BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的，换句话说RNN的深度不是固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其他sequence长很多，这样training时，计算很麻烦BN不适用于RNN等动态网络，适用于CNN；LN适用于RNN。"},{"title":"图神经网络入门","date":"2022-06-09T06:42:51.894Z","url":"/2022/06/09/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"感谢： 博客  李沐介绍视频 0. 前言首先，让我们确定什么是图。图代表了一组实体（结点）之间的关系（边）。 V node 节点 E link 连接 U master node 整个图 1. 描述（抽象）为了进一步描述每个节点、边或整个图，我们将图中的每一部分信息进行储存。 在存储方面 我们存下了点，边，整个图的信息 进行存储 图 也分为有向图和无向图 2. 图的实例化讲解作者 使用了两个反直觉的数据类型用图来表示 2.1 图像我们通常认为图像是具有图像通道的矩形网格，将它们表示为数组（例如244x244x3的浮点）。另一种思考图像的方式是具有规则结构的图，其中每个像素代表一个节点，并通过边连接到相邻的像素。每个非边界像素正好有8个邻居，每个节点存储的信息是一个代表像素RGB值的3维矢量。 通过邻接矩阵来可视化图形的连通性的一种方法。我们对节点进行排序，在这种情况下，在一个简单的5x5的笑脸图像中，每个节点都有25个像素，如果两个节点共享一条边，就用一个条目来填充n×n的矩阵。请注意，下面这三种表示方法都是对同一份数据的不同看法。 图像的三种表示方法 2.2 文本我们可以通过为每个字符、词或标记关联索引来数字化文本，并将文本表示为这些索引的序列。这就形成了一个简单的有向图，其中每个字符或索引都是一个节点，并通过一条边与后面的节点相连。 当然，在实践中，这通常不是文本和图像的编码方式：这些图表示是多余的，因为所有图像和所有文本都会有非常规则的结构。例如，图像的邻接矩阵有一个带状结构，因为所有的节点（像素）都是以网格方式连接的。文本的邻接矩阵只是一条对角线，因为每个词只与前一个词和后一个词相连接 3. 什么样子的问题会用到图结构3.1 图级看起来很弱智的 找圈圈个数 3.2 节点级​ 节点级任务关注的是预测图中每个节点的身份或角色。​ 节点级预测问题的一个典型例子是Zach的空手道俱乐部。​ 该数据集是一个单一的社会网络图，由在政治裂痕后宣誓效忠于两个空手道俱乐部之一的个人组成。正如故事所言，Hi先生（教练）和John H（管理员）之间的争执在空手道俱乐部中造成了分裂。节点代表空手道练习者个人，边则代表这些成员在空手道之外的互动关系。预测问题是对一个给定的成员在争斗后是否会效忠于Hi先生或John H进行分类。在这种情况下，一个节点与教官或管理员之间的距离与这个标签高度相关 [] ​ 按照图像的类比，节点级预测问题类似于图像分割，我们试图标记图像中每个像素的作用。对于文本，类似的任务是预测句子中每个词的语音部分（如名词、动词、副词等）。 3.3 边级 通过语义分割 分析出实体对象 然后对实体对象之间的关系 使用机器学习进行预测 之后用图来进行表示 4. 图的表示 自己的看法 dim &#x3D; 0 的位置上 每个节点一一对应 节点有节点 自己的分类 边也是一样 如果想表示一个图两个节点是否相互连接 可以使用【node，node】来表示 global 也有其自己的分类表示 4.1 网络表示 这个GNN在图的每个分量上使用一个单独的多层感知器（MLP）（或你最喜欢的可微分模型）；我们称之为GNN层。对于每个节点向量，我们应用MLP并得到一个学习的节点向量。我们对每条边做同样的工作，学习每条边的嵌入，也对全局背景向量做同样的工作，为整个图学习一个单一的嵌入。 4.2 通过池化信息 来完成图预测（简单gnn模型）可能 我们知道了边的信息 如果通过边的信息来 计算 节点的信息 对节点进行预测池化分两步 收集节点的边信息 通过汇总这些信息来进行预测 所以 如果我们只知道 边的信息 使用池化来路由（或传递）信息到它需要去的地方。该模型看起来像这样。（这个机制貌似叫 消息传递） 使用点信息来 信息传递出 边的总类 使用点信息来算出 图的种类 对点的信息进行汇聚 然后 就像cnn 一样 对这个图的信息进行处理 4.3 通过信息传递 完成 图的更新信息传递分为三步 收集每个节点 所有相邻节点的嵌入 g函数 通过聚合函数 聚合消息 所有汇集的消息 通过一个更新函数（通常学习神经网络） 正如池化可以应用于节点或边一样，消息传递也可以在节点或边之间发生。 这些步骤是利用图的连接性的关键。我们将在GNN层中建立更详细的消息传递变体，以产生表现力和力量不断增强的GNN模型。 "},{"title":"2022年会话推荐综述","date":"2022-06-05T11:45:38.441Z","url":"/2022/06/05/2022%E5%B9%B4%E4%BC%9A%E8%AF%9D%E6%8E%A8%E8%8D%90%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"最近对于会话推荐有了新的兴趣 文章题目： A Survey on Session-based Recommender Systems 0. 前言 提供了一个统一的框架来对SBRSs研究进行分类 SBRS的统一问题陈述，其中SBRS建立在正式概念之上：用户、项目、动作、交互和会话我们全面概述了会话数据的独特特性以及由此带来的SBRSs挑战 会话任务方法进行了系统的分类和比较 全面了解如何应对挑战以及SBRS领域取得了哪些进展简要介绍了SBRSs的每一类方法以及关键技术细节 讨论了SBRS研究中存在的问题和前景 1. 序列推荐和会话推荐的区别 作者对于Boundary解释 是指在事务事件中启动和结束特定会话的开始-结束交互对 会话 可以分为 有序会话 和 无序对话 这个session 内 交互 内item 是不是按顺序分布的来区分是否为 无序有序 对于边界间隔 session 有很多个 而 序列 只有单一一个 对于 其 嵌入的主要关系 基于 会话的 是 共现关系 而 基于 序列的 是 顺序依赖关系 2. 会话推荐2.1 目的会话推荐需要注意attention SBRS旨在通过学习会话内或会话间的依赖关系，预测给定已知部分的会话的未知部分（例如，一个项目或一批项目），或给定历史会话的未来会话（例如，下一个篮子）。 原则上，SBRS不一定依赖会话内的顺序信息，但对于有序会话，可以利用自然存在的顺序依赖性进行建议。 2.2 框架主要工作分为三个子领域 其子领域 可以分为 下一次交互推荐 下一次部分会话推荐（即 下一个会话出现了一部分 预测剩余的部分） 下一次会话推荐 Point-Of-Interest (POI) 生词 其实 在框架体现中 可以看出 下一个项目的推荐是 最多的还是一个交互的推荐 2.3 相关的研究作者认为现有的研究没有发现任何系统地将这一研究领域正规化的研究，或全面分析会话数据的独特特征和SBRS所面临的关键挑战。更不用说提供一个深入的的总结，或详细说明该领域存在的公开研究问题。 对于 相关的研究 习惯将 RS 和 SBRS 混为一谈 且 特别针对 SBRS 的研究特别少。工作主要集中在序列感知RSs上，只讨论了一小部分基于有序会话数据的SBRS工作，而忽略了基于无序会话的SBRS。 2.4 会话推荐的主要符号 一个表征通常被指定为一个潜在的向量 通过这里 可以看出 不同于 序列推荐的 ui 组合 在会话推荐中 更倾向于 UVO 这样的三元组组合 2.5 SBRS问题陈述一个RS可以被看作是一个系统[8, 9]，它由多个基本实体组成，包括用户。物品和它们的行为，例如，用户与物品的互动。这些基本实体和行为构成了会话的核心成分，也就是SBRS的核心实体。因此，我们首先介绍这些实体和行为的定义和属性，然后在此基础上定义SBRS问题。基于它们的定义。这些定义和属性将被进一步用于SBRS的特征和这些定义和属性将进一步用于SBRSs的特征和分类，等等。 2.5.1 用户表示（与序列推荐有些不同）在SBRS中，用户是对物品（如产品）采取行动的主体，如点击、购买。并接受推荐结果。让u表示一个用户，每个用户都有一个唯一的ID和一组描述她的属性，例如，一个用户的性别，它有多个值。如：男性和女性。一个用户的属性可能会影响她对项目的操作，并进一步影响相应的会话。 除了可以明显观察到的显性属性外，还有一些隐性属性，它们反映了用户的内部状态。例如她的情绪和意图，也可能对她的行为产生重大影响。所有的用户共同组成了用户集，即U &#x3D; {u1,u2,. . . ,u |U |}。需要注意的是，一个会话的用户信息可能并不总是可用的，原因有二： (1)由于隐私保护，它不会被记录下来； (2)一些用户在与在线平台互动时不会登录 如amazon.com。因此，会话成为匿名的 2.5.2 item表示 v表示一个项目，该项目与唯一ID和一组属性相关联，以提供项目的描述信息，例如项目的类别和价格。数据集中的所有项目构成项目集，即V&#x3D;{v1，v2，…，V | V |} 没什么好说的 很简单 2.5.3 动作表示 用户通常在会话中对某个项目执行操作，例如单击某个项目。让a表示一个动作，该动作与一个唯一ID和一组属性相关联，以提供其属性信息，例如动作的类型，并具有多个值，例如单击、查看和购买。请注意，某些操作可能与特定项目无关，例如搜索操作或目录导航操作。但如参考文献中所述，它们仍可能为SBRS提供有用的信息 2.5.4 交互 user 交互 available o &#x3D; &lt;u, v, a&gt; no available o &#x3D; &lt;v, a&gt; no available and action only one o &#x3D; 2.5.5 会话会话包含交互!! s &#x3D; {o1,o2, . . . ,o |s |}. 注意一点 单一会话中 可能会出现重复的交互 每一个对话 都与一组属性相关联 例如 持续时间s （ 20分钟或40分钟 ） 属性 定义与影响 session length 会话的长度定义为会话中包含的交互总数。这是会话的一个基本属性 internal order（内部秩序） 一个会话的内部秩序指的是其内部交互的秩序。通常情况下，在不同的会话中存在着不同类型的灵活秩序，即无秩序、灵活秩序和秩序。 action type 在现实世界中，一些会话只包含一种类型的操作，例如购买，而其他会话可能包含多种类型的操作，例如单击、购买。会话中动作类型的数量决定会话内依赖关系是否是同质的（基于单个动作）或异构（基于多种类型的操作），这对于准确的建议很重要 user information 用户信息在连接同一用户在不同时间发生的会话方面起着重要作用，因此其可用性决定了为特定用户跨多个会话建模长期个性化偏好的可能性。用户信息的属性是指会话中用户信息的可用性。实际上，SBRS最初被提议用于处理用户信息不可用的匿名会话 session-data structure 是指与会话相关的层次结构组成的多层次结构。交互层由每个会话中的交互组成，而会话层则由当前用户的多个历史会话组成。（即区分会话和交互） 这个图 可以看出 会话的第五个属性 ： 会话的层次结构 2.6 问题定义2.6.1 输入(1) 当前会话的已知部分（即已发生的交互的列表），这是SBRS的输入。它只为下一个交互（项目）建立会话内的依赖关系，或下一个部分会话的建议（参见第2.2节）。 (2) 已知的历史会话列表，它是主要为下一个会话（如abasket）推荐建立会话间依赖关系模型 (3) 前两者的组合，这是SBRSs的输入为下一次交互的推荐建立会话内和会话间依赖关系的模型。或下一个部分会话建议。 在特定情况下，当前会话或历史会话的输入部分可以是匿名或非匿名、有序或无序的，并具有单一或多种类型的操作。根据我们的观察，大多数现有的SBRS假设输入会话是有序的，并且具有单一类型的操作 2.6.2 输出 SBRS的目标是根据给定的会话上下文，即已知的会话信息，提出建议 （1）在下一次交互建议中，输出是备选交互（项目）的列表，按最佳匹配排序为会话中的下一次交互（项目）；（2） 在下一部分会话建议中，输出是完成当前会话的交互（项目）列表；（3）在下一次会议建议中，输出是组成下一次会议的补充互动（项目）列表 2.7 挑战针对2.5.5 对于会话提出的5个属性 提出了相对应的5个挑战 2.7.1 会话长度根据会话长度，会话大致可分为三种类型：长会话、中会话和短会话，而长会话、中会话和短会话的具体定义可能因特定数据集而异 会话 描述 挑战 长会话 一个长会话包含相对较多的交互，例如超过10次。总的来说，通过更多的互动，长时间的会话可以提供更多的上下文信息，以获得更准确的建议。然而，由于用户行为的不确定性，长会话更有可能包含与其中其他交互无关的随机交互。这会带来嘈杂的信息，从而降低建议的性能 第一个挑战是如何有效地减少不相关交互中的噪声信息。另一个挑战是如何有效地学习复杂的依赖关系以获得更好的推荐性能 中会话 中等会话通常包含中等数量的交互，例如4到9次。根据我们对电子商务行业交易记录生成的会话数据的观察，中间会话是最常见的情况。与长会话和短会话相比，中等会话不太可能包含太多不相关的交互，而它通常包含基于会话的推荐（SBR）所需的上下文信息。 即如何有效地提取相关和准确的上下文信息以获得准确的建议。 短对话 一个简短的会话包含非常有限的交互，例如，通常少于4次，导致可供推荐的信息有限。例如，在由两个交互组成的脱机匿名会话中，可以用来推荐第二个交互（项目）的唯一上下文信息是会话中的第一个交互。一种极端情况是建议会话的第一次交互。 如何在有限的背景信息下有效地提出建议 2.7.2 内部顺序 （会话内是否存在顺序） 内部顺序 描述 挑战 无序对话 无序会话包含的交互之间没有任何时间顺序，也就是说，会话中的交互发生得早或晚没有区别。因此，通常使用的序列模型不适用。与顺序依赖相比，基于共现的依赖通常相对较弱且模糊，更难学习。此外，交互之间大多数基于共现的依赖关系都是集体依赖关系，即会话中的多个上下文交互协同导致下一个交互的发生，而下一个交互更难捕获。 如何有效地学习交互之间相对较弱和模糊的依赖关系，尤其是那些集体依赖关系 有序对话 有序会话包含多个具有严格顺序的交互，它们之间通常存在强的顺序依赖关系 长顺序会话中有效学习级联的长期顺序依赖是一个挑战 灵活安排 灵活排序的会话既不是完全无序的，也不是完全有序的，即会话的某些部分是有序的，而其他部分不是 须仔细考虑和准确了解灵活排序会话中的复杂依赖关系 来自于如何有效地学习复杂和混合的依赖关系，即有序交互之间的顺序依赖关系和无序交互之间的非顺序依赖关系 2.7.3 动作类型 动作类型 描述 挑战 单一动作 单一类型的操作会话仅包括一种类型的操作，例如单击项目，因此只有一种类型的依赖关系来自同一类型的操作，这相对容易学习 很好学习 多种动作 一个多类型的动作会话包括多种类型的动作[54]，从而导致多种类型的交互。在多类型操作会话中存在复杂的依赖关系。具体来说，依赖性不仅存在于同一类型的交互上（例如，点击项目），还存在于不同类型的交互上（例如，点击和购买）。 如何有效准确地了解行动内和行动间类型的依赖关系，以获得准确的建议 2.7.4 用户信息 用户信息 描述 挑战 不匿名 非匿名会话包含与相关用户信息的非匿名交互，从而支持同一用户在不同时间生成的不同会话之间的连接。这使得了解用户的长期偏好以及其在会话中的演变成为可能 准确地了解个性化的长期偏好，而不是多个非匿名会话，这是一个相当具有挑战性的问题 匿名 在匿名会话中，由于缺少连接同一用户生成的多个会话的用户信息，因此几乎不可能为当前会话收集以前的历史会话。因此，只有来自当前会话的上下文信息才能用于建议。 利用有限的上下文信息精确捕获用户的个性化偏好以提供准确的推荐是一个挑战 2.7.5 会话数据结构 数据结构 描述 挑战 单级会话 单级会话数据集通常是一组匿名会话，其中每个会话由多个没有属性信息或历史会话信息的交互组成。在这种情况下，建议只能使用单级依赖项，即会话内的交互依赖项。因此，由于缺乏其他级别的辅助信息，基于单级别会话数据构建的SBRS很容易受到冷启动或数据稀疏问题的影响 即当只有交互依赖关系可用时，如何克服冷启动和稀疏性问题以获得准确的建议 多级会话 多级会话数据涉及至少两个级别的层次结构，即交互级别加属性级别和&#x2F;或会话级别。在这种情况下，每个级别内和不同级别之间的依赖关系都会影响后续的建议。例如，多个项目的类别（属性级别）可能会影响这些项目是否会在一个会话中一起购买（交互级别） 如何全面了解级别内和级别间的依赖关系以获得有效和准确的建议，成为构建在多级会话数据上的SBRS面临的一个关键挑战 3 SBRS方法的分类和比较分类部分 这里写了个大概 还得仔细去看 三大方法为 常规SRS方法 潜在特征方法 深度神经网络方法 3.1 常规方法 3.2 SBRSs的潜在表示方法 SBRSs的潜在表示方法首先使用浅层模型为会话内的每个交互构建低维潜在表示。学习到的信息表示对这些交互之间的依赖关系进行编码，然后将用于后续基于会话的建议 3.3 SBRSs的深度神经网络方法 4. SBRS应用程序、算法和数据集4.1 SBRS应用"},{"title":"快速傅里叶变换","date":"2022-06-03T01:52:58.905Z","url":"/2022/06/03/%E5%BF%AB%E9%80%9F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/","categories":[["undefined",""]],"content":"6&#x2F;3 日 经过两天的阅读 大致掌握了 快速傅里叶变换是个啥 在这里 致谢：   视频对于我理解不到的部分 知乎的文字部分做了充分的解释 0. 前言快速傅里叶是一个怎么样的算法 一个有效 且 漂亮的算法 1. 举个例子 有一个多项式a 和一个多项式b 在a为两阶多项式 b同样为两阶多项式 算出来的c为四阶函数 对于一个d阶的多项式 可以通过 d+1 个点 来确认曲线 线性方程 转换成 矩阵形式 当点互不相同时，对于未知数而言，其系数矩阵为范德蒙矩阵，必可逆，故有唯一解，这唯一确定了多项式系数也即唯一确定了多项式 2. 回到方程本身 对于 方程进行多项式拆分成 偶函数和奇函数 然后划分成这个鬼样子 这个鬼样子的方程 有一个特性 为什么要去x^2 呢 应为 当x取正负的时候 x^2 总是为正 这样就能少算一个值 然后 多获得 一个点 后面有个奇函数项 其值是不存在配对项的 所以 这个循环是有问题的 3. 复数的提出在这里 最创新的思维来了 有一些复数的平方之后 依旧是正负成对出现 通过 将x替换成复数范围内的值 既可以完成我们点与点成对出现的目标 欧拉方程 使用w将多项式中x替换 其时间复杂度为 因为值是对应的 所以 只需要计算一半的值就好了"},{"title":"python mysql输出","date":"2022-05-28T12:08:29.860Z","url":"/2022/05/28/python-mysql%E8%BE%93%E5%87%BA/","categories":[["undefined",""]],"content":"这个文章会有点长 从安装到mysql基础 到 python 操作 mysql OK 现在开始 mysql安装和卸载 8.0.26安装感谢： 文章没什么大问题 注意 安装配置的问题 下载安装包（没错） 安装配置（有一些很注意的东西） 解压 编写MySQL配置文件 在解压目录下新建my.ini文件 将下面文本拷贝进my.ini文件中 注意 下面的 两个地址basedir 和 datadir 这个很重要 配置环境变量 （没错） 卸载感谢： 教程完全无误 很不错 mysql 教程感谢： 这边找到了一个很好的工具网站 可以通过 json 转 mysql  Python之pymysql详解pymysql 是个python 操作数据库的工具 然后 我连了下数据库 发现能用 很不错 感谢 ： "},{"title":"论文的代码复现以及代码设置","date":"2022-05-15T12:46:17.091Z","url":"/2022/05/15/%E4%BB%A3%E7%A0%81%E5%BC%80%E5%A7%8B/","categories":[["undefined",""]],"content":"​ 如同我想说的那样 我希望在我写代码或者复现别人代码的开始的时候 就能使用一下这一篇文章对于我的思路的梳理 有较大的作用 感谢：  2022年5月15日 晚上 我看到了这篇文章 导入包和版本查询 可复现性在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。 显卡设置 如果只需要一张显卡 如果需要指定多张显卡，比如0，1号显卡。 也可以在命令行运行代码时设置显卡： 清除显存（在跑高显存的代码） 也可以使用在命令行重置GPU的指令 "},{"title":"caser 学习总结","date":"2022-05-14T07:38:35.023Z","url":"/2022/05/14/caser-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","categories":[["undefined",""]]},{"title":"专利的撰写","date":"2022-05-07T03:12:09.358Z","url":"/2022/05/07/%E4%B8%93%E5%88%A9%E7%9A%84%E6%92%B0%E5%86%99/","categories":[["undefined",""]],"content":"昨天2022年5月6号的昆工的王老师 来到我实验室对门 我穿着拖孩 去听老师的课 收益良多 感谢 昆工王老师 1. 前言小论文改专利 十分easy （因为 小论文在发的过程中，编辑就帮你审核过了） 专利通过第五个专利号 就能区分 1 发明 2 实用 3 外观 这三种 不要被外观专利给欺骗了 哈哈哈 创新思维 + 技术手段 &#x3D; 授权专利 2. 大方向专利怎么写在问题方向为解决实际问题和解决理论上的问题 2.1 实际方向提出问题+怎么解决问题 2.2 理论上给定一个设定场景 总结现有的问题 现有的方法是如何解决问题的 3. 专利内容3.1 背景技术第一段 大背景 第二段 现有专利方法 100 - 200字 （现有技术） 3.2 发明内容针对解决的问题 技术方案 效果 对于 要保护的步骤 提出 总步骤 对于自己创新点的步骤 写出具体的细分步骤 （具体创新点进行详尽解释） 具体的效果 3.3 权利要求书可以算是具体实施方式的简化版 3.4 具体实施方式对于专利步骤进行完整的解释 越多越好 要有图表 完整"},{"title":"colab使用","date":"2022-05-07T02:49:27.418Z","url":"/2022/05/07/colab%E4%BD%BF%E7%94%A8/","categories":[["undefined",""]],"content":"首先说第一句话 这玩意真难用 不能上传压缩包 然后网上解压（可能我还没找到解决方法） 可能是我水平不太行 感谢我的师兄 李子杰师兄 给予我账号 和 验证码（每次如一日的验证码给予） 在使用的时候 主要出现了三个问题 不会运行.py 文件 不会运行google硬盘的.ipynb文件(我的解决方法相当暴力) 文件上传（文件夹上传） 问题1 文件上传和py文件运行 上传google drive 打开Google drive并登陆 在空白处右键，可在drive中上传文件\\文件夹 （时间比较长） 打开colab 新建笔记本 挂载 Google Drive 进入文件所在目录（当然这里的path 是需要改的啦） 运行目录下的.py文件（执行系统命令，需要在命令前加感叹号） ​ 问题二 不会运行google硬盘的.ipynb文件 优雅的方法来了！！！！怎么理解的呢 当然是 在跑colab跑王老师的代码的时候 自己做琢磨出来的 第一步 打开文件 第二步 打开笔记本 第三步 选你想要 "},{"title":"深度框架 4D数据格式","date":"2022-05-04T02:28:26.245Z","url":"/2022/05/04/%E6%B7%B1%E5%BA%A6%E6%A1%86%E6%9E%B6-%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/","categories":[["undefined",""]],"content":"感谢： 实验室的涛哥  0. 前言对于 Caser竖直卷积 的操作 嗯嗯嗯 在阅读论文的时候 看的不是特别明白 咱就想着 看看代码 于是乎 代码也看得不是怎么明白 最简单最简单 的就是 自己讲想要用的代码给co下来 自己跑一边 于是乎 于是乎 我发现了一个自己的一个很基础的错误 吐槽一下 可能本人功底不行 咱实在没看懂 论文这一段是啥 1. 基本概念深度学习框架，数据为4D 用NCHW或NHWC表示 N - Batch C - Channel H - Height W - Width 在pytorch 为 NCHW 在tensorflow 缺省NHWC GPU支持NCHW 2. 逻辑表达 假定N &#x3D; 2，C &#x3D; 16，H &#x3D; 5，W &#x3D; 4，那么这个4D数据，看起来是这样的： 3. 举例子 本人的竖直卷积 tensor([[-0.5610, -0.0843, 0.8302, 0.0297, 0.3825, 0.6435, 0.2758, 0.3105, -0.1820]], grad_fn&#x3D;) torch.Size([1, 9]) 9 &#x3D; 输出通道数 * 要卷积的次数 3*3 "},{"title":"argparse操作","date":"2022-05-03T12:28:12.894Z","url":"/2022/05/03/argparse%E6%93%8D%E4%BD%9C/","categories":[["undefined",""]],"content":"0. 前言对于 argparse 这个命令行小助手 其 对于深度网络的开发具有相当重要的作用 一开始 对于开发者来说 对于其描述 add_argument 定义 一眼就能看出来 这个东西需要啥 要给啥 其使用 具体三个步骤 实例化 ArgumentParser 使用add_argument函数添加参数 使用parse_args 解析参数 1. 实例化ArgumentParser（挺固定的） 2. 添加参数举一下本人的例子 在add_argument 中间有三个参数 参数一： 看作变量名 前面得加 – 参数二： 变量数据类型 参数三： 缺省值（default） 目前 俺觉得俺能学这三个参数就好了 3. 解析参数最后会被解析成 神奇操作 这里存在 我叫做变量的继承 挺有意思的 被封装成namespace对象 不能在使用add_argument 函数对其 变量进行添加"},{"title":"对于embedding操作 新的理解","date":"2022-05-01T13:15:55.905Z","url":"/2022/05/01/%E5%AF%B9%E4%BA%8Eembedding%E6%93%8D%E4%BD%9C-%E6%96%B0%E7%9A%84%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"2022年5月1日21点20分 先放一张图片 high！ 一下 在这里终于能理解到 为什么pytorch 的好用 和我对embedding 操作 将近一个月的误解！！ 我真的是high到不行了 0. 前言 （很重要）​ 对于一位5月1号 在看着风骚律师 在看着一个名字叫做caser推荐论文的同学。这里面论文里面的一句话，深刻的激发了我。即 俺终于知道如何编码序列推荐数据了！ ​ 其实 在这里面最主要的一个体会是！ ​ 勇敢 即 主动迈出那一步！！ ​ 说太多了 ​ 先看看论文里面的那句话！ ​ 这里的set和universe 用的太美了 brilliant！ ​ 下面这句话非常重要 ​ 用户的交互序列其实就是一个物品序列的组合！！！ 1. 自己以前的误解1.1 误解1本人一直以为 embedding 一定得是one-hot 转换成 embedding-vector 即 我认为简单数字是不能embedding的 大错特错！！！ 数字为什么就不能表示物品的特征呢！ 数字为什么就不能表示物品的特征呢！ 数字为什么就不能表示物品的特征呢！ 数字 简单，那么可爱 就应该被embedding 我觉得我这里的误解 应该是犯了教条主义的错误！！！ 哈哈哈 1.2 误解2即 我认为的交互序列是存在点击和不点击这样子的其他属性的 正好最近阅读了 知识追踪 反而没起到正作用 反而放这个让我的误解加深了！ 论文里面的一句话！ 用户序列 中的元素 即为物品序列的子元素 1.3 误解3我觉得这里 应该是犯了 实践-理论-实践 这个基本道路的错误 为什么 你在没有实践后 就贸然的翻阅理论呢！ 可笑!! 我这一周 基本一直在思考 如何找人家 是如何讲数据集 处理成 序列推荐 模型的数据 然后 一直在抱怨 为什么 我的数据集 相对 cv 的 数据 是多难处理 最后 在读caser 这篇文章的时候 才发现自己是多么的沙雕！ 本来就是一句话的事情 自己想的太复杂了 ​ 心得1 多看论文 说不定有个人能说的很明白 一句话的事儿 心得2 多实践 只有多试试 才能 发现找到新的道路 2. 上代码 观察两个output 其 元素是对应的 其元素是对应的 对应的 好的 我说完了！"},{"title":"PyTorch 张量操作","date":"2022-04-26T12:53:22.333Z","url":"/2022/04/26/pytorch-%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C/","categories":[["undefined",""]],"content":"感谢：  0. 前言​ 在阅读别人编写的源码的过程中，对于他人能够灵活操作torch向量 感觉十分的神奇，于是乎俺也来写一个文档作为一个手册的使用不就好了嘛。（先统计） 1. 基本数据形式读Caser论文有感！其对于数据的操控 我作为一个单纯的研究生感到大受震撼 4d数据结构深度学习框架，数据为4D 用NCHW或NHWC表示 N - Batch C - Channel H - Height W - Width 在pytorch 为 NCHW 在tensorflow 缺省NHWC GPU支持NCHW tensor.size [N,C,H,W] 1.1 有些数据 这样子的[N,H,W]当然 在我推荐领域 channel 也不存在rgb 这么丰富的颜色 直接看代码 给channel 设置成1 就好 1.2 还有些数据 是这个亚子 [w,h]那就这个亚子吧 因为 加batch（一般不为1） 所以 我也没啥好的方法来着 1.3 有些数据 是最后这个样子 [N,H] [N,W]那这个样子呢 是存在补救空间的 嗯嗯！ 2. 我看到的操作张量的数据类型 PyTorch有9种CPU张量类型和9种GPU张量类型 张量基本信息 数据类型转换 torch.div 可以看出来 这玩意是要对其的 squeeze()当其中没有参数的时候，其缺省值是将里面维度为1的进行压缩 感谢： 压缩 原来是压缩 维度为1的操作啊 首先 先说明白 维度一维 线 二维 面 三维 正方体 在程序中 就是 0，1，2 来表示上面的三维 本人自己的操作 unsqueeze()我的很直接的说法就是 这玩意是拿来扩维的 这个函数必须在其中填入dim&#x3D;？这个参数 如果单纯从数字角度来考虑这个事情，那就很简单 例如 torch.cat我觉得我在代码中的注释 很好的解释了这个问题 torch.view感谢：  网上的看不懂啊 网上的看不懂啊 网上的看不懂啊 可能是我水平差了 我用我的大白话来讲明白这个事儿 ​ 日常我们输入卷积层的数据形式 为 4D，view这个小妖精呢，我的感觉就像是捏泥巴。我脑子响起了东北玩神曲 给大家看看有意思的 注意看两次的size 打印 我对于这个玩意的感觉就是将数字打散 然后重新组合起来 十分有趣 torch.mm torch.bmm torch.baddbmm感谢：  "},{"title":"git 沙雕使用","date":"2022-04-26T12:26:08.345Z","url":"/2022/04/26/git-%E6%B2%99%E9%9B%95%E4%BD%BF%E7%94%A8/","categories":[["undefined",""]],"content":"我作为个人开发者，对于git的使用了解个大概就好 首先 先拉一张图 在git的分级中存在 这样的工作目录 这个暂存区 言如其名 我也不晓得是啥 本地仓库 就是文件发送的最后一个位置 远程仓库 对于 我们来说就是github了 1. 本人可能进行的操作 够了！"},{"title":"毕业论文思路","date":"2022-04-23T06:40:11.842Z","url":"/2022/04/23/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E6%80%9D%E8%B7%AF/","categories":[["undefined",""]],"content":"前言做一个能扫描视频内容的搜索框  我能思考到的技术点 ocr扫描 视频内容对其 模糊搜索 关键帧抽取 时间 ：2022&#x2F;4&#x2F;23 功能 生成视频目录 视频信息ocr读取 我是一个来自 7月2号的作者这个项目 其实 做完了 使用了jeccg-boot 完成了搜索框的 功能 github 链接 是  该项目 有较大帮助的有 jeecgboot： 然后 出现的较大问题有 数据库的合法输入问题（导致 我一直在删数据库内容） 作为一个项目来说 这个项目还是不太完备 我这个里面没有用到深度学习的技巧 因为咱还是没找到对应的数据集 我是一个2022&#x2F;8&#x2F;3 的作者想法怪多 做一个视频推荐的功能 视频的内容划分 这个功能要是能做成那种 同步上传 然后 直接增加到数据库那种 （热部署） 推荐的可视化（哇 我真的是越来越开心了） 目前 能感知到的技术要求 模型的线上部署 搜索框的功能完善 模型的训练的问题 我是一个2023&#x2F;4&#x2F;6 号的作者想法没有了 做个双塔吧 一边是知识追踪 一边是推荐系统 完事儿 目前 双塔是弄出来了 在看序列推荐之类的文章 准备 caser的变种"},{"title":"顺序推荐模型综述","date":"2022-04-21T11:32:08.954Z","url":"/2022/04/21/%E9%A1%BA%E5%BA%8F%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"感谢： 名字：Sequential Recommender Systems: Challenges, Progress and Prospects ​ 首先：在阅读这一篇文献的时候，这篇综述不仅仅像我看过的普通的文章的整理，它对于整个问题的分析，感想后，对于这个问题目前的解决的方法，对于行业目前的状况进行了把握，虽然这一篇19年的文献，本人更希望这篇文章是21年的，毕竟能给予更多的指导。 ​ 文章提出了目前序列推荐模型5个困难，对于模型进行了分类，且对于开放方向进行了整理。 ​ 本次我最明显的收益是 即序列中的短序列为会话（） ​ 哦吼 多说无益 干活！ 0. 前言啥子是序列推荐模型（SRS）咧？ 大师或者小学二年级学生会这么回答 input: u,i交互 S output:一个排名靠前的候选列表 R 过程：对复杂顺序关系建模 公式呢？ R &#x3D; argmax f(S) S &#x3D; {i1, i2, …, i|S|} ij &#x3D;&lt; u, a, v &gt; 元素 解释 u 用户 a 行为 v 物品 1. 推荐模型的挑战按作者总结，一共有5个挑战。 1.1 长序列的处理 序列的高阶依赖 长期的顺序依赖关系 挑战这一个工程非常有限，其主要的方法是对应上面的两个小问题进行分模型处理 1.2 以灵活的顺序处理用户项目交互序列​ 并非所有相邻的交互都在序列中顺序相关，eg 购物序列S2&#x3D;{牛奶、黄油、面粉}中，先买牛奶和黄油并不重要，但是面粉的顺序取决于它们的组合。 ​ 看到上面的例子，可以得到，对于灵活顺序的序列处理，做好捕捉集体的顺序依赖关系，而不是逐点依赖关系。如何在柔性顺序的假设下捕获集体序列依赖成为SRS中处理柔性顺序序列的关键挑战。 ​ 目前的工作相对较少，在深度学习领域，CNN在其体现出不同区域的之间的局部和全局依赖关系，来应对这个工作。（之前小看了这篇CNN文章 等下看） ​ 题目：Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding ​ 模型名字： CASER 1.3 处理噪声数据​ 噪声的定义：在序列中，与交互预测产生干扰，在用户项交互序列中，一些历史交互是弱相关甚至不相关。（不晓得兴趣漂移算不算噪声） ​ 一篇自己看不懂的文章，但是完美的解决了这个问题 ​ 简单的说一下这个文章的意思吧，其通过分析一个序列中 非常重要的序列项和一些无关紧要的序列项，通过替换非常重要的 建立负样本。替换无关紧要的建议正样本。通过对比学习来实现这篇。（文章的具体内容还得看） 其难点与新颖点 序列项的分类 对比学习的使用 ​ 题目：CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation ​ 模型名：题目里面有 1.4 处理异构关系的用户交互​ 长期顺序依赖关系与短期顺序依赖关系大不相同，它们不能以相同的方式建模 ​ 混合模型是其解决问题的方式 1.5 具有层次结构的交互序列​ 在这里 我终于理解 会话对于推荐模型的意义 ​ 与用户项交互序列相关联的层次结构主要有两种： 元数据和用户项交互之间的层次结构。 子序列和用户项交互之间的层次结构。 ​ SRSs的另一个关键挑战是如何将嵌入这两种层次结构中的层次依赖性合并到顺序依赖学习中，以生成更准确的顺序推荐。 2 常用模型这里分为常用的和高级的 2.1 基础的 RNN 在这里主要写问题 任何相邻的交互必须相互依赖 只能捕获点相关 CNN 解决了RNN的问题 其卷积核的方式 能以相关的序列附近的关系 没有很强的顺序假设 无法捕获长期依赖 题目： A Simple Convolutional Generative Network for Next Item Recommendation 模型名字：NextitNet GNN 图点线结构 是最近工作的方向，相关工作目前较少 2.2 高级 attention 应用于浅层网络，处理带有噪声的交互序列 memory 内存网络，提高模型的表达能力 混合模型 解决短序列和长序列的组合 2.3 开放方向上下文感知 外界环境对于选择的影响 整合买卖交互（买卖之间的聊天） 跨域 不好理解 给个例子 在现实世界中，用户在特定时间段内购买的物品通常来自多个域，而不是一个域。本质上，来自不同领域的项目之间存在一些顺序依赖关系，例如在购买汽车后购买汽车保险。这种跨域顺序依赖在大多数SRS中被忽略"},{"title":"cin的沙雕理解","date":"2022-04-16T11:53:20.269Z","url":"/2022/04/16/cin%E7%9A%84%E6%B2%99%E9%9B%95%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"0. 前言感谢：     在主流的FM与DNN的结合方法中，这个明显是并行的结构 大家晓得xdeepFM 这个东西嘛 这个东西与deepFM 的区别是 根本没有什么相似性 不信 看看网络结构图 这个是deepFM 这个是大名鼎鼎的xdeepFM ​ 个人的瞎吉儿理解：在结构上，这两个玩意如果实在工程使用上 我肯定会选择deepFM。xdeepFM在网络结构上就存在这一些复杂性，在deepFM的基础上增加了linear层和cin这个不晓得是啥的玩意。在理解中 这个应该是我们xdeepFM 想要替换在deepFM 中 FM的部分 1. 模型的结构难点和自己的思考既然说这个模型我都看了两天 自然有其难点的地方 cin 的操作啊 还有其为什么强的地方 1.1 难以理解的cin讲明白一个东西首先先讲明输入输出 输入：field后的embedding层向量 输出：sum pooling后concat一个向量 1.2 介绍角色说明啊 m 是 filed的个数 D 为 embedd维度 Hk 为第k层特征向量的数量 Xk 为第k层的隐向量 冤大头 首先说明白冤大头的样子 embedd 1 embedd 2 ******** embedd m 这个东西会反复使用（的确是冤大头） 假如说 我有一个这样的隐藏层 我这边感觉第一个隐藏层肯定是随机生成的 其与冤大头 拥有一个相同长度的D边 与隐藏层进行外积操作 于是可以得到一个 长方形的过渡张量 其作用是求出下一个隐藏层 1.3 下一个隐藏层求法本人将用最简单的话来说明白下一个图的意思 欧克 图里面最直接就能看到一个躺着的张量 ，上面能看到，一个平铺的向量 其宽为D 长度为Hk+1 先从最基础的绿色的点开始 每一层都会得到一个绿色的点，张量的高为D 所以得到隐向量的宽为D 隐向量的长度为Hk+1,这个数据的设置 没有任何根据 属于随心所欲 绿色点的计算 会使用到一整个面的橙色的点 1.4 张量的生成我们获得了 隐向量 将其与 冤大头做外积 既可以获得下一层的过渡张量 然后 根据1.3的做法 生成对应的隐藏层 1.5 （1.3，1.4）循环噢噢噢噢 终于有k个隐藏层了呢 1.6 汇总 每个隐藏层为，对于第层，将所有的特征映射进行一个池化操作（sum pooling）【例如对上图Feature map 1向量进行一个累加】： 因此便得到一个池化向量对于第隐藏层。 最后在对于所有的隐藏层的池化向量进行一个拼接： 2. cin的性能 空间复杂度 主要的学习参数就是，经过上述分析，第层的共有个参数。假设包括CIN经过一个二元分类任务，那么CIN总共的学习参数为 时间复杂度 计算的时间复杂度为，那么对于层CIN的总时间复杂度为 优点 （1）交互是向量的交互，不是位（元素）级别（bit-wise）的交互； （2）高阶特征交互是显示的； （3）网络的复杂性不会随着交互程度的增加而呈指数增长； 3. 个人的理解对于xDeepFM模型，我们假设CIN的深度与特征映射的数量都为1，则「xDeepFM就相当于DeepFM的一个泛化」。当进一步删除DNN部分，并且对于特征映射使用一个sum filter，那xDeepFM就将为一个传统的FM模型。【联想之前的分解】 "},{"title":"小看的FM","date":"2022-04-13T07:04:51.808Z","url":"/2022/04/13/%E5%B0%8F%E7%9C%8B%E7%9A%84FM/","categories":[["undefined",""]],"content":"感谢：    -1 大前提这个是无奈之举 发现 能写的太多了 强调一下 one-hot 编码问题 首先展示： 里面有 像click的分类值 有 country,day,ad_type则是对应的特征。对于这种categorical特征，一般都是进行one-hot编码处理。 在代码中是这样处理的 画风一变！！ 因为是categorical特征，所以经过one-hot编码以后，不可避免的样本的数据就变得很稀疏。举个非常简单的例子，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。由此可见，数据的稀疏性，是我们在实际应用场景中面临的一个非常常见的挑战与问题。 one-hot编码带来的另一个问题是特征空间变大。同样以上面淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。 0. 前提啊啊啊~~ 咱这一周2022年4月13日这个时间呐 发现！！ FM 很重要 其理论的推理。现在看起来十分的重要 在工业界，这个模型 简直是yyds 其在CTR预估和推荐领域广泛使用 特征组合对于推荐排序是非常非常重要的，而FM这个思路已经很简洁优雅地体现了这个思想了（主要是二阶特征组合） 牛逼的点： 在embedding前夕 提出了类embedding的处理方式 FM对于每个特征，学习一个大小为k的一维向量，于是，两个特征 和 的特征组合的权重值，通过特征对应的向量 和 的内积 来表示。这本质上是在对特征进行embedding化表征，和目前非常常见的各种实体embedding本质思想是一脉相承的，但是很明显在FM这么做的年代（2010年），还没有现在能看到的各种眼花缭乱的embedding的形式与概念。所以FM作为特征embedding，可以看作当前深度学习里各种embedding方法的老前辈。当然，FM这种模式有它的前辈模型吗？有，等会会谈。其实，和目前的各种深度DNN排序模型比，它仅仅是少了2层或者3层MLP隐层，用来直接对多阶特征非线性组合建模而已，其它方面基本相同。 1. FM 前老弟（看起来就很复杂）这个式子吧 从此公式可以看出组合特征一共有n(n-1)&#x2F;2个，如果特征n上百个，组合特征上万个，就是任意两个wij相互独立，样本数据很稀疏，xixj为非零的项会非常的少，导致训练样本的不足，很容易导致参数 wij 不准确，最终将严重影响模型的性能和稳定性，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。 这个东西 在未来 将为一直成为推荐FM的垫脚石（敲砖引玉） 2. FM 老弟在 工业界 还在使用 大多工业推荐排序系统采取LR这种“线性模型+人工特征组合引入非线性”的模式。因为LR模型具有简单方便易解释容易上规模等诸多好处，所以目前仍然有不少实际系统仍然采取这种模式。但是，LR模型最大的缺陷就是人工特征工程，耗时费力费人力资源，那么能否将特征组合的能力体现在模型层面呢？ 使用了特征的隐向量 作为特征的相对应权重 上面的wij 使用 特征隐向量（辅助向量）的乘积 本人大白话讲一遍，啊啊啊 上面的式子太暴力，脑子思考一下就发现权重项太多，我在上面的这篇文献中发现这么一句：**对于任何正定实矩阵 只要k足够大，都存在k维向量组成的矩阵 使得 ** 在这个公式下 这个V啊 就是辅助向量。 这个k相对于n来说 可太小了 所以 就从目前来说 权重系数 降低到了 3. 时间复杂度的减小多图 警惕！ 一个相对复炸的式子 3.1 step1 3.2 step2 3.3 step3 第三步转换不是太直观，可能需要简单推导一下，很多人可能会卡在这一步，所以这里解释解释。 其实吧，如果把k维特征向量内积求和公式抽到最外边后，公式就转成了上图这个公式了（不考虑最外边k维求和过程的情况下）。它有两层循环，内循环其实就是指定某个特征的第f位（这个f是由最外层那个k指定的）后，和其它任意特征对应向量的第f位值相乘求和；而外循环则是遍历每个的第f位做循环求和。这样就完成了指定某个特征位f后的特征组合计算过程。最外层的k维循环则依此轮循第f位，于是就算完了步骤三的特征组合 3.4 step4对上一页公式图片展示过程用公式方式，再一次改写（参考上图），其实就是两次提取公共因子而已，这下应该明白了吧？要是还不明白，那您的诊断结果是数学公式帕金森晚期，跟我一个毛病，咱俩病友同病相怜，我也没辙了。 "},{"title":"Capsule 网络","date":"2022-04-10T14:19:13.807Z","url":"/2022/04/10/Capsule%20%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C/","categories":[["undefined",""]],"content":"0. 简单总结感谢：  为了改善CNN对旋转不具备不变性，学习不到3D空间信息和CNN只关注要检测的目标是否存在，而不关注这些组件之间的位置和相对的空间关系。 CNN 自己的处理 虽然max pooling在很多任务上提高了原始CNN的准确率，但是我们也可以看到max pooling丢失了很多有价值的信息，并没有很好地理解内容 Capsule尝试去解决这些问题 （优势） Capsule可以学习到物体之间的位置关系，例如它可以学习到眉毛下面是眼睛，鼻子下面是嘴唇，可以减轻前面的目标组件乱序问题 Capsule可以对3D空间的关系进行明确建模，capsule可以学习到上面和下面的图片是同一个类别，只是视图的角度不一样。Capsule可以更好地在神经网络的内部知识表达中建立层次关系。 在训练的时候 能使用相对较少的数据集，但相对来说 其训练时间较CNN变得更长 1. capsule的结构​ capsule 是向量，其可以理解为object的某个类别，其模长表示某个entity存在的概率，其方向表示某个entity属性 其计算的方式 为以下四步： 对输入向量做乘法，其中 和 分别来自与前面的 capsule 的输出，在单个 capsule 内部，对 和 分别乘上 和 得到了 新的 和 。 对输入向量进行标量加权，令与相乘，与相乘，其中和均为标量，且。 对得到向量求和，得到。 向量到向量的非线性化，将得到的结果向量 进行转换，即通过函数 得到结果 ，作为这个capsule 的输出，且这个结果 可以作为下一个 capsule 的输入 2. 细节（训练方式）动态寻路算法 鬼都看不懂下面这个 直观理解 其中两个高层胶囊的输出用紫色向量 表示，橙色向量表示接受自某个低层胶囊的输入，其他黑色向量表示接受其他低层胶囊的输入。左边的紫色输出 和橙色输入 指向相反的方向，所以它们并不相似，这意味着它们点积是负数，更新路由系数的时候将会减少。右边的紫色输出 和橙色输入 指向相同方向，它们是相似的，因此更新参数的时候路由系数 会增加。在所有高层胶囊及其所有输入上重复应用该过程，得到一个路由参数集合，达到来自低层胶囊的输出和高层胶囊输出的最佳匹配。 "},{"title":"transformer 详解！（写的像人话一点）","date":"2022-04-10T12:50:30.634Z","url":"/2022/04/10/transformer%20%E8%AF%A6%E8%A7%A3%EF%BC%81%EF%BC%88%E5%86%99%E7%9A%84%E5%83%8F%E4%BA%BA%E8%AF%9D%E4%B8%80%E7%82%B9%EF%BC%89/","categories":[["undefined",""]],"content":"0. 奠基大佬 Bahdanau Attention &amp; Luong Attention 1. 自注意力和多头 Self Attention &amp; Multi-head Attention 为什么自注意力呢？ 相对于 RNN，考虑长距离依赖，还要可以并行！ constant path length &amp; variable-sized perceptive field：任意两个位置（特指远距离）的关联不再需要通过 Hierarchical perceptive field 的方式，它的 perceptive field 是整个句子，所以任意两个位置建立关联是常数时间内的。 parallelize : 没有了递归的限制，就像 CNN 一样可以在每一层内实现并行。 1.1 宏观角度看自注意力机制​ 随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。 ​ 如果你熟悉RNN（循环神经网络），回忆一下它是如何维持隐藏层的。RNN会将它已经处理过的前面的所有单词&#x2F;向量的表示与它正在处理的当前单词&#x2F;向量结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。 1.2 微观角度在NLP 中 ，k &#x3D; v 具体流程（本人的脑子思考）： 词向量（为x1） 生成qkv（词向量乘对应的权重矩阵 wq wk wv） qk计算注意力分数（scaled-dot product） 注意力分数softmax （alignment function） 通过注意力分数softmax 计算最后的值向量 求和 （context vector） 1.3 代码实践softmax 的 dim &#x3D; 1 2. 注意力简单解释 自己发现 softmax dim 范围其实是dim &#x3D; 1"},{"title":"attention机制 简单理解（废弃） 说实话看不太懂 太抽象","date":"2022-04-10T07:05:28.523Z","url":"/2022/04/10/attention%20%E6%9C%BA%E5%88%B6/","categories":[["undefined",""]],"content":"在反复回卷的attention中，本人感觉心里憔悴 一定得总结一个俺能看懂的文章 最近发现了这样的一篇 感谢：    阿里妹导读：曾被 paper 中各种各样的 Attention 搞得晕晕乎乎，尽管零零散散地整理过一些关于Attention 的笔记，重点和线索依然比较凌乱。今天，阿里巴巴工程师楠易，将 Attention 的知识系统性地梳理、回顾、总结，不求深刻，但求浅显，希望能帮助对 Attention 有疑惑的同学。 0. 什么是attention Attention（注意力）机制如果浅层的理解，跟他的名字非常匹配。他的核心逻辑就是「从关注全部到关注重点」 1. attention 分类涉及所有的 attention 都继承于这个抽象类。这里我写了两个抽象类，一个叫 alignment-based，一个叫 memroy-based。 1.1 alignment-based 模型 c 为 context y1 y2 —–yn 为输入 input 输出为z 1.2 拆分 attention model分为三部曲 score function ：度量环境变量与当前输入向量的相似性；在当前环境下，应该关注哪些信息 ​ alignment function：计算attention weight （权重） 通常使用softmax进行归一化 ​ generate context vector function : 根据 attention weight 得到输出向量 ​ 在整体视角下，就像下图这个样子： ​ 1.3 memory-based 模型 长得很像transformer​ 另一种视角是 QKV模型，假设输入为 q，Memory 中以（k，v）形式存储需要的上下文。感觉在 Q&amp;A 任务中，这种设置比较合理，transformer 是采用的这种建模方式。k 是 question，v 是 answer，q 是新来的 question，看看历史 memory 中 q 和哪个 k 更相似，然后依葫芦画瓢，根据相似 k 对应的 v，合成当前 question 的 answer 1.4 建模方式三步 address memory （score function）： 在memory找相似的东西 ​ normalize（alignment function） ： ​ read content（ gen context vector function ）： ​ 2. attention 细节在attention机制中，其建模方式主要就是以下的三类 按人话说 找相关 度量环境向量与当前输入向量的相似性；找到当前环境下，应该 focus 哪些输入信息（ score-function ） 算权重 计算 attention weight，通常都使用 softmax 进行归一化 （ alignment function ） 出结果 根据 attention weight 得到输出向量 （ generate context vector function ） 2.1 score function 的区别score function 在本质上是度量两个向量的相似度。找出相关的部分 两个向量在一个空间 使用 dot 点乘方式（或者 scaled dot product，scaled 背后的原因是为了减小数值，softmax 的梯度大一些，学得更快一些），简单好使。 不在同一个空间 需要一些变换（在一个空间也可以变换），additive 对输入分别进行线性变换后然后相加，multiplicative 是直接通过矩阵乘法来变换 2.2 alignment function 区别在 soft attention 中，又划分了 global&#x2F;local attention 。 global attention 是所有输入向量作为加权集合，使用 softmax 作为 alignment function，local 是部分输入向量才能进入这个池子。 local的目的 背后逻辑是要减小噪音，进一步缩小重点关注区域。 如何缩小关注区域 local-m 基于的假设生硬简单，就直接 pass了。local-p 有一个预估操作，预计当前时刻应该关注输入序列（总长度为S）的什么位置 pt（引入了两个参数向量，vp，wp），然后在 alignment function 中做了一点儿调整，在 softmax 算出来的attention wieght 的基础上，加了一个以 pt 为中心的高斯分布来调整 alignment 的结果。 在应用中 发现 从global&#x2F;local 视角的分类来看，更常用的依然还是 global attention，因为复杂化的local attention 带来的效果增益感觉并不大 2.3 generate context vector functionsoft&#x2F;hard attention 最直观的一种理解是，hard attention 是一个随机采样，采样集合是输入向量的集合，采样的概率分布是alignment function 产出的 attention weight。因此，hard attention 的输出是某一个特定的输入向量。soft attention 是一个带权求和的过程，求和集合是输入向量的集合，对应权重是 alignment function 产出的 attention weight。hard &#x2F; soft attention 中，soft attention 是更常用的（后文提及的所有 attention 都在这个范畴），因为它可导，可直接嵌入到模型中进行训练，hard attention 文中 suggests a Monte Carlo based sampling approximation of gradient。 "},{"title":"各种慢","date":"2022-04-06T13:34:45.459Z","url":"/2022/04/06/%E5%90%84%E7%A7%8D%E6%85%A2/","categories":[["undefined",""]],"content":"一个常用的各种慢的解决复制帖 pip慢 -i  conda 慢添加清华镜像源，代码如下所示： "},{"title":"criteo数据集的处理 生动","date":"2022-04-06T12:25:47.026Z","url":"/2022/04/06/criteo%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"之前 写过一篇 太烂了 在重新写一篇生动的 数据预处理与特征工程： 目前项目涉及三个数据集（criteo，amazon，movielen） 其中 criteo数据集肯定是使用的最多的啦 这里 对于数据的梳理 （对于数据的处理 我觉得也较通用） 特地 整理一个文档 0. 观察数据（其实很重要） 数据 有0 1 2 这样的离散数据 和 一些68fd1e64 80e26c9b fb936136 7b4723c4 不晓得是啥的数据 感觉能确定的是 第一列的数据 是标签数据 欧克 总结数据如下 有标签 离散 和 不晓得是啥的数据（先确定为连续数据吧） 按照习惯 其target一般特征后面 1. 读取数据 1.1 标准数据查看三件套 1.2 数据质量分析 查看缺失值 异常值分析 1.3 数据分类 2. 缺失值处理针对缺失值 的 离散 和连续缺失的值 进行缺失值处理 （缺失一般是异常值） 一般是四步操作 查看变量类别 查看变量缺失值情况 对于缺失值进行标注 查看标注完缺失值情况 3. sklearn.preprocessing数据预处理这里搬运一下 分箱的目的 离散变量便于特征的增加和减少，便于模型快速迭代 稀疏向量内积乘法更快，计算结果便于存储，容易扩展 离散化后的特征对异常数据有很强的鲁棒性，例如，连续异常值5000可能对模型影响很大，但如果分箱后，模型影响很小 为模型引入非线性，提升模型表达能力，加大拟合 模型更加稳定，不会因为各别数据增加而影响模型精度 简化模型，防止模型过拟合 3.1 处理连续型特征在处理连续型特征 有两个方法 二值化 与 分段 二值化 二值化使用的类是 sklearn.preprocessing.Binarizer 根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射到1。二值化是对文本计数数据的常见操作，分析人员可以决定仅考虑某种现象的存在与否。 分段 分段使用的类 preprocessing.KBinsDiscretizer，相对麻烦点，给出下面的参数列表 3.2 处理分类型特征在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，除了专用来处理文字的算法，其他算法在fit的时候全部要求输入数组或矩阵，也不能够导入文字型数据（其实手写决策树和普斯贝叶斯可以处理文字，但是sklearn中规定必须导入数值型）。 然而在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是[“小学”，“初中”，“高中”，“大学”]，付费方式可能包含[“支付宝”，“现金”，“微信”]等等。在这种情况下，为了让数据适应算法和库，我们必须将数据进行编码，即是说，将文字型数据转换为数值型 文字型数据 转换成数字 处理后的效果 [][qzetZn.md.png] 4. 划分标签和特征 5. 划分训练，验证，测试集 5.1 数据划分"},{"title":"从din到dien 推荐模型（文献部分）","date":"2022-04-04T06:17:53.421Z","url":"/2022/04/04/%E4%BB%8Edin%E5%88%B0dien%20%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%EF%BC%88%E6%96%87%E7%8C%AE%E9%83%A8%E5%88%86%EF%BC%89/","categories":[["undefined",""]],"content":"感谢：     ​ 最近比较迷茫，看了看自己的论文阅读表，这两篇阿里巴巴的文献映入眼帘，作为工业界的老大。其思维比较新颖。且较当时比较的模型有较大百分比的提升。CTR预测， 所谓的CTR，就是点击率Click Through Rate，而CTR预估算法在广告系统中起着至关重要的作用。从这两篇文章中可以看到阿里妈妈在深度学习方向上的探索，也可以窥视到阿里妈妈的CTR算法的发展脉络。 0. 一切从特征选取开始 主要有4个特征组，如下图所示， 1）用户画像特征， 2）用户行为特征，即用户点击过的商品 3）待曝光的广告，广告其实也是商品，后文中我们统称为candidate 4）上下文特征。 1. 模型的鼻祖 （embedding + mlp） 如果说有10个商品的embedding，embedding的维度为16。在业界，pooling的处理方式就两个。 sum pooling 对应的维度数字求和 mean pooling 对应的维度数字求平均 1.1 问题​ 在电商这个场景中，通常用户的兴趣具有多样性，可能在一段时间内点击过衣服，电子产品，鞋子等。而对于不同的candidate来说，浏览过的相关商品对于预测帮助更大，不相关的商品对于ctr预估可能并不起作用，例如用户看过的衣服，鞋子对于iphone的预测并没有帮助 2. DIN​ 为了解决上述的问题，既然提到了相关，那肯定得考虑到&#x3D;&#x3D;注意力机制&#x3D;&#x3D;了啦 ​ 解决思路是： 在pooling的时候，与candidate相关的商品权重大一些，与candidate不相关的商品权重小一些，这是一种Attention的思想。将candidate与点击序列中的每个商品发生交互来计算attention分数。具体计算方法如图3中右上角的小网络所示，输入包括商品和candidate的embedding向量，以及两者的外积。对于不同的candidate，得到的用户表示向量也不同，具有更大的灵活性。 ​ 模型基础上 提出了Activation Unit单元 来提取商品与目标广告之间的相关性。 其中activation unit的输入包括两个部分，一个是原始的用户行为embedding向量、广告embedding向量；另外一个是两者Embedding向量经过外积计算后得到的向量，文章指出这种方式有利于relevance modeling。 ​ 2.1 attention归一化处理​ 一般来说，做attention的时候，需要对所有的分数通过softmax做归一化，这样做有两个好处，一是保证权重非负，二是保证权重之和为1。但是在DIN的论文中强调，不对点击序列的attention分数做归一化，直接将分数与对应商品的embedding向量做加权和，目的在于保留用户的兴趣强度。例如，用户的点击序列中90%是衣服，10%是电子产品，有一件T恤和一部手机需要预测CTR，那么T恤会激活大部分的用户行为，使得根据T恤计算出来的用户行为向量在数值上更大，相对手机而言。 2.2 DIN的创新点​ DIN的论文中还提出了两个小的改进点。一个是对L2正则化的改进，在进行SGD优化的时候，每个mini-batch都只会输入部分训练数据，反向传播只针对部分非零特征参数进行训练，添加上L2之后，需要对整个网络的参数包括所有特征的embedding向量进行训练，这个计算量非常大且不可接受。论文中提出，在每个mini-batch中只对该batch的特征embedding参数进行L2正则化。第二个是提出了一个激活函数Dice。对于Relu或者PRelu来说，rectified point(梯度发生变化的点)都在0值，Dice对每个特征以mini-batch为单位计算均值和方差，然后将rectified point调整到均值位置。 3. DIENDIN在捕捉连续行为之间的依赖关系很弱，行为-》利益，其中隐性利益很难通过行为充分反映 DIEN，全称是Deep Interest Evolution Network，即用户兴趣进化网络。这个算法中用两层架构来抽取和使用用户兴趣特征： 兴趣抽取层Interest Extractor Layer: 从用户行为序列中提取信息 兴趣进化层Interest Evolving Layer: 从用户行为序列中找到目标相关的兴趣，对其进行建模 3.1 兴趣抽取层（这里使用的是GRU）​ 在广告与商品之间并不是单纯的商品对应的关系（即我喜欢的是这个） ​ 兴趣抽取层Interest Extractor Layer的主要目标是从embedding数据中提取出interest。但一个用户在某一时间的interest不仅与当前的behavior有关，也与之前的behavior相关，所以作者们使用GRU单元来提取interest。GRU单元的表达式如下。 其中，σ是sigmoid操作，而◦是内积操作。 作者的辅助测试 ​ 作者设计了一个二分类模型来计算兴趣抽取的准确性，我们将用户下一时刻真实的行为e(t+1)作为正例，负采样得到的行为作为负例e(t+1)’，分别与抽取出的兴趣h(t)结合输入到设计的辅助网络中，得到预测结果，并通过logloss计算一个辅助的损失 3.2 兴趣进化层 （带有注意力的GRU） interest在变化过程中遵循如下规律：1）interest drift：用户在某一段时间的interest会有一定的集中性。比如用户可能在一段时间内不断买书，在另一段时间内不断买衣服。2）interest individual：一种interest有自己的发展趋势，不同种类的interest之间很少相互影响，例如买书和买衣服的interest基本互不相关。 3.2.1 attention机制兴趣抽取层获得的隐向量，ht 与 被embedding后的广告向量 产生内积后softmax取相关参数 3.2.2 attention方式 GRU with attentional input (AIGRU) 这种方式将attention直接作用于输入，无需修改GRU的结构： ​ Attention based GRU(AGRU) 这种方式需要修改GRU的结构，此时hidden state的输出变为： ​ GRU with attentional update gate (AUGRU) 这种方式需要修改GRU的结构，此时hidden state的输出变为: ​ "},{"title":"RNN模型","date":"2022-04-03T13:18:49.397Z","url":"/2022/04/03/RNN%E7%B1%BB%E6%A8%A1%E5%9E%8B/","categories":[["undefined",""]],"content":"感谢： 文章 写的怪好的 但是 在博客的体现并不怎么好看 如果想看 去看md文档 0. RNN先简单介绍一下一般的RNN。 这里： x为当前状态下数据的输入， h表示接收到的上一个节点的输入。 y为当前节点状态下的输出，而**h’**为传递到下一个节点的输出。 通过上图的公式可以看到，输出 h’ 与 x 和 h 的值都相关。 而 y 则常常使用 h’ 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。 对这里的y如何通过 h’ 计算得到往往看具体模型的使用方式。 通过序列形式的输入，我们能够得到如下形式的RNN。 1. LSTM 长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。 相比RNN只有一个传递状态 ，LSTM有两个传输状态，一个 （cell state），和一个 （hidden state）。（Tips：RNN中的 对于LSTM中的 ） 其中对于传递下去的 改变得很慢，通常输出的 是上一个状态传过来的 加上一些数值。 而 则在不同节点下往往会有很大的区别。 1.1 LSTM 结构深入下面具体对LSTM的内部结构来进行剖析。 首先使用LSTM的当前输入 和上一个状态传递下来的 拼接训练得到四个状态。 其中， ， ， 是由拼接向量乘以权重矩阵之后，再通过一个 激活函数转换成0到1之间的数值，来作为一种门控状态。而 则是将结果通过一个 激活函数将转换成-1到1之间的值（这里使用 是因为这里是将其做为输入数据，而不是门控信号） 下面开始进一步介绍这四个状态在LSTM内部的使用。（敲黑板） 其经典图片是这样的 1.2 主要的阶段LSTM内部主要有三个阶段： 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。 具体来说是通过计算得到的 （f表示forget）来作为忘记门控，来控制上一个状态的 哪些需要留哪些需要忘。 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 表示。而选择的门控信号则是由 （i代表information）来进行控制。 将上面两步得到的结果相加，即可得到传输给下一个状态的 。也就是上图中的第一个公式。 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 来进行控制的。并且还对上一阶段得到的 进行了放缩（通过一个tanh激活函数进行变化）。 与普通RNN类似，输出 往往最终也是通过 变化得到。 1.3 总结以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。 但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。 2. GRU GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的 。 其性能与LSTM相当 ，但是 其计算的消耗较前者少。 2.1 输入输出GRU的输入输出结构与普通的RNN是一样的。 有一个当前的输入 ，和上一个节点传递下来的隐状态（hidden state） ，这个隐状态包含了之前节点的相关信息。 结合 和 ，GRU会得到当前隐藏节点的输出 和传递给下一个节点的隐状态 。 2.2 内部结构 通过上一个传输下来的状态 和当前节点的输入 来获取两个门控状态。 其中 控制重置的门控（reset gate）， 为控制更新的门控（update gate）。 得到门控信号之后，首先使用重置门控来得到“重置”之后的数据 ，再将 与输入 进行拼接，再通过一个tanh激活函数来将数据放缩到**-1~1**的范围内。即得到如下图2-3所示的 。 这里的 主要是包含了当前输入的 数据。有针对性地对 添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“。 最后介绍GRU最关键的一个步骤，我们可以称之为”更新记忆“阶段。 在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控 （update gate）。 更新表达式： 首先再次强调一下，门控信号（这里的 ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。 有读者发现在pytorch里面的GRU[链接]写法相比原版对 多了一个映射，相当于一个GRU变体，猜测是多加多这个映射能让整体实验效果提升较大。如果有了解的同学欢迎评论指出。 GRU很聪明的一点就在于，我们使用了同一个门控 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。 ：表示对原本隐藏状态的选择性“遗忘”。这里的 可以想象成遗忘门（forget gate），忘记 维度中一些不重要的信息。 ： 表示对包含当前节点信息的 进行选择性”记忆“。与上面类似，这里的 同理会忘记 维度中的一些不重要的信息。或者，这里我们更应当看做是对 维度中的某些信息进行选择。 ：结合上述，这一步的操作就是忘记传递下来的 中的某些维度信息，并加入当前节点输入的某些维度信息。 可以看到，这里的遗忘 和选择 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （ ），我们就会使用包含当前输入的 中所对应的权重进行弥补 。以保持一种”恒定“状态。 2.3 总结GRU输入输出的结构与普通的RNN相似，其中的内部思想与LSTM相似。 与LSTM相比，GRU内部少了一个”门控“，参数比LSTM少，但是却也能够达到与LSTM相当的功能。考虑到硬件的计算能力和时间成本，因而很多时候我们也就会选择更加”实用“的GRU啦。"},{"title":"Recommender-System-Pytorch 项目 网络参数指南","date":"2022-03-28T12:43:29.890Z","url":"/2022/03/28/Recommender-System-Pytorch%20%E9%A1%B9%E7%9B%AE%20%E7%BD%91%E7%BB%9C%E5%87%BD%E6%95%B0/","categories":[["undefined",""]],"content":"个人觉得成熟的rebole的工具 不太适合萌新 来操作自己对于项目的建设 最近找了一个新的项目 来操作 感觉本项目 更贴近萌新到大佬写代码过程 于是乎 有了这篇指南 embedding操作0. 官方操作下面是官方例子 官方的解释： torch.nn.``Embedding(num_embeddings, embedding_dim, padding_idx&#x3D;None, max_norm&#x3D;None, norm_type&#x3D;2.0, scale_grad_by_freq&#x3D;False, sparse&#x3D;False, _weight&#x3D;None, device&#x3D;None, dtype&#x3D;None) num_embeddings：嵌入字典的大小（词的个数）； embedding_dim：每个嵌入向量的大小； padding_idx：若给定，则每遇到 padding_idx 时，位于 padding_idx 的嵌入向量（即 -padding_idx 映射所对应的向量）为0； max_norm：若给定，则每个大于 max_norm 的数都会被规范化为 max_norm； norm_type：为 max_norm 计算 p-范数的 p值； scale_grad_by_freq：若给定，则将按照 mini-batch 中 words 频率的倒数 scale gradients； sparse：若为 True，则 weight 矩阵将是稀疏张量。 1. 自己的瞎吉儿理解这里呀 就只需要理解好 前三个就好 对于前两个的理解 torch.nn.Embedding 的权重为 num_embeddings * embedding_dim 的矩阵，例如输入10个词，每个词用3为向量表示，则权重为10*3的矩阵； 对于 padding_idx 理解 可以看出 “6” 所对应映射的向量被填充了0。 网络初始化 1. 初始化函数 均匀分布torch.nn.init.uniform_(tensor, a&#x3D;0, b&#x3D;1)服从~U ( a , b ) U(a, b)U(a,b) 正太分布torch.nn.init.normal_(tensor, mean&#x3D;0, std&#x3D;1)服从~N ( m e a n , s t d ) N(mean, std)N(mean,std) 初始化为常数torch.nn.init.constant_(tensor, val)初始化整个矩阵为常数val Xavier 基本思想是通过网络层时，输入和输出的方差相同，包括前向传播和后向传播。具体看以下博文： 为什么需要Xavier 初始化？如果初始化值很小，那么随着层数的传递，方差就会趋于0，此时输入值 也变得越来越小，在sigmoid上就是在0附近，接近于线性，失去了非线性如果初始值很大，那么随着层数的传递，方差会迅速增加，此时输入值变得很大，而sigmoid在大输入值写倒数趋近于0，反向传播时会遇到梯度消失的问题 xavier初始化的简单推导 kaiming (He initialization) 以后再说 现在没用上 bug解决感谢涛哥 对于源码的修改 错误信息 在改错的时候注意看最后一行 即 修改在这里定位好 然后修改成： "},{"title":"推荐数据集处理（分桶） 重要（比较水）","date":"2022-03-28T03:06:43.179Z","url":"/2022/03/28/%E6%8E%A8%E8%8D%90%E6%95%B0%E6%8D%AE%E9%9B%86%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"1. 探索数据集1.1 数据读入 1.2 查看数据 1.3 数据分类 1.4 查看缺失值情况（标注） 1.5 数据分箱 1.6 训练数据与标签分离 2. 字段维度（field_dim） 与 数据划分 2.1 字段维度获取（field_dim） 2.2 数据划分 "},{"title":"python中axis=0和axis=1的理解","date":"2022-03-25T01:45:41.345Z","url":"/2022/03/25/axis%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"原文链接： axis的重点在于方向，而不是行和列。1表示横轴，方向从左到右；0表示纵轴，方向从上到下。 即axis&#x3D;1为横向，axis&#x3D;0为纵向，而不是行和列，具体到各种用法而言也是如此。当axis&#x3D;1时，如果是求平均，那么是从左到右横向求平均；如果是拼接，那么也是左右横向拼接；如果是drop，那么也是横向发生变化，体现为列的减少 axis &#x3D; 0 纵向处理 axis &#x3D; 1 横向处理 "},{"title":"信用卡交易数据解读与探索（数据合并）","date":"2022-03-21T07:01:44.023Z","url":"/2022/03/21/%E4%BF%A1%E7%94%A8%E5%8D%A1%E4%BA%A4%E6%98%93%E6%95%B0%E6%8D%AE%E8%A7%A3%E8%AF%BB/","categories":[["undefined",""]],"content":"数据分析首先还是对数据集进行解释，以及简单验证数据集的正确性。信用卡交易记录包括了两个数据集，分别是historical_transactions和new_merchant_transactions。两个数据集字段类似，只是记录了不同时间区间的信用卡消费情况： 这里的数据存在两个 一个18以前的数据集 一个18以后的 数据解读 首先简单查看有哪些字段一致： 并且我们进一步发现，交易记录中的merhcant_id信息并不唯一： 造成该现象的原因可能是商铺在逐渐经营过程动态变化，而基于此，在后续的建模过程中，我们将优先使用交易记录中表中的相应记录。 数据预处理 连续&#x2F;离散字段标注 首先也是一样，需要对其连续&#x2F;离散变量进行标注。当然该数据集中比较特殊的一点，是存在一个时间列，我们将其单独归为一类： 字段类型转换&#x2F;缺失值填补 "},{"title":"商户数据解读与探索(包含较为复杂的数据处理)","date":"2022-03-21T01:54:47.085Z","url":"/2022/03/21/%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E6%B8%85%E6%B4%97/","categories":[["undefined",""]],"content":"复杂的数据处理过程（含清洗）1. 数据解读 2. 数据探索 正确性检验 查看id出现次数是否唯一 缺失值分析 能够发现，第二个匿名分类变量存在较多缺失值，而avg_sales_lag3&#x2F;6&#x2F;12缺失值数量一致，则很有可能是存在13个商户同时确实了这三方面信息。其他数据没有缺失，数据整体来看较为完整。 3. 数据预处理3.1 离散&#x2F;连续字段标注由于商户数据集中特征同时存在分类变量和离散变量，因此我们首先可以根据字段的说明对不同属性特征进行统一的划分： 3.2 离散数据处理 离散变量数据情况 离散变量字典编码 接下来对离散变量进行字典编码，即将object对象类型按照sort顺序进行数值化（整数）编码。例如原始category_1取值为Y&#x2F;N，通过sort排序后N在Y之前，因此在重新编码时N取值会重编码为0、Y取值会重编码为1。以此类推。 需要注意的是，从严格角度来说，变量类型应该是有三类，分别是连续性变量、名义型变量以及有序变量。连续变量较好理解，所谓名义变量，指的是没有数值大小意义的分类变量，例如用1表示女、0表示男，0、1只是作为性别的指代，而没有1&gt;0的含义。而所有有序变量，其也是离散型变量，但却有数值大小含义，如上述most_recent_purchases_range字段，销售等级中A&gt;B&gt;C&gt;D&gt;E，该离散变量的5个取值水平是有严格大小意义的，该变量就被称为有序变量。 在实际建模过程中，如果不需要提取有序变量的数值大小信息的话，可以考虑将其和名义变量一样进行独热编码。但本阶段初级预处理时暂时不考虑这些问题，先统一将object类型转化为数值型。&#x3D;&#x3D;（object类型转换类型）&#x3D;&#x3D; 测试 3.3 连续变量数据探索 据此我们发现连续型变量中存在部分缺失值，并且部分连续变量还存在无穷值inf，需要对其进行简单处理。 无穷值处理 缺失值处理 不同于无穷值的处理，缺失值处理方法有很多。但该数据集缺失数据较少，33万条数据中只有13条连续特征缺失值，此处我们先简单采用均值进行填补处理，后续若有需要再进行优化处理。 "},{"title":"kaggle 及 conda jupyter 内核准备入门","date":"2022-03-20T11:02:48.709Z","url":"/2022/03/20/kaggle%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"环境安装和准备感谢：  anaconda + jupyter 获取kaggle.json &amp;emsp;&amp;emsp;在安装完成kaggle之后，进入Kaggle的个人主页（点击右上角头像），点击Create New API Token，则可创建一个kaggle.json文件，并自动开始下载 ​ - 将kaggle.json文件移动到.kaggle文件夹内 安装内核使用anaconda虚拟环境作为jupyter notebook内核 主要是将python 用作 能在jupyter运行的内核 删除内核"},{"title":"深度推荐系统 下","date":"2022-03-19T07:31:56.636Z","url":"/2022/03/19/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(2)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（2）2.4 FM与深度学习模型2.4.1 FNN - 用FM的隐向量完成embedding层初始化 FNN相较于deep crossing模型的区别 对于embedding进行了改进，在模型初始化时引入了有价值的先验信息 在训练时特征被划分了不同的特征域，每个特征域有对应的embedding层 2.4.2 deepFM（啊哈哈 大怨种来啦）究竟是怎样的大佬 看得懂我下面这句话 反正我是看不懂的 强调 红色为 权重1连接 就是是啥就是啥 用FM代替wide部分 FM部分 这个FM layer 在图上面 直观的看啊 有 一个+ 和 好多个 * 这个+ 是 FM层 的线性部分 这个*呢 就是 FM层的 特征组合部分（这里应该是 vi 点积 vj） 其输出公式为 deep 部分 这个deep部分啊 就是多层感知机嘛 没啥难的 ​ ​ FM与深度模型的组合有两种，一种是二者并行，另一种是二者串行。DeepFM就是并行的一种结构。并行就是FM将输入部分计算完之后单独拿出来，得到一组特征表示，然后再利用深度模型（多层全连接）对输入部分进行高阶的特征组合。最后把二者的特征进行concact，得到一组特征，最后对这组特征进行分类或者回归。其实这只是特征的一种组合方式，目的就是为了得到特征的高阶表示。 输出公式 细节（权重共用） 下面解释好好的看 这里的第二点如何理解呢，假设我们的k&#x3D;5，首先，对于输入的一条记录，同一个field 只有一个位置是1，那么在由输入得到dense vector的过程中，输入层只有一个神经元起作用，得到的dense vector其实就是输入层到embedding层该神经元相连的五条线的权重，即vi1，vi2，vi3，vi4，vi5。这五个值组合起来就是我们在FM中所提到的Vi。在FM部分和DNN部分，这一块是共享权重的，对同一个特征来说，得到的Vi是相同的。 2.4.3 总结特征工程在这条路上已经穷尽了可能性的尝试，模型的提升空间会非常小。但是很重要 2.5 注意力机制的应用 Attention机制的本质 attention机制的本质是从人类视觉注意力机制中获得灵感(可以说很‘以人为本’了)。大致是我们视觉在感知东西的时候，一般不会是一个场景从到头看到尾每次全部都看，而往往是根据需求观察注意特定的一部分。而且当我们发现一个场景经常在某部分出现自己想观察的东西时，我们就会进行学习在将来再出现类似场景时把注意力放到该部分上。这可以说就是注意力机制的本质内容了。至于它本身包含的‘自上而下’和‘自下而上’方式就不在过多的讨论。 Attention机制的理解 Attention机制其实就是一系列注意力分配系数，也就是一系列权重参数罢了。 2.5.1 AFM - 引入注意力机制的FM 注意力网络的作用是为每一个交叉特征提供权重 2.5.2 DIN basemodel 基本模型是获得一个固定长度的用户的表示向量，但不管候选广告是什么，此表示向量对于给定用户均保持不变。这样，维度受限的用户表示向量将成为表达用户多样化兴趣的瓶颈 DIN通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给定的广告）。 注意力在其上面的形式是激活单元来生成注意力得分"},{"title":"训练集和数据集数据探索","date":"2022-03-18T13:03:49.043Z","url":"/2022/03/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","categories":[["undefined",""]],"content":"说些废话链接：  比赛 很不错 但是 现在 没有时间去看 数据分析 1. 数据解读 上面的操作就很easy 2. 数据质量分析​ 接下来简单数据探索。在实际建模过程中，首先我们会先校验数据的正确性，并检验缺失值、异常值等情况。 数据正确性校验 所谓数据正确性，指的是数据本身是否符合基本逻辑，例如此处信用卡id作为建模分析对象独一无二的标识，我们需要验证其是否确实独一无二，并且训练集和测试集信用卡id无重复。 判断缺失值情况 3. 异常值分析 describe()方法 异常值检验。由于我们尚未对数据集特征进行预处理，因此我们先查看标签列的异常值情况。首先我们可以用describe()方法查看这一列的基本统计信息： 通过直方图观察 由于是连续变量可以借助概率密度直方图进行分布的观察： $3\\delta$原则进行异常值识别 ​ 能够发现，大部分用户忠诚度评分都集中在[-10,10]之间，并且基本符合正态分布，唯一需要注意的是有个别异常值取值在-30以下，该数据在后续分析中需要额外注意。我们可以简单查看有多少用户的标签数值是小于30的： 当然，对于连续变量，一般可以采用$3\\delta$原则进行异常值识别，此处我们也可以简单计算下异常值范围： &amp;emsp;&amp;emsp;需要注意的是，此处我们是围绕标签进行的异常值检测，而本案例中标签并不是自然数值测量或统计的结果（如消费金额、身高体重等），而是通过某种公式人工计算得出（详见赛题分析）。出现如此离群点极有可能是某类特殊用户的标记。因此不宜进行异常值处理，而应该将其单独视作特殊的一类，在后续建模分析时候单独对此类用户进行特征提取与建模分析。 4. 规律一致性分析&amp;emsp;&amp;emsp;接下来，进行训练集和测试集的规律一致性分析。 &amp;emsp;&amp;emsp;所谓规律一致性，指的是需要对训练集和测试集特征数据的分布进行简单比对，以“确定”两组数据是否诞生于同一个总体，即两组数据是否都遵循着背后总体的规律，即两组数据是否存在着规律一致性。 &amp;emsp;&amp;emsp;我们知道，尽管机器学习并不强调样本-总体的概念，但在训练集上挖掘到的规律要在测试集上起到预测效果，就必须要求这两部分数据受到相同规律的影响。一般来说，对于标签未知的测试集，我们可以通过特征的分布规律来判断两组数据是否取自同一总体 单变量分析 当然，我们需要同时对比训练集和测试集的四个特征，可以通过如下代码实现： 多级联合分布 ​ 接下来，我们进一步查看联合变量分布。所谓联合概率分布，指的是将离散变量两两组合，然后查看这个新变量的相对占比分布。例如特征1有0&#x2F;1两个取值水平，特征2有A&#x2F;B两个取值水平，则联合分布中就将存在0A、0B、1A、1B四种不同取值水平，然后进一步查看这四种不同取值水平出现的分布情况。 ​ 实际建模过程中，规律一致性分析是非常重要但又经常容易被忽视的一个环节。通过规律一致性分析，我们可以得出非常多的可用于后续指导后续建模的关键性意见。通常我们可以根据规律一致性分析得出以下基本结论 ​ 作用： 如果分布非常一致，则说明所有特征均取自同一整体，训练集和测试集规律拥有较高一致性，模型效果上限较高，建模过程中应该更加依靠特征工程方法和模型建模技巧提高最终预测效果 如果分布不太一致，则说明训练集和测试集规律不太一致，此时模型预测效果上限会受此影响而被限制，并且模型大概率容易过拟合，在实际建模过程中可以多考虑使用交叉验证等方式防止过拟合，并且需要注重除了通用特征工程和建模方法外的trick的使用； "},{"title":"深度推荐系统 上","date":"2022-03-15T13:20:16.514Z","url":"/2022/03/15/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(1)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（1）演化方式： 改变神经网络的复杂程度 改变特征交叉方式 wide&amp;deep模型 FM深度版本 注意力机制与推荐系统结合 序列模型与推荐模型结合 强化学习与深度学习结合 2.1 神经网络复杂程度2.1.1 Auto-rec–单层神经网络推荐模型（easy） 通过自编码器原理，还原输入的结果。 重建函数： 目标函数： 目标函数l2正则： 参考： 2.1.2 Deep Crossing模型–经典深度学习架构 应用场景 网络结构 embedding层，stacking层，multiple residual units层，scoring层 ​ 反思 embedding+多层神经网络，相较于传统的二阶特征交叉能力，deep crossing拥有深度交叉的能力 2.1.2 NeuralCF - CF与深度学习的结合先回忆一下传统的矩阵分解怎么做 物品-用户共现矩阵分解成用户向量和物品向量 向量embedding化 embedding后向量取内积（重要） 得到分数 这个模型 复杂的位置就是在第三步操作上 使用多层神经网络去替换这个卷积操作 &#x3D;&#x3D;优势&#x3D;&#x3D; 利用神经网络来拟合任意函数，灵活地组成不同的特征，按需增加或减少模型的复杂度 &#x3D;&#x3D;劣势&#x3D;&#x3D; 基于协同过滤构造,没有引入更多其他类型的特征 在实践中，防止过拟合的风险 2.2 加强特征交叉能力2.2.1 PNN模型感谢：  ​ ​ 相较于 deep crossing模型中的stacking层，PNN模型替换成了乘积层。其他的在模型的输入，embeding层，多层神经网络以及最终的输出层上没有结构上的不同。 ​ product layer层，左边为线性部分，认为 特征之间的关系是and“且”的一种关系，而非add”加”的关系。 ​ z&#x3D;concat([emb1,emb2..,embn],axis&#x3D;1) 其右边操作为乘积操作，有内积和外积的区别。外积在操作上会将问题的复杂度从原来的m到 $m^2$,在选择上更应该慎重。 优势 ​ 定义了外积和内积操作更有针对性地强调不同特征之间的交互 局限 ​ 在外积操作上，为了效率经行大量的简化操作，对所有特征进行无差别的交叉，在一定程度上忽略了原始特征中包含的价值信息。 2.2.2 product layer 内积 PNN中p的计算方式如下，即使用内积来代表pij： 2.2.3 product layer 外积 OPNN中p的计算方式如下： 此时pij为MM的矩阵，计算一个pij的时间复杂度为MM，而p是NNMM的矩阵，因此计算p的事件复杂度为NNMM。从而计算lp的时间复杂度变为D1 * NNM*M。这个显然代价很高的。为了减少负责度，论文使用了叠加的思想，它重新定义了p矩阵： 2.3 记忆能力与泛化能力的综合2.3.1 wide&amp;deep模型​ wide部分是让模型具有较强的“记忆能力”，deep部分是让模型具有泛化能力。这样的结构使模型兼具了逻辑回归和深度神经网络的优点–能快速处理并且记忆大量的历史行为特征，并且具有强大的表达能力。 在提出W&amp;D模型，平衡Wide模型和Deep模型的记忆能力和泛化能力。实际上是lr+dnn。记忆（memorization） 通过特征叉乘对原始特征做非线性变换，输入为高维度的稀疏向量。通过大量的特征叉乘产生特征相互作用的“记忆（Memorization）”，高效且可解释，但要泛化需要更多的特征工程。 泛化（generalization）只需要少量的特征工程，深度神经网络通过embedding的方法，使用低维稠密特征输入，可以更好地泛化训练样本中未出现过的特征组合。但当user-item交互矩阵稀疏且高阶时，容易出现“过泛化（over-generalize）”导致推荐的item相关性差 工程应用 2.3.2 wide&amp;deep进化 deep&amp;cross模型 Cross Network ​ 交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式： ​ xl和xl+1 分别是第l层和第l+1层cross layer的输出，wl和bl是这两层之间的连接参数。注意上式中所有的变量均是列向量，W也是列向量，并不是矩阵。xl+1 &#x3D; f(xl, wl, bl) + xl. 每一层的输出，都是上一层的输出加上feature crossing f。而f就是在拟合该层输出和上一层输出的残差。 ​ Cross Layer 设计的巧妙之处全部体现在上面的计算公式中，我们先看一些明显的细节：1) 每层的神经元个数都相同，都等于输入 的维度 DCN能够有效地捕获有限度的有效特征的相互作用，学会高度非线性的相互作用，不需要人工特征工程或遍历搜索，并具有较低的计算成本。1）提出了一种新的交叉网络，在每个层上明确地应用特征交叉，有效地学习有界度的预测交叉特征，并且不需要手工特征工程或穷举搜索。2）跨网络简单而有效。通过设计，各层的多项式级数最高，并由层深度决定。网络由所有的交叉项组成，它们的系数各不相同。3）跨网络内存高效，易于实现。4）实验结果表明，交叉网络（DCN）在LogLoss上与DNN相比少了近一个量级的参数量"},{"title":"传统推荐系统","date":"2022-03-14T13:48:48.181Z","url":"/2022/03/14/%E4%BC%A0%E7%BB%9F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","categories":[["undefined",""]],"content":"0. 说一些废话这个文章 是大概阅读了 深度推荐系统 写出来的文章 很单纯 那时候 的文章 和热门印象很深的是 FM 和 MF 弄混了 1. 推荐系统技术架构数据+模型 1.1 数据部分通过特征工程，将客户端或服务端采集到的数据进行特征处理 1.2 模型部分主题 一般由召回层，排序层，补充数据与算法层组成 2. 传统推荐模型（粗略整理）2.1 协同过滤分为 用户过滤和物品过滤 2.1.1 用户过滤（没人用）公式一 余弦相似度 公式二 皮尔逊相关系数（减少了用户评分的影响） 2.1.2 物品过滤 基于历史数据，构建用户-物品共现矩阵（m*n） 计算共现矩阵两两向量间的相似性 获得用户历史行为数据的正反馈物品列表 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的topK物品 对于相似度进行排序，生成最终的推荐列表 2.2 矩阵分解算法–狗都不用通过分解协同过滤生成的共现矩阵 得到用户和物品的隐向量 2.2.1 矩阵分解的求解方法梯度下降 目标函数 2.3 特征交叉单一特征的表达性 没有特征组合起来的表达性好 且 单一特征会损失一定量的信息 2.3.1 POLY2模型暴力将特征n个 变成了$ n^2 $ 会将数据更加稀疏 增加训练复杂度 2.3.2 FM模型-隐向量特征交叉（嗯嗯嗯！）感谢： 个人的愚蠢回忆说实话 有些东西一定要好好折磨一下 才能有新的收获 ！ 比如这个 本人大白话讲一遍，啊啊啊 上面的式子太暴力，脑子思考一下就发现权重项太多，我在上面的这篇文献中发现这么一句：**对于任何正定实矩阵 只要k足够大，都存在k维向量组成的矩阵 使得 ** 在这个公式下 这个V啊 就是辅助向量。 这个可爱的V呢 满足 （我想到了2077） 隐向量 就是为每个特征 学习一个隐权重向量（latent vector） 交互使用两个向量取内积就好 优势 1. 权重参数减少到nk ​ 2. 训练复杂度降低到nk级别 2.3.3 FFM模型 特征域感知概念（可笑 咱把东西想简单了）数据还是上一次的数据 FFM模型中引入了类别的概念，即field 在上面的广告点击案例中，“Day&#x3D;26&#x2F;11&#x2F;15”、“Day&#x3D;1&#x2F;7&#x2F;14”、“Day&#x3D;19&#x2F;2&#x2F;15”这三个特征都是代表日期的，可以放到同一个field中。同理，Country也可以放到一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户国籍，广告类型，日期等等 在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量 v_i,fj。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day&#x3D;26&#x2F;11&#x2F;15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来 训练过程中，需要学习n个特征在f个域上的k维隐向量，参数 n * k * f 复杂度为 k$ n^2 $ 远多于FM模型的 nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 O(kn^2)。 数据的处理像Country，day 这样的categorical特征 可以通过 one-hot进行生成 如果是 像price这样的数值特征，则并不需要对其进行单独编码，但需要为其生成一个field。 个人的纯真理解（一眼顶真） 敲散说明白假如说 我这里有4个field 一共有10个feature。 动动小脑就晓得 假如说 field1 对应 feat1到feat4 则对应 feat1到feat4 要生成对应的 field2到field4的权重（这个就是场感知啊！！！） 脑瓜子随便一想 哇靠 真复杂 2.4 GBDT+LR 特征工程模型化 原始特征向量x，通过树分裂 将转化的特征类似于one-hot的向量来表示原始的特征，特征组合能力特别强 但是容易产生过拟合，以及这样的过程丢失了大量特征数值信息。 2.5 MLR 深度学习开始的曙光2.5.1 MLR与LR的区别 普通的LR模型 无法拟合我们所需的曲线 但是MLR模型正常拟合出来了 2.5.2 目标公式 如果m为1 则为普通的LR模型 当m越大 模型的拟合能力越强 而同样 需要的训练样本也变得更大 （阿里巴巴 经验12） 2.5.3 优点 端到端的非线性学习能力 模型稀疏性强 "}]