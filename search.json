[{"date":"2022-06-26T10:33:20.696Z","url":"/2022/06/26/%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/","categories":[["undefined",""]],"content":" 模型调参看的是 李沐的视频   前言 开始一个好的基线 改一个值 重新训练模型 重复很多次 去获得直觉 什么超参数很重要 模型对于超参数 的敏感程度（Adam比SGD 调参要简单很多） 超参数的范围在哪里 结果会好 做好笔记 需要认真的管理 （训练日志和超参数）execl 或着 word tensorboardweight&amp;bias 重现很难（硬件，库，代码，随机性） 自动调参 HPO 超参数优化 更加泛化一点 NAS HPO的小一点版本 专注于神经网络 HPO 哈哈哈搜索空间 算法1 黑盒算法遍历！ 找到最好的 也是最傻的算法 算法2 多准确度（加速）目前再用的算法 小数据集 缩小模型规模 层 通道数 快点停止 搜索策略（黑盒子）第一个暴力穷举（贵） 第二个随机取（有效） 写代码很简单 第三个 贝叶斯 （李沐说这个大研究方向 咱不研究 咱不用） 搜索策略 （多准确度）深度学习较多sh算法 节省花销对于多超参数 随机选取n个参数 去训练m个epoch 算法过程 类似一种 递归的思想在其中 选取n&#x2F;2的超参数进行训练 训练m次epoch 选取n&#x2F;4的超参数训练 训练2m次epoch m n 的选取 取决于 你的预算花销 hyperband 用的较多多跑几个sh算法 在每次跑完后 减小n 增大m 每次都是用不同的m 和 n 影响是 对于 m n的选取 没那么打的影响 Neural Architecture Search (NAS)神经结构搜索 这里只关心 神经网络的参数 强化学习 nas代价非常贵 模型优美 one-shot方法学习模型架构 外加超参数 因为 这个东西看起来就很大 所以 关注点在于 只关心架构之间的排名关系 用一个近似的指标 ： 只训练很少的epoch 来验证 one-shot方法 – 可微架构搜索 多种候选层方法 在每一层 在layer层 中 所有的候选层方法 都会有个输出 对于这个输出 添加一个大于0 且 相加为1 的权重系数相乘 最后 学习 得到 在候选层 中权重最大的给保留下来 通过学习 来判断最好的那一条路径 使用DARTS 这个方法 可以在三个GPU天 完成sota效果 简单实用 effientNet调参不多 cnn优化 更多层 更多输出通道 输入图像更大 目前研究方向 可解释性 调整参数 能使用在边缘设备都能跑 整个流程更加的自动化 "},{"title":"研究的艺术","date":"2022-06-26T02:28:16.575Z","url":"/2022/06/26/%E7%A0%94%E7%A9%B6%E7%9A%84%E8%89%BA%E6%9C%AF/","categories":[["undefined",""]],"content":"研究读者知道什么 想要知道什么 研究是什么东西 收集信息（提出解决方案） 回答一个疑问 解决一个问题 为什么要写作 会记住 帮助理解 测试你的想法 通过写作来看你的想法是不是对的 为什么要用论文格式对于 对方的方便 以及更好的理解 和读者进行连接写作是一个想象中的对话作者自己定义的角色 和 给读者假定一个角色 而且 角色定好 就没有变了 理解作者角色 我找到一些有趣的信息 给你们看看 我找到一个解决方案 在实际问题上 我找到一个重要问题的答案 对应读者角色 娱乐我 （追求新奇的信息） 帮助我解决实际问题（直接跳干货位置） 帮助我更好的理解（大部分的研究者） 怎么去问问题 怎么去找答案 你的研究 是否让人感兴趣 预测读者的回应 我的观点是不是跟所有人的观点是对立的 会不会做标准的一些问句 来反对解决方案 是不是特别关心我是怎么一步一步的解决问题 去讲故事去写故事"},{"title":"虚拟机和mobasterm连接","date":"2022-06-24T13:53:11.268Z","url":"/2022/06/24/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%92%8Cmobasterm%E8%BF%9E%E6%8E%A5/","categories":[["undefined",""]],"content":"未来可能这个教程会被反复的翻阅 我纠结这个东西纠结了好久 现在写一下 首先感谢：   0. 前言这个连接 主要看两个东西 第一个看 虚拟机 即看是不是在同一网段内 第二个 看 虚拟机的ssh服务是不是安装着 我在安装ssh时候出现了错误 感谢：  解决方式： ok 这时候 应该就能快乐连接了"},{"title":"python遇到安装不了的包的解决","date":"2022-06-22T05:03:17.638Z","url":"/2022/06/22/python%E9%81%87%E5%88%B0%E5%AE%89%E8%A3%85%E4%B8%8D%E4%BA%86%E7%9A%84%E5%8C%85%E7%9A%84%E8%A7%A3%E5%86%B3/","categories":[["undefined",""]],"content":"感谢： 操作流程  包编译好的文件 2022&#x2F;6&#x2F;22 今天试了试 paddle ocr 包 说实话 安装的的过程有点慢吞吞 于是乎出现了这个样子的错误 错误的大致描述是 c 没有编译好这个包 嗯！ 1. 破局假如 你的python环境是 3.9 你就在这个网站 找人家编好的文件 然后下载完成后按照：文件位置+文件名的格式，直接pip install，例如我的安装的是： 然后 这个包就转好了！ 2. 感悟 有问题 多看看论坛 github不好使的时候，用用gitee pytorch不好使的时候 用用paddle "},{"title":"GRU4Rec 推荐论文","date":"2022-06-12T03:15:52.399Z","url":"/2022/06/12/gru4rec-%E6%8E%A8%E8%8D%90%E8%AE%BA%E6%96%87/","categories":[["undefined",""]],"content":"这是16年出的一篇基于session时序建模的召回模型paper，在当时大部分模型只考虑用户最后点击行为，而忽略用户历史点击行为。虽然用户最后一次点击行为与用户下一次点击的item相关度很高，但是用户历史点击行为一方面丰富了用户画像，另一方面用户的兴趣是多峰的，不一定最后一次点击是最相关的。 模型到现在看的话 实在是简单了一些 重要的是得看他的亮点 1. 亮点说实话 第一个两点我看了半天（可能是上午的吃的太少 抑或是 钱给的太少） 1.1 亮点1 训练时session重组 好笑的是 我怎么忘了这个东西 本文的训练过程是 通过i1,1 来预测 i1.2 所以训练序列要比 会话序列 要短一个 假设一个batch有三条数据，代表不同的session。大家看到session1有4条数据，session2有3条数据，session3有6条，真正作为input的话，每个session中item序列长度只有3，2，5，next item是作为groud truth。记录下batch最短的input 长度为2。首先组装 作为第一个batch的input，output是 ，然后组装第二个batch，input： ，output： ，这时候session2已经处理完了，但是session1，session2还没有处理完，这时候用session4来替换session2，切换session的时候要重新初始化下hidden state。这时候第三个batch的input变成了 ，output是 。到第四个batch的时候session1也处理完了，使用session5来替换。这样下来就能支持多个session并行处理，极大的提升了训练速度。 注意为了保证Session内的连续性和Session间的独立性： 1.2 亮点2 negative sampling作者解释这种负采样方法，首先提出了一个假设，那就是用户不太可能将没见过的东西标记为负样本。如果一个物品很流行，而用户却没有在next-item位置进行点击，那么用户大概率在该时序点不喜欢这个物品。 基于这样的假设和观察，作者认为，应该基于物品的流行程度进行负采样，那么作者就提出了一个巧妙的快速负采样方法，那就是一个batch中的其他序列的next-item作为负样本。如下图: 序列的next-item的gt是1, 5, 8对于1来讲 5，8是负样本。和随机抽样相比，个人觉得优势有两个: 一个是基于pop的采样，可解释性更强，另一个是对于batch内需要计算的item是一致的，那么可以将batch内所有序列的next-item预测，统一成一个矩阵运算，大大降低显存占用 同一个batch 时序 其他的mini-batch 为负样本 表示 用户在这个时间内 不喜欢该样本 2. 作者的发现 多layer不一定好 one-hot的编码更好 每一步输入前面所有的信息和前一步信息性能差不多 提高GRU的宽度有帮助 作者的发现表明 GRU 足够可以编码 item 信息(one-hot更好)，单个layer就可以达到很好的效果，对序列的建模能力比较好。"},{"title":"TAGNN 文章阅读","date":"2022-06-10T13:41:20.736Z","url":"/2022/06/10/TAGNN-%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB-%E6%9C%AA%E5%86%99%E5%AE%8C/","categories":[["undefined",""]],"content":"自己瞎说一点 这篇文章 是发表在SIGIR上 一篇文章 作者一开始指出 顺序模型 无法有效有效的捕捉项目之间的来回关系。 建议通过会话图发现会话下的复杂过渡模式[2，10，11]。通过将会话中的项目建模为会话图，这种对会话中丰富的时间模式进行编码的自然方法可以为每个项目生成更精确的表示。 然后 使用一个固定的向量 限制了推荐模型的表达能力 观察到，在对特定候选项进行预测时，没有必要将所有用户兴趣嵌入到一个向量中。例如，假设客户的历史会话为（游泳衣、钱包、牛奶、煎锅）。如果我们想为她推荐一个手提包，我们会关注她对钱包的兴趣，而不是对煎锅的兴趣。也就是说，如果给定一个目标项，可以具体激活具有丰富行为的用户的兴趣 这篇论文在SR-GNN的基础上，沿用了门控图神经网络（Gated Graph Neural Networks，GGNN）模型，并加入了对预测目标敏感的embedding表示，下面进行介绍。 0. 前言文章贡献 会话中的项目建模为会话图，以捕获会话中复杂的项目转换 图神经网络来获得项目嵌入 会话中自适应地激活用户的不同兴趣，我们提出了一种新的目标关注网络 （重点） "},{"title":"模型过拟合处理","date":"2022-06-09T08:51:28.478Z","url":"/2022/06/09/%E8%BF%87%E6%8B%9F%E5%90%88%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"0. 前言这篇文章想写好久了，一直拖着。其一开始 在阅读 SASrec的时候 发现为了加入注意力机制 给了三个处理过拟合的方法 为此 想写一个文章 来整理一下 这三个技术的作用 1. dropout感谢： 1.1 出现的原因在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。 过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。 综上所述，训练深度神经网络的时候，总是会遇到两大缺点： （1）容易过拟合 （2）费时 Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。 1.2 啥是dropoutDropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。 Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征 1.3 why？（1）取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。 （2）减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。 （3）Dropout类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝 2. 残差网络(Residual Network)，残差连接(skip-connect)感谢： 2.1 动机深度神经网络的退化问题 以及 梯度弥散&#x2F;爆炸 问题 残差网络很好地解决了深度神经网络的退化问题，并在ImageNet和CIFAR-10等图像任务上取得了非常好的结果，同等层数的前提下残差网络也收敛得更快。这使得前馈神经网络可以采用更深的设计。除此之外，去除个别神经网络层，残差网络的表现不会受到显著影响 3. 归一化 BN 和 LN区别  还是区别  归一化的整体理解  norm pytorch 怎么用 3.1 作用 Norm起作用的本质是它平滑了Loss，保持了梯度下降过程中的稳定。 "},{"title":"图神经网络入门","date":"2022-06-09T06:42:51.894Z","url":"/2022/06/09/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"感谢： 博客  李沐介绍视频 0. 前言首先，让我们确定什么是图。图代表了一组实体（结点）之间的关系（边）。 V node 节点 E link 连接 U master node 整个图 1. 描述（抽象）为了进一步描述每个节点、边或整个图，我们将图中的每一部分信息进行储存。 在存储方面 我们存下了点，边，整个图的信息 进行存储 图 也分为有向图和无向图 2. 图的实例化讲解作者 使用了两个反直觉的数据类型用图来表示 2.1 图像我们通常认为图像是具有图像通道的矩形网格，将它们表示为数组（例如244x244x3的浮点）。另一种思考图像的方式是具有规则结构的图，其中每个像素代表一个节点，并通过边连接到相邻的像素。每个非边界像素正好有8个邻居，每个节点存储的信息是一个代表像素RGB值的3维矢量。 通过邻接矩阵来可视化图形的连通性的一种方法。我们对节点进行排序，在这种情况下，在一个简单的5x5的笑脸图像中，每个节点都有25个像素，如果两个节点共享一条边，就用一个条目来填充n×n的矩阵。请注意，下面这三种表示方法都是对同一份数据的不同看法。 图像的三种表示方法 2.2 文本我们可以通过为每个字符、词或标记关联索引来数字化文本，并将文本表示为这些索引的序列。这就形成了一个简单的有向图，其中每个字符或索引都是一个节点，并通过一条边与后面的节点相连。 当然，在实践中，这通常不是文本和图像的编码方式：这些图表示是多余的，因为所有图像和所有文本都会有非常规则的结构。例如，图像的邻接矩阵有一个带状结构，因为所有的节点（像素）都是以网格方式连接的。文本的邻接矩阵只是一条对角线，因为每个词只与前一个词和后一个词相连接 3. 什么样子的问题会用到图结构3.1 图级看起来很弱智的 找圈圈个数 3.2 节点级​ 节点级任务关注的是预测图中每个节点的身份或角色。​ 节点级预测问题的一个典型例子是Zach的空手道俱乐部。​ 该数据集是一个单一的社会网络图，由在政治裂痕后宣誓效忠于两个空手道俱乐部之一的个人组成。正如故事所言，Hi先生（教练）和John H（管理员）之间的争执在空手道俱乐部中造成了分裂。节点代表空手道练习者个人，边则代表这些成员在空手道之外的互动关系。预测问题是对一个给定的成员在争斗后是否会效忠于Hi先生或John H进行分类。在这种情况下，一个节点与教官或管理员之间的距离与这个标签高度相关 [] ​ 按照图像的类比，节点级预测问题类似于图像分割，我们试图标记图像中每个像素的作用。对于文本，类似的任务是预测句子中每个词的语音部分（如名词、动词、副词等）。 3.3 边级 通过语义分割 分析出实体对象 然后对实体对象之间的关系 使用机器学习进行预测 之后用图来进行表示 4. 图的表示 自己的看法 dim &#x3D; 0 的位置上 每个节点一一对应 节点有节点 自己的分类 边也是一样 如果想表示一个图两个节点是否相互连接 可以使用【node，node】来表示 global 也有其自己的分类表示 4.1 网络表示 这个GNN在图的每个分量上使用一个单独的多层感知器（MLP）（或你最喜欢的可微分模型）；我们称之为GNN层。对于每个节点向量，我们应用MLP并得到一个学习的节点向量。我们对每条边做同样的工作，学习每条边的嵌入，也对全局背景向量做同样的工作，为整个图学习一个单一的嵌入。 4.2 通过池化信息 来完成图预测（简单gnn模型）可能 我们知道了边的信息 如果通过边的信息来 计算 节点的信息 对节点进行预测池化分两步 收集节点的边信息 通过汇总这些信息来进行预测 所以 如果我们只知道 边的信息 使用池化来路由（或传递）信息到它需要去的地方。该模型看起来像这样。（这个机制貌似叫 消息传递） 使用点信息来 信息传递出 边的总类 使用点信息来算出 图的种类 对点的信息进行汇聚 然后 就像cnn 一样 对这个图的信息进行处理 4.3 通过信息传递 完成 图的更新信息传递分为三步 收集每个节点 所有相邻节点的嵌入 g函数 通过聚合函数 聚合消息 所有汇集的消息 通过一个更新函数（通常学习神经网络） 正如池化可以应用于节点或边一样，消息传递也可以在节点或边之间发生。 这些步骤是利用图的连接性的关键。我们将在GNN层中建立更详细的消息传递变体，以产生表现力和力量不断增强的GNN模型。 "},{"title":"2022年会话推荐综述","date":"2022-06-05T11:45:38.441Z","url":"/2022/06/05/2022%E5%B9%B4%E4%BC%9A%E8%AF%9D%E6%8E%A8%E8%8D%90%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"最近对于会话推荐有了新的兴趣 文章题目： A Survey on Session-based Recommender Systems 0. 前言 提供了一个统一的框架来对SBRSs研究进行分类 SBRS的统一问题陈述，其中SBRS建立在正式概念之上：用户、项目、动作、交互和会话我们全面概述了会话数据的独特特性以及由此带来的SBRSs挑战 会话任务方法进行了系统的分类和比较 全面了解如何应对挑战以及SBRS领域取得了哪些进展简要介绍了SBRSs的每一类方法以及关键技术细节 讨论了SBRS研究中存在的问题和前景 1. 序列推荐和会话推荐的区别 作者对于Boundary解释 是指在事务事件中启动和结束特定会话的开始-结束交互对 会话 可以分为 有序会话 和 无序对话 这个session 内 交互 内item 是不是按顺序分布的来区分是否为 无序有序 对于边界间隔 session 有很多个 而 序列 只有单一一个 对于 其 嵌入的主要关系 基于 会话的 是 共现关系 而 基于 序列的 是 顺序依赖关系 2. 会话推荐2.1 目的会话推荐需要注意attention SBRS旨在通过学习会话内或会话间的依赖关系，预测给定已知部分的会话的未知部分（例如，一个项目或一批项目），或给定历史会话的未来会话（例如，下一个篮子）。 原则上，SBRS不一定依赖会话内的顺序信息，但对于有序会话，可以利用自然存在的顺序依赖性进行建议。 2.2 框架主要工作分为三个子领域 其子领域 可以分为 下一次交互推荐 下一次部分会话推荐（即 下一个会话出现了一部分 预测剩余的部分） 下一次会话推荐 Point-Of-Interest (POI) 生词 其实 在框架体现中 可以看出 下一个项目的推荐是 最多的还是一个交互的推荐 2.3 相关的研究作者认为现有的研究没有发现任何系统地将这一研究领域正规化的研究，或全面分析会话数据的独特特征和SBRS所面临的关键挑战。更不用说提供一个深入的的总结，或详细说明该领域存在的公开研究问题。 对于 相关的研究 习惯将 RS 和 SBRS 混为一谈 且 特别针对 SBRS 的研究特别少。工作主要集中在序列感知RSs上，只讨论了一小部分基于有序会话数据的SBRS工作，而忽略了基于无序会话的SBRS。 2.4 会话推荐的主要符号 一个表征通常被指定为一个潜在的向量 通过这里 可以看出 不同于 序列推荐的 ui 组合 在会话推荐中 更倾向于 UVO 这样的三元组组合 2.5 SBRS问题陈述一个RS可以被看作是一个系统[8, 9]，它由多个基本实体组成，包括用户。物品和它们的行为，例如，用户与物品的互动。这些基本实体和行为构成了会话的核心成分，也就是SBRS的核心实体。因此，我们首先介绍这些实体和行为的定义和属性，然后在此基础上定义SBRS问题。基于它们的定义。这些定义和属性将被进一步用于SBRS的特征和这些定义和属性将进一步用于SBRSs的特征和分类，等等。 2.5.1 用户表示（与序列推荐有些不同）在SBRS中，用户是对物品（如产品）采取行动的主体，如点击、购买。并接受推荐结果。让u表示一个用户，每个用户都有一个唯一的ID和一组描述她的属性，例如，一个用户的性别，它有多个值。如：男性和女性。一个用户的属性可能会影响她对项目的操作，并进一步影响相应的会话。 除了可以明显观察到的显性属性外，还有一些隐性属性，它们反映了用户的内部状态。例如她的情绪和意图，也可能对她的行为产生重大影响。所有的用户共同组成了用户集，即U &#x3D; {u1,u2,. . . ,u |U |}。需要注意的是，一个会话的用户信息可能并不总是可用的，原因有二： (1)由于隐私保护，它不会被记录下来； (2)一些用户在与在线平台互动时不会登录 如amazon.com。因此，会话成为匿名的 2.5.2 item表示 v表示一个项目，该项目与唯一ID和一组属性相关联，以提供项目的描述信息，例如项目的类别和价格。数据集中的所有项目构成项目集，即V&#x3D;{v1，v2，…，V | V |} 没什么好说的 很简单 2.5.3 动作表示 用户通常在会话中对某个项目执行操作，例如单击某个项目。让a表示一个动作，该动作与一个唯一ID和一组属性相关联，以提供其属性信息，例如动作的类型，并具有多个值，例如单击、查看和购买。请注意，某些操作可能与特定项目无关，例如搜索操作或目录导航操作。但如参考文献中所述，它们仍可能为SBRS提供有用的信息 2.5.4 交互 user 交互 available o &#x3D; &lt;u, v, a&gt; no available o &#x3D; &lt;v, a&gt; no available and action only one o &#x3D; 2.5.5 会话会话包含交互!! s &#x3D; {o1,o2, . . . ,o |s |}. 注意一点 单一会话中 可能会出现重复的交互 每一个对话 都与一组属性相关联 例如 持续时间s （ 20分钟或40分钟 ） 属性 定义与影响 session length 会话的长度定义为会话中包含的交互总数。这是会话的一个基本属性 internal order（内部秩序） 一个会话的内部秩序指的是其内部交互的秩序。通常情况下，在不同的会话中存在着不同类型的灵活秩序，即无秩序、灵活秩序和秩序。 action type 在现实世界中，一些会话只包含一种类型的操作，例如购买，而其他会话可能包含多种类型的操作，例如单击、购买。会话中动作类型的数量决定会话内依赖关系是否是同质的（基于单个动作）或异构（基于多种类型的操作），这对于准确的建议很重要 user information 用户信息在连接同一用户在不同时间发生的会话方面起着重要作用，因此其可用性决定了为特定用户跨多个会话建模长期个性化偏好的可能性。用户信息的属性是指会话中用户信息的可用性。实际上，SBRS最初被提议用于处理用户信息不可用的匿名会话 session-data structure 是指与会话相关的层次结构组成的多层次结构。交互层由每个会话中的交互组成，而会话层则由当前用户的多个历史会话组成。（即区分会话和交互） 这个图 可以看出 会话的第五个属性 ： 会话的层次结构 2.6 问题定义2.6.1 输入(1) 当前会话的已知部分（即已发生的交互的列表），这是SBRS的输入。它只为下一个交互（项目）建立会话内的依赖关系，或下一个部分会话的建议（参见第2.2节）。 (2) 已知的历史会话列表，它是主要为下一个会话（如abasket）推荐建立会话间依赖关系模型 (3) 前两者的组合，这是SBRSs的输入为下一次交互的推荐建立会话内和会话间依赖关系的模型。或下一个部分会话建议。 在特定情况下，当前会话或历史会话的输入部分可以是匿名或非匿名、有序或无序的，并具有单一或多种类型的操作。根据我们的观察，大多数现有的SBRS假设输入会话是有序的，并且具有单一类型的操作 2.6.2 输出 SBRS的目标是根据给定的会话上下文，即已知的会话信息，提出建议 （1）在下一次交互建议中，输出是备选交互（项目）的列表，按最佳匹配排序为会话中的下一次交互（项目）；（2） 在下一部分会话建议中，输出是完成当前会话的交互（项目）列表；（3）在下一次会议建议中，输出是组成下一次会议的补充互动（项目）列表 2.7 挑战针对2.5.5 对于会话提出的5个属性 提出了相对应的5个挑战 2.7.1 会话长度根据会话长度，会话大致可分为三种类型：长会话、中会话和短会话，而长会话、中会话和短会话的具体定义可能因特定数据集而异 会话 描述 挑战 长会话 一个长会话包含相对较多的交互，例如超过10次。总的来说，通过更多的互动，长时间的会话可以提供更多的上下文信息，以获得更准确的建议。然而，由于用户行为的不确定性，长会话更有可能包含与其中其他交互无关的随机交互。这会带来嘈杂的信息，从而降低建议的性能 第一个挑战是如何有效地减少不相关交互中的噪声信息。另一个挑战是如何有效地学习复杂的依赖关系以获得更好的推荐性能 中会话 中等会话通常包含中等数量的交互，例如4到9次。根据我们对电子商务行业交易记录生成的会话数据的观察，中间会话是最常见的情况。与长会话和短会话相比，中等会话不太可能包含太多不相关的交互，而它通常包含基于会话的推荐（SBR）所需的上下文信息。 即如何有效地提取相关和准确的上下文信息以获得准确的建议。 短对话 一个简短的会话包含非常有限的交互，例如，通常少于4次，导致可供推荐的信息有限。例如，在由两个交互组成的脱机匿名会话中，可以用来推荐第二个交互（项目）的唯一上下文信息是会话中的第一个交互。一种极端情况是建议会话的第一次交互。 如何在有限的背景信息下有效地提出建议 2.7.2 内部顺序 （会话内是否存在顺序） 内部顺序 描述 挑战 无序对话 无序会话包含的交互之间没有任何时间顺序，也就是说，会话中的交互发生得早或晚没有区别。因此，通常使用的序列模型不适用。与顺序依赖相比，基于共现的依赖通常相对较弱且模糊，更难学习。此外，交互之间大多数基于共现的依赖关系都是集体依赖关系，即会话中的多个上下文交互协同导致下一个交互的发生，而下一个交互更难捕获。 如何有效地学习交互之间相对较弱和模糊的依赖关系，尤其是那些集体依赖关系 有序对话 有序会话包含多个具有严格顺序的交互，它们之间通常存在强的顺序依赖关系 长顺序会话中有效学习级联的长期顺序依赖是一个挑战 灵活安排 灵活排序的会话既不是完全无序的，也不是完全有序的，即会话的某些部分是有序的，而其他部分不是 须仔细考虑和准确了解灵活排序会话中的复杂依赖关系 来自于如何有效地学习复杂和混合的依赖关系，即有序交互之间的顺序依赖关系和无序交互之间的非顺序依赖关系 2.7.3 动作类型 动作类型 描述 挑战 单一动作 单一类型的操作会话仅包括一种类型的操作，例如单击项目，因此只有一种类型的依赖关系来自同一类型的操作，这相对容易学习 很好学习 多种动作 一个多类型的动作会话包括多种类型的动作[54]，从而导致多种类型的交互。在多类型操作会话中存在复杂的依赖关系。具体来说，依赖性不仅存在于同一类型的交互上（例如，点击项目），还存在于不同类型的交互上（例如，点击和购买）。 如何有效准确地了解行动内和行动间类型的依赖关系，以获得准确的建议 2.7.4 用户信息 用户信息 描述 挑战 不匿名 非匿名会话包含与相关用户信息的非匿名交互，从而支持同一用户在不同时间生成的不同会话之间的连接。这使得了解用户的长期偏好以及其在会话中的演变成为可能 准确地了解个性化的长期偏好，而不是多个非匿名会话，这是一个相当具有挑战性的问题 匿名 在匿名会话中，由于缺少连接同一用户生成的多个会话的用户信息，因此几乎不可能为当前会话收集以前的历史会话。因此，只有来自当前会话的上下文信息才能用于建议。 利用有限的上下文信息精确捕获用户的个性化偏好以提供准确的推荐是一个挑战 2.7.5 会话数据结构 数据结构 描述 挑战 单级会话 单级会话数据集通常是一组匿名会话，其中每个会话由多个没有属性信息或历史会话信息的交互组成。在这种情况下，建议只能使用单级依赖项，即会话内的交互依赖项。因此，由于缺乏其他级别的辅助信息，基于单级别会话数据构建的SBRS很容易受到冷启动或数据稀疏问题的影响 即当只有交互依赖关系可用时，如何克服冷启动和稀疏性问题以获得准确的建议 多级会话 多级会话数据涉及至少两个级别的层次结构，即交互级别加属性级别和&#x2F;或会话级别。在这种情况下，每个级别内和不同级别之间的依赖关系都会影响后续的建议。例如，多个项目的类别（属性级别）可能会影响这些项目是否会在一个会话中一起购买（交互级别） 如何全面了解级别内和级别间的依赖关系以获得有效和准确的建议，成为构建在多级会话数据上的SBRS面临的一个关键挑战 3 SBRS方法的分类和比较分类部分 这里写了个大概 还得仔细去看 三大方法为 常规SRS方法 潜在特征方法 深度神经网络方法 3.1 常规方法 3.2 SBRSs的潜在表示方法 SBRSs的潜在表示方法首先使用浅层模型为会话内的每个交互构建低维潜在表示。学习到的信息表示对这些交互之间的依赖关系进行编码，然后将用于后续基于会话的建议 3.3 SBRSs的深度神经网络方法 4. SBRS应用程序、算法和数据集4.1 SBRS应用"},{"title":"快速傅里叶变换","date":"2022-06-03T01:52:58.905Z","url":"/2022/06/03/%E5%BF%AB%E9%80%9F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/","categories":[["undefined",""]],"content":"6&#x2F;3 日 经过两天的阅读 大致掌握了 快速傅里叶变换是个啥 在这里 致谢：   视频对于我理解不到的部分 知乎的文字部分做了充分的解释 0. 前言快速傅里叶是一个怎么样的算法 一个有效 且 漂亮的算法 1. 举个例子 有一个多项式a 和一个多项式b 在a为两阶多项式 b同样为两阶多项式 算出来的c为四阶函数 对于一个d阶的多项式 可以通过 d+1 个点 来确认曲线 线性方程 转换成 矩阵形式 当点互不相同时，对于未知数而言，其系数矩阵为范德蒙矩阵，必可逆，故有唯一解，这唯一确定了多项式系数也即唯一确定了多项式 2. 回到方程本身 对于 方程进行多项式拆分成 偶函数和奇函数 然后划分成这个鬼样子 这个鬼样子的方程 有一个特性 为什么要去x^2 呢 应为 当x取正负的时候 x^2 总是为正 这样就能少算一个值 然后 多获得 一个点 后面有个奇函数项 其值是不存在配对项的 所以 这个循环是有问题的 3. 复数的提出在这里 最创新的思维来了 有一些复数的平方之后 依旧是正负成对出现 通过 将x替换成复数范围内的值 既可以完成我们点与点成对出现的目标 欧拉方程 使用w将多项式中x替换 其时间复杂度为 因为值是对应的 所以 只需要计算一半的值就好了"},{"title":"python mysql输出","date":"2022-05-28T12:08:29.860Z","url":"/2022/05/28/python-mysql%E8%BE%93%E5%87%BA/","categories":[["undefined",""]],"content":"这个文章会有点长 从安装到mysql基础 到 python 操作 mysql OK 现在开始 mysql安装和卸载 8.0.26安装感谢： 文章没什么大问题 注意 安装配置的问题 下载安装包（没错） 安装配置（有一些很注意的东西） 解压 编写MySQL配置文件 在解压目录下新建my.ini文件 将下面文本拷贝进my.ini文件中 注意 下面的 两个地址basedir 和 datadir 这个很重要 配置环境变量 （没错） 卸载感谢： 教程完全无误 很不错 mysql 教程感谢： 这边找到了一个很好的工具网站 可以通过 json 转 mysql  Python之pymysql详解pymysql 是个python 操作数据库的工具 然后 我连了下数据库 发现能用 很不错 感谢 ： "},{"title":"会议总结","date":"2022-05-28T12:02:32.002Z","url":"/2022/05/28/%E4%BC%9A%E8%AE%AE%E6%80%BB%E7%BB%93/","categories":[["undefined",""]],"content":"5&#x2F;28日 有幸 有两个同学读博士 9&#x2F;30 - 12&#x2F;17 咱也不晓得是个好事还是个坏事 今天的帮老师 真的是 把我震惊到了 还有教育机器人 不会说招我是来干这个的吧 吐槽完成 开始码字 徐老师的总结 问题化学习 基础 发文章 新文章 掌握 创新点 思路 把握好 个人战斗 和 团队工作 top顶会 博客 论文 中文文献也得看 模型-代码复现-idea 做好文献整理（从几个方向进行总结 给文章打好标签） 项目申请 申请项目 注意申请内容 记住后期结题 毕业答辩 注意格式 两章算法 一章系统 小论文 多发小论文 代码和模型（问的多） 2-3篇 实验做得好 文章差不多一个星期能写好 工作方向突感感觉 自己就只有一年的时间 来准备自己的工作计划了 挺焦急的 现在是秋招提前批 注意胆子大 加油！"},{"title":"代码开始","date":"2022-05-15T12:46:17.091Z","url":"/2022/05/15/%E4%BB%A3%E7%A0%81%E5%BC%80%E5%A7%8B/","categories":[["undefined",""]],"content":"​ 如同我想说的那样 我希望在我写代码或者复现别人代码的开始的时候 就能使用一下这一篇文章对于我的思路的梳理 有较大的作用 感谢：  2022年5月15日 晚上 我看到了这篇文章 导入包和版本查询 可复现性在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。 显卡设置 如果只需要一张显卡 如果需要指定多张显卡，比如0，1号显卡。 也可以在命令行运行代码时设置显卡： 清除显存（在跑高显存的代码） 也可以使用在命令行重置GPU的指令 "},{"title":"caser 学习总结","date":"2022-05-14T07:38:35.023Z","url":"/2022/05/14/caser-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","categories":[["undefined",""]]},{"title":"修改意见","date":"2022-05-12T01:20:08.628Z","url":"/2022/05/12/%E4%BF%AE%E6%94%B9%E6%84%8F%E8%A7%81/","categories":[["undefined",""]],"content":"贡献那块位置 第一章knowledge point - &gt; skill （1）situation -&gt; state （2）layer -&gt; 模块 （3）模型改成原来的方法 第二章​ 相关工作（相对来说 可以乱写） 对于同一篇文章的不同方法 是可以引用多次 使用不同的方法 可以通过中文的参考文献 来找到外文的资料 个性化学习 考虑到学生因素 KT知识追踪 第三章3.1 拆散 3.2 改名字 学习者因素分析 提出 我们创新的两个点 3.3 embedding 总结 lpkt的embedding过程 总结 dif sdf learning 阶段 4.4 改名字"},{"title":"专利的撰写","date":"2022-05-07T03:12:09.358Z","url":"/2022/05/07/%E4%B8%93%E5%88%A9%E7%9A%84%E6%92%B0%E5%86%99/","categories":[["undefined",""]],"content":"昨天2022年5月6号的昆工的王老师 来到我实验室对门 我穿着拖孩 去听老师的课 收益良多 感谢 昆工王老师 1. 前言小论文改专利 十分easy （因为 小论文在发的过程中，编辑就帮你审核过了） 专利通过第五个专利号 就能区分 1 发明 2 实用 3 外观 这三种 不要被外观专利给欺骗了 哈哈哈 创新思维 + 技术手段 &#x3D; 授权专利 2. 大方向专利怎么写在问题方向为解决实际问题和解决理论上的问题 2.1 实际方向提出问题+怎么解决问题 2.2 理论上给定一个设定场景 总结现有的问题 现有的方法是如何解决问题的 3. 专利内容3.1 背景技术第一段 大背景 第二段 现有专利方法 100 - 200字 （现有技术） 3.2 发明内容针对解决的问题 技术方案 效果 对于 要保护的步骤 提出 总步骤 对于自己创新点的步骤 写出具体的细分步骤 （具体创新点进行详尽解释） 具体的效果 3.3 权利要求书可以算是具体实施方式的简化版 3.4 具体实施方式对于专利步骤进行完整的解释 越多越好 要有图表 完整"},{"title":"colab使用","date":"2022-05-07T02:49:27.418Z","url":"/2022/05/07/colab%E4%BD%BF%E7%94%A8/","categories":[["undefined",""]],"content":"首先说第一句话 这玩意真难用 不能上传压缩包 然后网上解压（可能我还没找到解决方法） 可能是我水平不太行 感谢我的师兄 李子杰师兄 给予我账号 和 验证码（每次如一日的验证码给予） 在使用的时候 主要出现了三个问题 不会运行.py 文件 不会运行google硬盘的.ipynb文件(我的解决方法相当暴力) 文件上传（文件夹上传） 问题1 文件上传和py文件运行 上传google drive 打开Google drive并登陆 在空白处右键，可在drive中上传文件\\文件夹 （时间比较长） 打开colab 新建笔记本 挂载 Google Drive 进入文件所在目录（当然这里的path 是需要改的啦） 运行目录下的.py文件（执行系统命令，需要在命令前加感叹号） ​ 问题二 不会运行google硬盘的.ipynb文件 优雅的方法来了！！！！怎么理解的呢 当然是 在跑colab跑王老师的代码的时候 自己做琢磨出来的 第一步 打开文件 第二步 打开笔记本 第三步 选你想要 "},{"title":"深度框架 4D数据格式","date":"2022-05-04T02:28:26.245Z","url":"/2022/05/04/%E6%B7%B1%E5%BA%A6%E6%A1%86%E6%9E%B6-%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/","categories":[["undefined",""]],"content":"感谢： 实验室的涛哥  0. 前言对于 Caser竖直卷积 的操作 嗯嗯嗯 在阅读论文的时候 看的不是特别明白 咱就想着 看看代码 于是乎 代码也看得不是怎么明白 最简单最简单 的就是 自己讲想要用的代码给co下来 自己跑一边 于是乎 于是乎 我发现了一个自己的一个很基础的错误 吐槽一下 可能本人功底不行 咱实在没看懂 论文这一段是啥 1. 基本概念深度学习框架，数据为4D 用NCHW或NHWC表示 N - Batch C - Channel H - Height W - Width 在pytorch 为 NCHW 在tensorflow 缺省NHWC GPU支持NCHW 2. 逻辑表达 假定N &#x3D; 2，C &#x3D; 16，H &#x3D; 5，W &#x3D; 4，那么这个4D数据，看起来是这样的： 3. 举例子 本人的竖直卷积 tensor([[-0.5610, -0.0843, 0.8302, 0.0297, 0.3825, 0.6435, 0.2758, 0.3105, -0.1820]], grad_fn&#x3D;) torch.Size([1, 9]) 9 &#x3D; 输出通道数 * 要卷积的次数 3*3 "},{"title":"argparse操作","date":"2022-05-03T12:28:12.894Z","url":"/2022/05/03/argparse%E6%93%8D%E4%BD%9C/","categories":[["undefined",""]],"content":"0. 前言对于 argparse 这个命令行小助手 其 对于深度网络的开发具有相当重要的作用 一开始 对于开发者来说 对于其描述 add_argument 定义 一眼就能看出来 这个东西需要啥 要给啥 其使用 具体三个步骤 实例化 ArgumentParser 使用add_argument函数添加参数 使用parse_args 解析参数 1. 实例化ArgumentParser（挺固定的） 2. 添加参数举一下本人的例子 在add_argument 中间有三个参数 参数一： 看作变量名 前面得加 – 参数二： 变量数据类型 参数三： 缺省值（default） 目前 俺觉得俺能学这三个参数就好了 3. 解析参数最后会被解析成 神奇操作 这里存在 我叫做变量的继承 挺有意思的 被封装成namespace对象 不能在使用add_argument 函数对其 变量进行添加"},{"title":"对于embedding操作 新的理解","date":"2022-05-01T13:15:55.905Z","url":"/2022/05/01/%E5%AF%B9%E4%BA%8Eembedding%E6%93%8D%E4%BD%9C-%E6%96%B0%E7%9A%84%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"2022年5月1日21点20分 先放一张图片 high！ 一下 在这里终于能理解到 为什么pytorch 的好用 和我对embedding 操作 将近一个月的误解！！ 我真的是high到不行了 0. 前言 （很重要）​ 对于一位5月1号 在看着风骚律师 在看着一个名字叫做caser推荐论文的同学。这里面论文里面的一句话，深刻的激发了我。即 俺终于知道如何编码序列推荐数据了！ ​ 其实 在这里面最主要的一个体会是！ ​ 勇敢 即 主动迈出那一步！！ ​ 说太多了 ​ 先看看论文里面的那句话！ ​ 这里的set和universe 用的太美了 brilliant！ ​ 下面这句话非常重要 ​ 用户的交互序列其实就是一个物品序列的组合！！！ 1. 自己以前的误解1.1 误解1本人一直以为 embedding 一定得是one-hot 转换成 embedding-vector 即 我认为简单数字是不能embedding的 大错特错！！！ 数字为什么就不能表示物品的特征呢！ 数字为什么就不能表示物品的特征呢！ 数字为什么就不能表示物品的特征呢！ 数字 简单，那么可爱 就应该被embedding 我觉得我这里的误解 应该是犯了教条主义的错误！！！ 哈哈哈 1.2 误解2即 我认为的交互序列是存在点击和不点击这样子的其他属性的 正好最近阅读了 知识追踪 反而没起到正作用 反而放这个让我的误解加深了！ 论文里面的一句话！ 用户序列 中的元素 即为物品序列的子元素 1.3 误解3我觉得这里 应该是犯了 实践-理论-实践 这个基本道路的错误 为什么 你在没有实践后 就贸然的翻阅理论呢！ 可笑!! 我这一周 基本一直在思考 如何找人家 是如何讲数据集 处理成 序列推荐 模型的数据 然后 一直在抱怨 为什么 我的数据集 相对 cv 的 数据 是多难处理 最后 在读caser 这篇文章的时候 才发现自己是多么的沙雕！ 本来就是一句话的事情 自己想的太复杂了 ​ 心得1 多看论文 说不定有个人能说的很明白 一句话的事儿 心得2 多实践 只有多试试 才能 发现找到新的道路 2. 上代码 观察两个output 其 元素是对应的 其元素是对应的 对应的 好的 我说完了！"},{"title":"PyTorch 张量操作","date":"2022-04-26T12:53:22.333Z","url":"/2022/04/26/pytorch-%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C/","categories":[["undefined",""]],"content":"感谢：  0. 前言​ 在阅读别人编写的源码的过程中，对于他人能够灵活操作torch向量 感觉十分的神奇，于是乎俺也来写一个文档作为一个手册的使用不就好了嘛。（先统计） 1. 基本数据形式读Caser论文有感！其对于数据的操控 我作为一个单纯的研究生感到大受震撼 4d数据结构深度学习框架，数据为4D 用NCHW或NHWC表示 N - Batch C - Channel H - Height W - Width 在pytorch 为 NCHW 在tensorflow 缺省NHWC GPU支持NCHW tensor.size [N,C,H,W] 1.1 有些数据 这样子的[N,H,W]当然 在我推荐领域 channel 也不存在rgb 这么丰富的颜色 直接看代码 给channel 设置成1 就好 1.2 还有些数据 是这个亚子 [w,h]那就这个亚子吧 因为 加batch（一般不为1） 所以 我也没啥好的方法来着 1.3 有些数据 是最后这个样子 [N,H] [N,W]那这个样子呢 是存在补救空间的 嗯嗯！ 2. 我看到的操作张量的数据类型 PyTorch有9种CPU张量类型和9种GPU张量类型 张量基本信息 数据类型转换 torch.div 可以看出来 这玩意是要对其的 squeeze()当其中没有参数的时候，其缺省值是将里面维度为1的进行压缩 感谢： 压缩 原来是压缩 维度为1的操作啊 首先 先说明白 维度一维 线 二维 面 三维 正方体 在程序中 就是 0，1，2 来表示上面的三维 本人自己的操作 unsqueeze()我的很直接的说法就是 这玩意是拿来扩维的 这个函数必须在其中填入dim&#x3D;？这个参数 如果单纯从数字角度来考虑这个事情，那就很简单 例如 torch.cat我觉得我在代码中的注释 很好的解释了这个问题 torch.view感谢：  网上的看不懂啊 网上的看不懂啊 网上的看不懂啊 可能是我水平差了 我用我的大白话来讲明白这个事儿 ​ 日常我们输入卷积层的数据形式 为 4D，view这个小妖精呢，我的感觉就像是捏泥巴。我脑子响起了东北玩神曲 给大家看看有意思的 注意看两次的size 打印 我对于这个玩意的感觉就是将数字打散 然后重新组合起来 十分有趣 torch.mm torch.bmm torch.baddbmm感谢：  "},{"title":"git 沙雕使用","date":"2022-04-26T12:26:08.345Z","url":"/2022/04/26/git-%E6%B2%99%E9%9B%95%E4%BD%BF%E7%94%A8/","categories":[["undefined",""]],"content":"我作为个人开发者，对于git的使用了解个大概就好 首先 先拉一张图 在git的分级中存在 这样的工作目录 这个暂存区 言如其名 我也不晓得是啥 本地仓库 就是文件发送的最后一个位置 远程仓库 对于 我们来说就是github了 1. 本人可能进行的操作 够了！"},{"title":"毕业论文思路","date":"2022-04-23T06:40:11.842Z","url":"/2022/04/23/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E6%80%9D%E8%B7%AF/","categories":[["undefined",""]],"content":"前言做一个能扫描视频内容的搜索框  我能思考到的技术点 ocr扫描 视频内容对其 模糊搜索 关键帧抽取 时间 ：2022&#x2F;4&#x2F;23 功能 生成视频目录 视频信息ocr读取 "},{"title":"顺序推荐模型综述","date":"2022-04-21T11:32:08.954Z","url":"/2022/04/21/%E9%A1%BA%E5%BA%8F%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/","categories":[["undefined",""]],"content":"感谢： 名字：Sequential Recommender Systems: Challenges, Progress and Prospects ​ 首先：在阅读这一篇文献的时候，这篇综述不仅仅像我看过的普通的文章的整理，它对于整个问题的分析，感想后，对于这个问题目前的解决的方法，对于行业目前的状况进行了把握，虽然这一篇19年的文献，本人更希望这篇文章是21年的，毕竟能给予更多的指导。 ​ 文章提出了目前序列推荐模型5个困难，对于模型进行了分类，且对于开放方向进行了整理。 ​ 本次我最明显的收益是 即序列中的短序列为会话（） ​ 哦吼 多说无益 干活！ 0. 前言啥子是序列推荐模型（SRS）咧？ 大师或者小学二年级学生会这么回答 input: u,i交互 S output:一个排名靠前的候选列表 R 过程：对复杂顺序关系建模 公式呢？ R &#x3D; argmax f(S) S &#x3D; {i1, i2, …, i|S|} ij &#x3D;&lt; u, a, v &gt; 元素 解释 u 用户 a 行为 v 物品 1. 推荐模型的挑战按作者总结，一共有5个挑战。 1.1 长序列的处理 序列的高阶依赖 长期的顺序依赖关系 挑战这一个工程非常有限，其主要的方法是对应上面的两个小问题进行分模型处理 1.2 以灵活的顺序处理用户项目交互序列​ 并非所有相邻的交互都在序列中顺序相关，eg 购物序列S2&#x3D;{牛奶、黄油、面粉}中，先买牛奶和黄油并不重要，但是面粉的顺序取决于它们的组合。 ​ 看到上面的例子，可以得到，对于灵活顺序的序列处理，做好捕捉集体的顺序依赖关系，而不是逐点依赖关系。如何在柔性顺序的假设下捕获集体序列依赖成为SRS中处理柔性顺序序列的关键挑战。 ​ 目前的工作相对较少，在深度学习领域，CNN在其体现出不同区域的之间的局部和全局依赖关系，来应对这个工作。（之前小看了这篇CNN文章 等下看） ​ 题目：Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding ​ 模型名字： CASER 1.3 处理噪声数据​ 噪声的定义：在序列中，与交互预测产生干扰，在用户项交互序列中，一些历史交互是弱相关甚至不相关。（不晓得兴趣漂移算不算噪声） ​ 一篇自己看不懂的文章，但是完美的解决了这个问题 ​ 简单的说一下这个文章的意思吧，其通过分析一个序列中 非常重要的序列项和一些无关紧要的序列项，通过替换非常重要的 建立负样本。替换无关紧要的建议正样本。通过对比学习来实现这篇。（文章的具体内容还得看） 其难点与新颖点 序列项的分类 对比学习的使用 ​ 题目：CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation ​ 模型名：题目里面有 1.4 处理异构关系的用户交互​ 长期顺序依赖关系与短期顺序依赖关系大不相同，它们不能以相同的方式建模 ​ 混合模型是其解决问题的方式 1.5 具有层次结构的交互序列​ 在这里 我终于理解 会话对于推荐模型的意义 ​ 与用户项交互序列相关联的层次结构主要有两种： 元数据和用户项交互之间的层次结构。 子序列和用户项交互之间的层次结构。 ​ SRSs的另一个关键挑战是如何将嵌入这两种层次结构中的层次依赖性合并到顺序依赖学习中，以生成更准确的顺序推荐。 2 常用模型这里分为常用的和高级的 2.1 基础的 RNN 在这里主要写问题 任何相邻的交互必须相互依赖 只能捕获点相关 CNN 解决了RNN的问题 其卷积核的方式 能以相关的序列附近的关系 没有很强的顺序假设 无法捕获长期依赖 题目： A Simple Convolutional Generative Network for Next Item Recommendation 模型名字：NextitNet GNN 图点线结构 是最近工作的方向，相关工作目前较少 2.2 高级 attention 应用于浅层网络，处理带有噪声的交互序列 memory 内存网络，提高模型的表达能力 混合模型 解决短序列和长序列的组合 2.3 开放方向上下文感知 外界环境对于选择的影响 整合买卖交互（买卖之间的聊天） 跨域 不好理解 给个例子 在现实世界中，用户在特定时间段内购买的物品通常来自多个域，而不是一个域。本质上，来自不同领域的项目之间存在一些顺序依赖关系，例如在购买汽车后购买汽车保险。这种跨域顺序依赖在大多数SRS中被忽略"},{"title":"cin的沙雕理解","date":"2022-04-16T11:53:20.269Z","url":"/2022/04/16/cin%E7%9A%84%E6%B2%99%E9%9B%95%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"0. 前言感谢：     在主流的FM与DNN的结合方法中，这个明显是并行的结构 大家晓得xdeepFM 这个东西嘛 这个东西与deepFM 的区别是 根本没有什么相似性 不信 看看网络结构图 这个是deepFM 这个是大名鼎鼎的xdeepFM ​ 个人的瞎吉儿理解：在结构上，这两个玩意如果实在工程使用上 我肯定会选择deepFM。xdeepFM在网络结构上就存在这一些复杂性，在deepFM的基础上增加了linear层和cin这个不晓得是啥的玩意。在理解中 这个应该是我们xdeepFM 想要替换在deepFM 中 FM的部分 1. 模型的结构难点和自己的思考既然说这个模型我都看了两天 自然有其难点的地方 cin 的操作啊 还有其为什么强的地方 1.1 难以理解的cin讲明白一个东西首先先讲明输入输出 输入：field后的embedding层向量 输出：sum pooling后concat一个向量 1.2 介绍角色说明啊 m 是 filed的个数 D 为 embedd维度 Hk 为第k层特征向量的数量 Xk 为第k层的隐向量 冤大头 首先说明白冤大头的样子 embedd 1 embedd 2 ******** embedd m 这个东西会反复使用（的确是冤大头） 假如说 我有一个这样的隐藏层 我这边感觉第一个隐藏层肯定是随机生成的 其与冤大头 拥有一个相同长度的D边 与隐藏层进行外积操作 于是可以得到一个 长方形的过渡张量 其作用是求出下一个隐藏层 1.3 下一个隐藏层求法本人将用最简单的话来说明白下一个图的意思 欧克 图里面最直接就能看到一个躺着的张量 ，上面能看到，一个平铺的向量 其宽为D 长度为Hk+1 先从最基础的绿色的点开始 每一层都会得到一个绿色的点，张量的高为D 所以得到隐向量的宽为D 隐向量的长度为Hk+1,这个数据的设置 没有任何根据 属于随心所欲 绿色点的计算 会使用到一整个面的橙色的点 1.4 张量的生成我们获得了 隐向量 将其与 冤大头做外积 既可以获得下一层的过渡张量 然后 根据1.3的做法 生成对应的隐藏层 1.5 （1.3，1.4）循环噢噢噢噢 终于有k个隐藏层了呢 1.6 汇总 每个隐藏层为，对于第层，将所有的特征映射进行一个池化操作（sum pooling）【例如对上图Feature map 1向量进行一个累加】： 因此便得到一个池化向量对于第隐藏层。 最后在对于所有的隐藏层的池化向量进行一个拼接： 2. cin的性能 空间复杂度 主要的学习参数就是，经过上述分析，第层的共有个参数。假设包括CIN经过一个二元分类任务，那么CIN总共的学习参数为 时间复杂度 计算的时间复杂度为，那么对于层CIN的总时间复杂度为 优点 （1）交互是向量的交互，不是位（元素）级别（bit-wise）的交互； （2）高阶特征交互是显示的； （3）网络的复杂性不会随着交互程度的增加而呈指数增长； 3. 个人的理解对于xDeepFM模型，我们假设CIN的深度与特征映射的数量都为1，则「xDeepFM就相当于DeepFM的一个泛化」。当进一步删除DNN部分，并且对于特征映射使用一个sum filter，那xDeepFM就将为一个传统的FM模型。【联想之前的分解】 "},{"title":"小看的FM","date":"2022-04-13T07:04:51.808Z","url":"/2022/04/13/%E5%B0%8F%E7%9C%8B%E7%9A%84FM/","categories":[["undefined",""]],"content":"感谢：    -1 大前提这个是无奈之举 发现 能写的太多了 强调一下 one-hot 编码问题 首先展示： 里面有 像click的分类值 有 country,day,ad_type则是对应的特征。对于这种categorical特征，一般都是进行one-hot编码处理。 在代码中是这样处理的 画风一变！！ 因为是categorical特征，所以经过one-hot编码以后，不可避免的样本的数据就变得很稀疏。举个非常简单的例子，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。由此可见，数据的稀疏性，是我们在实际应用场景中面临的一个非常常见的挑战与问题。 one-hot编码带来的另一个问题是特征空间变大。同样以上面淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。 0. 前提啊啊啊~~ 咱这一周2022年4月13日这个时间呐 发现！！ FM 很重要 其理论的推理。现在看起来十分的重要 在工业界，这个模型 简直是yyds 其在CTR预估和推荐领域广泛使用 特征组合对于推荐排序是非常非常重要的，而FM这个思路已经很简洁优雅地体现了这个思想了（主要是二阶特征组合） 牛逼的点： 在embedding前夕 提出了类embedding的处理方式 FM对于每个特征，学习一个大小为k的一维向量，于是，两个特征 和 的特征组合的权重值，通过特征对应的向量 和 的内积 来表示。这本质上是在对特征进行embedding化表征，和目前非常常见的各种实体embedding本质思想是一脉相承的，但是很明显在FM这么做的年代（2010年），还没有现在能看到的各种眼花缭乱的embedding的形式与概念。所以FM作为特征embedding，可以看作当前深度学习里各种embedding方法的老前辈。当然，FM这种模式有它的前辈模型吗？有，等会会谈。其实，和目前的各种深度DNN排序模型比，它仅仅是少了2层或者3层MLP隐层，用来直接对多阶特征非线性组合建模而已，其它方面基本相同。 1. FM 前老弟（看起来就很复杂）这个式子吧 从此公式可以看出组合特征一共有n(n-1)&#x2F;2个，如果特征n上百个，组合特征上万个，就是任意两个wij相互独立，样本数据很稀疏，xixj为非零的项会非常的少，导致训练样本的不足，很容易导致参数 wij 不准确，最终将严重影响模型的性能和稳定性，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。 这个东西 在未来 将为一直成为推荐FM的垫脚石（敲砖引玉） 2. FM 老弟在 工业界 还在使用 大多工业推荐排序系统采取LR这种“线性模型+人工特征组合引入非线性”的模式。因为LR模型具有简单方便易解释容易上规模等诸多好处，所以目前仍然有不少实际系统仍然采取这种模式。但是，LR模型最大的缺陷就是人工特征工程，耗时费力费人力资源，那么能否将特征组合的能力体现在模型层面呢？ 使用了特征的隐向量 作为特征的相对应权重 上面的wij 使用 特征隐向量（辅助向量）的乘积 本人大白话讲一遍，啊啊啊 上面的式子太暴力，脑子思考一下就发现权重项太多，我在上面的这篇文献中发现这么一句：**对于任何正定实矩阵 只要k足够大，都存在k维向量组成的矩阵 使得 ** 在这个公式下 这个V啊 就是辅助向量。 这个k相对于n来说 可太小了 所以 就从目前来说 权重系数 降低到了 3. 时间复杂度的减小多图 警惕！ 一个相对复炸的式子 3.1 step1 3.2 step2 3.3 step3 第三步转换不是太直观，可能需要简单推导一下，很多人可能会卡在这一步，所以这里解释解释。 其实吧，如果把k维特征向量内积求和公式抽到最外边后，公式就转成了上图这个公式了（不考虑最外边k维求和过程的情况下）。它有两层循环，内循环其实就是指定某个特征的第f位（这个f是由最外层那个k指定的）后，和其它任意特征对应向量的第f位值相乘求和；而外循环则是遍历每个的第f位做循环求和。这样就完成了指定某个特征位f后的特征组合计算过程。最外层的k维循环则依此轮循第f位，于是就算完了步骤三的特征组合 3.4 step4对上一页公式图片展示过程用公式方式，再一次改写（参考上图），其实就是两次提取公共因子而已，这下应该明白了吧？要是还不明白，那您的诊断结果是数学公式帕金森晚期，跟我一个毛病，咱俩病友同病相怜，我也没辙了。 "},{"title":"Capsule 网络","date":"2022-04-10T14:19:13.807Z","url":"/2022/04/10/Capsule%20%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C/","categories":[["undefined",""]],"content":"0. 简单总结感谢：  为了改善CNN对旋转不具备不变性，学习不到3D空间信息和CNN只关注要检测的目标是否存在，而不关注这些组件之间的位置和相对的空间关系。 CNN 自己的处理 虽然max pooling在很多任务上提高了原始CNN的准确率，但是我们也可以看到max pooling丢失了很多有价值的信息，并没有很好地理解内容 Capsule尝试去解决这些问题 （优势） Capsule可以学习到物体之间的位置关系，例如它可以学习到眉毛下面是眼睛，鼻子下面是嘴唇，可以减轻前面的目标组件乱序问题 Capsule可以对3D空间的关系进行明确建模，capsule可以学习到上面和下面的图片是同一个类别，只是视图的角度不一样。Capsule可以更好地在神经网络的内部知识表达中建立层次关系。 在训练的时候 能使用相对较少的数据集，但相对来说 其训练时间较CNN变得更长 1. capsule的结构​ capsule 是向量，其可以理解为object的某个类别，其模长表示某个entity存在的概率，其方向表示某个entity属性 其计算的方式 为以下四步： 对输入向量做乘法，其中 和 分别来自与前面的 capsule 的输出，在单个 capsule 内部，对 和 分别乘上 和 得到了 新的 和 。 对输入向量进行标量加权，令与相乘，与相乘，其中和均为标量，且。 对得到向量求和，得到。 向量到向量的非线性化，将得到的结果向量 进行转换，即通过函数 得到结果 ，作为这个capsule 的输出，且这个结果 可以作为下一个 capsule 的输入 2. 细节（训练方式）动态寻路算法 鬼都看不懂下面这个 直观理解 其中两个高层胶囊的输出用紫色向量 表示，橙色向量表示接受自某个低层胶囊的输入，其他黑色向量表示接受其他低层胶囊的输入。左边的紫色输出 和橙色输入 指向相反的方向，所以它们并不相似，这意味着它们点积是负数，更新路由系数的时候将会减少。右边的紫色输出 和橙色输入 指向相同方向，它们是相似的，因此更新参数的时候路由系数 会增加。在所有高层胶囊及其所有输入上重复应用该过程，得到一个路由参数集合，达到来自低层胶囊的输出和高层胶囊输出的最佳匹配。 "},{"title":"transformer 详解！（写的像人话一点）","date":"2022-04-10T12:50:30.634Z","url":"/2022/04/10/%E5%A4%9A%E7%A7%8D%E5%A4%9A%E6%A0%B7%E7%9A%84attention/","categories":[["undefined",""]],"content":"0. 奠基大佬 Bahdanau Attention &amp; Luong Attention 1. 自注意力和多头 Self Attention &amp; Multi-head Attention 为什么自注意力呢？ 相对于 RNN，考虑长距离依赖，还要可以并行！ constant path length &amp; variable-sized perceptive field ：任意两个位置（特指远距离）的关联不再需要通过 Hierarchical perceptive field 的方式，它的 perceptive field 是整个句子，所以任意两个位置建立关联是常数时间内的。 parallelize : 没有了递归的限制，就像 CNN 一样可以在每一层内实现并行。 1.1 宏观角度看自注意力机制​ 随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。 ​ 如果你熟悉RNN（循环神经网络），回忆一下它是如何维持隐藏层的。RNN会将它已经处理过的前面的所有单词&#x2F;向量的表示与它正在处理的当前单词&#x2F;向量结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。 1.2 微观角度在NLP 中 ，k &#x3D; v 具体流程（本人的脑子思考）： 词向量 生成qkv qk计算注意力分数（scaled-dot product） 注意力分数softmax （alignment function） 通过注意力分数softmax 计算最后的值向量 求和 （context vector） 1.3 代码实践softmax 的 dim &#x3D; 1 2. 注意力简单解释 自己发现 softmax dim 范围其实是dim &#x3D; 1"},{"title":"attention机制 简单理解（废弃） 说实话看不太懂 太抽象","date":"2022-04-10T07:05:28.523Z","url":"/2022/04/10/attention%20%E6%9C%BA%E5%88%B6/","categories":[["undefined",""]],"content":"在反复回卷的attention中，本人感觉心里憔悴 一定得总结一个俺能看懂的文章 最近发现了这样的一篇 感谢：    阿里妹导读：曾被 paper 中各种各样的 Attention 搞得晕晕乎乎，尽管零零散散地整理过一些关于Attention 的笔记，重点和线索依然比较凌乱。今天，阿里巴巴工程师楠易，将 Attention 的知识系统性地梳理、回顾、总结，不求深刻，但求浅显，希望能帮助对 Attention 有疑惑的同学。 0. 什么是attention Attention（注意力）机制如果浅层的理解，跟他的名字非常匹配。他的核心逻辑就是「从关注全部到关注重点」 1. attention 分类涉及所有的 attention 都继承于这个抽象类。这里我写了两个抽象类，一个叫 alignment-based，一个叫 memroy-based。 1.1 alignment-based 模型 c 为 context y1 y2 —–yn 为输入 input 输出为z 1.2 拆分 attention model分为三部曲 score function ：度量环境变量与当前输入向量的相似性；在当前环境下，应该关注哪些信息 ​ alignment function：计算attention weight （权重） 通常使用softmax进行归一化 ​ generate context vector function : 根据 attention weight 得到输出向量 ​ 在整体视角下，就像下图这个样子： ​ 1.3 memory-based 模型 长得很像transformer​ 另一种视角是 QKV模型，假设输入为 q，Memory 中以（k，v）形式存储需要的上下文。感觉在 Q&amp;A 任务中，这种设置比较合理，transformer 是采用的这种建模方式。k 是 question，v 是 answer，q 是新来的 question，看看历史 memory 中 q 和哪个 k 更相似，然后依葫芦画瓢，根据相似 k 对应的 v，合成当前 question 的 answer 1.4 建模方式三步 address memory （score function）： 在memory找相似的东西 ​ normalize（alignment function） ： ​ read content（ gen context vector function ）： ​ 2. attention 细节在attention机制中，其建模方式主要就是以下的三类 按人话说 找相关 度量环境向量与当前输入向量的相似性；找到当前环境下，应该 focus 哪些输入信息（ score-function ） 算权重 计算 attention weight，通常都使用 softmax 进行归一化 （ alignment function ） 出结果 根据 attention weight 得到输出向量 （ generate context vector function ） 2.1 score function 的区别score function 在本质上是度量两个向量的相似度。找出相关的部分 两个向量在一个空间 使用 dot 点乘方式（或者 scaled dot product，scaled 背后的原因是为了减小数值，softmax 的梯度大一些，学得更快一些），简单好使。 不在同一个空间 需要一些变换（在一个空间也可以变换），additive 对输入分别进行线性变换后然后相加，multiplicative 是直接通过矩阵乘法来变换 2.2 alignment function 区别在 soft attention 中，又划分了 global&#x2F;local attention 。 global attention 是所有输入向量作为加权集合，使用 softmax 作为 alignment function，local 是部分输入向量才能进入这个池子。 local的目的 背后逻辑是要减小噪音，进一步缩小重点关注区域。 如何缩小关注区域 local-m 基于的假设生硬简单，就直接 pass了。local-p 有一个预估操作，预计当前时刻应该关注输入序列（总长度为S）的什么位置 pt（引入了两个参数向量，vp，wp），然后在 alignment function 中做了一点儿调整，在 softmax 算出来的attention wieght 的基础上，加了一个以 pt 为中心的高斯分布来调整 alignment 的结果。 在应用中 发现 从global&#x2F;local 视角的分类来看，更常用的依然还是 global attention，因为复杂化的local attention 带来的效果增益感觉并不大 2.3 generate context vector functionsoft&#x2F;hard attention 最直观的一种理解是，hard attention 是一个随机采样，采样集合是输入向量的集合，采样的概率分布是alignment function 产出的 attention weight。因此，hard attention 的输出是某一个特定的输入向量。soft attention 是一个带权求和的过程，求和集合是输入向量的集合，对应权重是 alignment function 产出的 attention weight。hard &#x2F; soft attention 中，soft attention 是更常用的（后文提及的所有 attention 都在这个范畴），因为它可导，可直接嵌入到模型中进行训练，hard attention 文中 suggests a Monte Carlo based sampling approximation of gradient。 "},{"title":"Amazon数据集的处理 生动（没写完）","date":"2022-04-08T01:38:52.891Z","url":"/2022/04/08/Amazon%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"  数据集： 一个数据集3g 有点大 这个数据集按 作者的说法是处理好了的 先于之前一篇文章的写法，这次打算按照之前criteo数据集的编写方法 在写一个 算是数据集的第二篇 一共三篇 0. 观察数据（有一点重要） 类别有 label user itemID cateID 重要的是 后面的 hist_item_list 和 hist_cate_list 这两个列表 可以看到这两个列表使用了 | 作为分隔符 对于后两个列表的处理 将是比较重要的特点 可以观察到的数值特征 只有标签label一个 1. 读取数据"},{"title":"各种慢","date":"2022-04-06T13:34:45.459Z","url":"/2022/04/06/%E5%90%84%E7%A7%8D%E6%85%A2/","categories":[["undefined",""]],"content":"一个常用的各种慢的解决复制帖 pip慢 -i  conda 慢添加清华镜像源，代码如下所示： "},{"title":"criteo数据集的处理 生动","date":"2022-04-06T12:25:47.026Z","url":"/2022/04/06/criteo%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"之前 写过一篇 太烂了 在重新写一篇生动的 数据预处理与特征工程： 目前项目涉及三个数据集（criteo，amazon，movielen） 其中 criteo数据集肯定是使用的最多的啦 这里 对于数据的梳理 （对于数据的处理 我觉得也较通用） 特地 整理一个文档 0. 观察数据（其实很重要） 数据 有0 1 2 这样的离散数据 和 一些68fd1e64 80e26c9b fb936136 7b4723c4 不晓得是啥的数据 感觉能确定的是 第一列的数据 是标签数据 欧克 总结数据如下 有标签 离散 和 不晓得是啥的数据（先确定为连续数据吧） 按照习惯 其target一般特征后面 1. 读取数据 1.1 标准数据查看三件套 1.2 数据质量分析 查看缺失值 异常值分析 1.3 数据分类 2. 缺失值处理针对缺失值 的 离散 和连续缺失的值 进行缺失值处理 （缺失一般是异常值） 一般是四步操作 查看变量类别 查看变量缺失值情况 对于缺失值进行标注 查看标注完缺失值情况 3. sklearn.preprocessing数据预处理这里搬运一下 分箱的目的 离散变量便于特征的增加和减少，便于模型快速迭代 稀疏向量内积乘法更快，计算结果便于存储，容易扩展 离散化后的特征对异常数据有很强的鲁棒性，例如，连续异常值5000可能对模型影响很大，但如果分箱后，模型影响很小 为模型引入非线性，提升模型表达能力，加大拟合 模型更加稳定，不会因为各别数据增加而影响模型精度 简化模型，防止模型过拟合 3.1 处理连续型特征在处理连续型特征 有两个方法 二值化 与 分段 二值化 二值化使用的类是 sklearn.preprocessing.Binarizer 根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射到1。二值化是对文本计数数据的常见操作，分析人员可以决定仅考虑某种现象的存在与否。 分段 分段使用的类 preprocessing.KBinsDiscretizer，相对麻烦点，给出下面的参数列表 3.2 处理分类型特征在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，除了专用来处理文字的算法，其他算法在fit的时候全部要求输入数组或矩阵，也不能够导入文字型数据（其实手写决策树和普斯贝叶斯可以处理文字，但是sklearn中规定必须导入数值型）。 然而在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是[“小学”，“初中”，“高中”，“大学”]，付费方式可能包含[“支付宝”，“现金”，“微信”]等等。在这种情况下，为了让数据适应算法和库，我们必须将数据进行编码，即是说，将文字型数据转换为数值型 文字型数据 转换成数字 处理后的效果 [][qzetZn.md.png] 4. 划分标签和特征 5. 划分训练，验证，测试集 5.1 数据划分"},{"title":"从din到dien 推荐模型（文献部分）","date":"2022-04-04T06:17:53.421Z","url":"/2022/04/04/%E4%BB%8Edin%E5%88%B0dien%20%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%EF%BC%88%E6%96%87%E7%8C%AE%E9%83%A8%E5%88%86%EF%BC%89/","categories":[["undefined",""]],"content":"感谢：     ​ 最近比较迷茫，看了看自己的论文阅读表，这两篇阿里巴巴的文献映入眼帘，作为工业界的老大。其思维比较新颖。且较当时比较的模型有较大百分比的提升。CTR预测， 所谓的CTR，就是点击率Click Through Rate，而CTR预估算法在广告系统中起着至关重要的作用。从这两篇文章中可以看到阿里妈妈在深度学习方向上的探索，也可以窥视到阿里妈妈的CTR算法的发展脉络。 0. 一切从特征选取开始 主要有4个特征组，如下图所示， 1）用户画像特征， 2）用户行为特征，即用户点击过的商品 3）待曝光的广告，广告其实也是商品，后文中我们统称为candidate 4）上下文特征。 1. 模型的鼻祖 （embedding + mlp） 如果说有10个商品的embedding，embedding的维度为16。在业界，pooling的处理方式就两个。 sum pooling 对应的维度数字求和 mean pooling 对应的维度数字求平均 1.1 问题​ 在电商这个场景中，通常用户的兴趣具有多样性，可能在一段时间内点击过衣服，电子产品，鞋子等。而对于不同的candidate来说，浏览过的相关商品对于预测帮助更大，不相关的商品对于ctr预估可能并不起作用，例如用户看过的衣服，鞋子对于iphone的预测并没有帮助 2. DIN​ 为了解决上述的问题，既然提到了相关，那肯定得考虑到&#x3D;&#x3D;注意力机制&#x3D;&#x3D;了啦 ​ 解决思路是： 在pooling的时候，与candidate相关的商品权重大一些，与candidate不相关的商品权重小一些，这是一种Attention的思想。将candidate与点击序列中的每个商品发生交互来计算attention分数。具体计算方法如图3中右上角的小网络所示，输入包括商品和candidate的embedding向量，以及两者的外积。对于不同的candidate，得到的用户表示向量也不同，具有更大的灵活性。 ​ 模型基础上 提出了Activation Unit单元 来提取商品与目标广告之间的相关性。 其中activation unit的输入包括两个部分，一个是原始的用户行为embedding向量、广告embedding向量；另外一个是两者Embedding向量经过外积计算后得到的向量，文章指出这种方式有利于relevance modeling。 ​ 2.1 attention归一化处理​ 一般来说，做attention的时候，需要对所有的分数通过softmax做归一化，这样做有两个好处，一是保证权重非负，二是保证权重之和为1。但是在DIN的论文中强调，不对点击序列的attention分数做归一化，直接将分数与对应商品的embedding向量做加权和，目的在于保留用户的兴趣强度。例如，用户的点击序列中90%是衣服，10%是电子产品，有一件T恤和一部手机需要预测CTR，那么T恤会激活大部分的用户行为，使得根据T恤计算出来的用户行为向量在数值上更大，相对手机而言。 2.2 DIN的创新点​ DIN的论文中还提出了两个小的改进点。一个是对L2正则化的改进，在进行SGD优化的时候，每个mini-batch都只会输入部分训练数据，反向传播只针对部分非零特征参数进行训练，添加上L2之后，需要对整个网络的参数包括所有特征的embedding向量进行训练，这个计算量非常大且不可接受。论文中提出，在每个mini-batch中只对该batch的特征embedding参数进行L2正则化。第二个是提出了一个激活函数Dice。对于Relu或者PRelu来说，rectified point(梯度发生变化的点)都在0值，Dice对每个特征以mini-batch为单位计算均值和方差，然后将rectified point调整到均值位置。 3. DIENDIN在捕捉连续行为之间的依赖关系很弱，行为-》利益，其中隐性利益很难通过行为充分反映 DIEN，全称是Deep Interest Evolution Network，即用户兴趣进化网络。这个算法中用两层架构来抽取和使用用户兴趣特征： 兴趣抽取层Interest Extractor Layer: 从用户行为序列中提取信息 兴趣进化层Interest Evolving Layer: 从用户行为序列中找到目标相关的兴趣，对其进行建模 3.1 兴趣抽取层（这里使用的是GRU）​ 在广告与商品之间并不是单纯的商品对应的关系（即我喜欢的是这个） ​ 兴趣抽取层Interest Extractor Layer的主要目标是从embedding数据中提取出interest。但一个用户在某一时间的interest不仅与当前的behavior有关，也与之前的behavior相关，所以作者们使用GRU单元来提取interest。GRU单元的表达式如下。 其中，σ是sigmoid操作，而◦是内积操作。 作者的辅助测试 ​ 作者设计了一个二分类模型来计算兴趣抽取的准确性，我们将用户下一时刻真实的行为e(t+1)作为正例，负采样得到的行为作为负例e(t+1)’，分别与抽取出的兴趣h(t)结合输入到设计的辅助网络中，得到预测结果，并通过logloss计算一个辅助的损失 3.2 兴趣进化层 （带有注意力的GRU） interest在变化过程中遵循如下规律：1）interest drift：用户在某一段时间的interest会有一定的集中性。比如用户可能在一段时间内不断买书，在另一段时间内不断买衣服。2）interest individual：一种interest有自己的发展趋势，不同种类的interest之间很少相互影响，例如买书和买衣服的interest基本互不相关。 3.2.1 attention机制兴趣抽取层获得的隐向量，ht 与 被embedding后的广告向量 产生内积后softmax取相关参数 3.2.2 attention方式 GRU with attentional input (AIGRU) 这种方式将attention直接作用于输入，无需修改GRU的结构： ​ Attention based GRU(AGRU) 这种方式需要修改GRU的结构，此时hidden state的输出变为： ​ GRU with attentional update gate (AUGRU) 这种方式需要修改GRU的结构，此时hidden state的输出变为: ​ "},{"title":"RNN模型","date":"2022-04-03T13:18:49.397Z","url":"/2022/04/03/RNN%E7%B1%BB%E6%A8%A1%E5%9E%8B/","categories":[["undefined",""]],"content":"感谢： 0. RNN 先简单介绍一下一般的RNN。 这里： !() 为当前状态下数据的输入， 表示接收到的上一个节点的输入。 为当前节点状态下的输出，而 为传递到下一个节点的输出。 通过上图的公式可以看到，输出 h’ 与 x 和 h 的值都相关。 而 y 则常常使用 h’ 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。 对这里的y如何通过 h’ 计算得到往往看具体模型的使用方式。 通过序列形式的输入，我们能够得到如下形式的RNN。 1. LSTM 长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。 相比RNN只有一个传递状态 ，LSTM有两个传输状态，一个 （cell state），和一个 （hidden state）。（Tips：RNN中的 对于LSTM中的 ） 其中对于传递下去的 改变得很慢，通常输出的 是上一个状态传过来的 加上一些数值。 而 则在不同节点下往往会有很大的区别。 1.1 LSTM 结构深入下面具体对LSTM的内部结构来进行剖析。 首先使用LSTM的当前输入 和上一个状态传递下来的 拼接训练得到四个状态。 其中， ， ， 是由拼接向量乘以权重矩阵之后，再通过一个 激活函数转换成0到1之间的数值，来作为一种门控状态。而 则是将结果通过一个 激活函数将转换成-1到1之间的值（这里使用 是因为这里是将其做为输入数据，而不是门控信号） 下面开始进一步介绍这四个状态在LSTM内部的使用。（敲黑板） 其经典图片是这样的 1.2 主要的阶段LSTM内部主要有三个阶段： 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。 具体来说是通过计算得到的 （f表示forget）来作为忘记门控，来控制上一个状态的 哪些需要留哪些需要忘。 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 表示。而选择的门控信号则是由 （i代表information）来进行控制。 将上面两步得到的结果相加，即可得到传输给下一个状态的 。也就是上图中的第一个公式。 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 来进行控制的。并且还对上一阶段得到的 进行了放缩（通过一个tanh激活函数进行变化）。 与普通RNN类似，输出 往往最终也是通过 变化得到。 1.3 总结以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。 但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。 2. GRU GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的 。 其性能与LSTM相当 ，但是 其计算的消耗较前者少。 2.1 输入输出GRU的输入输出结构与普通的RNN是一样的。 有一个当前的输入 ，和上一个节点传递下来的隐状态（hidden state） ，这个隐状态包含了之前节点的相关信息。 结合 和 ，GRU会得到当前隐藏节点的输出 和传递给下一个节点的隐状态 。 2.2 内部结构 通过上一个传输下来的状态 和当前节点的输入 来获取两个门控状态。 其中 控制重置的门控（reset gate）， 为控制更新的门控（update gate）。 得到门控信号之后，首先使用重置门控来得到“重置”之后的数据 ，再将 与输入 进行拼接，再通过一个tanh激活函数来将数据放缩到**-1~1**的范围内。即得到如下图2-3所示的 。 这里的 主要是包含了当前输入的 数据。有针对性地对 添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“。 最后介绍GRU最关键的一个步骤，我们可以称之为”更新记忆“阶段。 在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控 （update gate）。 更新表达式： 首先再次强调一下，门控信号（这里的 ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。 有读者发现在pytorch里面的GRU[链接]写法相比原版对 多了一个映射，相当于一个GRU变体，猜测是多加多这个映射能让整体实验效果提升较大。如果有了解的同学欢迎评论指出。 GRU很聪明的一点就在于，我们使用了同一个门控 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。 ：表示对原本隐藏状态的选择性“遗忘”。这里的 可以想象成遗忘门（forget gate），忘记 维度中一些不重要的信息。 ： 表示对包含当前节点信息的 进行选择性”记忆“。与上面类似，这里的 同理会忘记 维度中的一些不重要的信息。或者，这里我们更应当看做是对 维度中的某些信息进行选择。 ：结合上述，这一步的操作就是忘记传递下来的 中的某些维度信息，并加入当前节点输入的某些维度信息。 可以看到，这里的遗忘 和选择 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （ ），我们就会使用包含当前输入的 中所对应的权重进行弥补 。以保持一种”恒定“状态。 2.3 总结GRU输入输出的结构与普通的RNN相似，其中的内部思想与LSTM相似。 与LSTM相比，GRU内部少了一个”门控“，参数比LSTM少，但是却也能够达到与LSTM相当的功能。考虑到硬件的计算能力和时间成本，因而很多时候我们也就会选择更加”实用“的GRU啦。"},{"title":"Recommender-System-Pytorch 项目 网络参数指南","date":"2022-03-28T12:43:29.890Z","url":"/2022/03/28/Recommender-System-Pytorch%20%E9%A1%B9%E7%9B%AE%20%E7%BD%91%E7%BB%9C%E5%87%BD%E6%95%B0/","categories":[["undefined",""]],"content":"个人觉得成熟的rebole的工具 不太适合萌新 来操作自己对于项目的建设 最近找了一个新的项目 来操作 感觉本项目 更贴近萌新到大佬写代码过程 于是乎 有了这篇指南 embedding操作0. 官方操作下面是官方例子 官方的解释： torch.nn.``Embedding(num_embeddings, embedding_dim, padding_idx&#x3D;None, max_norm&#x3D;None, norm_type&#x3D;2.0, scale_grad_by_freq&#x3D;False, sparse&#x3D;False, _weight&#x3D;None, device&#x3D;None, dtype&#x3D;None) num_embeddings：嵌入字典的大小（词的个数）； embedding_dim：每个嵌入向量的大小； padding_idx：若给定，则每遇到 padding_idx 时，位于 padding_idx 的嵌入向量（即 -padding_idx 映射所对应的向量）为0； max_norm：若给定，则每个大于 max_norm 的数都会被规范化为 max_norm； norm_type：为 max_norm 计算 p-范数的 p值； scale_grad_by_freq：若给定，则将按照 mini-batch 中 words 频率的倒数 scale gradients； sparse：若为 True，则 weight 矩阵将是稀疏张量。 1. 自己的瞎吉儿理解这里呀 就只需要理解好 前三个就好 对于前两个的理解 torch.nn.Embedding 的权重为 num_embeddings * embedding_dim 的矩阵，例如输入10个词，每个词用3为向量表示，则权重为10*3的矩阵； 对于 padding_idx 理解 可以看出 “6” 所对应映射的向量被填充了0。 网络初始化 1. 初始化函数 均匀分布torch.nn.init.uniform_(tensor, a&#x3D;0, b&#x3D;1)服从~U ( a , b ) U(a, b)U(a,b) 正太分布torch.nn.init.normal_(tensor, mean&#x3D;0, std&#x3D;1)服从~N ( m e a n , s t d ) N(mean, std)N(mean,std) 初始化为常数torch.nn.init.constant_(tensor, val)初始化整个矩阵为常数val Xavier 基本思想是通过网络层时，输入和输出的方差相同，包括前向传播和后向传播。具体看以下博文： 为什么需要Xavier 初始化？如果初始化值很小，那么随着层数的传递，方差就会趋于0，此时输入值 也变得越来越小，在sigmoid上就是在0附近，接近于线性，失去了非线性如果初始值很大，那么随着层数的传递，方差会迅速增加，此时输入值变得很大，而sigmoid在大输入值写倒数趋近于0，反向传播时会遇到梯度消失的问题 xavier初始化的简单推导 kaiming (He initialization) 以后再说 现在没用上 bug解决感谢涛哥 对于源码的修改 错误信息 在改错的时候注意看最后一行 即 修改在这里定位好 然后修改成： "},{"title":"推荐数据集处理（分桶） 重要（比较水）","date":"2022-03-28T03:06:43.179Z","url":"/2022/03/28/%E6%8E%A8%E8%8D%90%E6%95%B0%E6%8D%AE%E9%9B%86%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","categories":[["undefined",""]],"content":"1. 探索数据集1.1 数据读入 1.2 查看数据 1.3 数据分类 1.4 查看缺失值情况（标注） 1.5 数据分箱 1.6 训练数据与标签分离 2. 字段维度（field_dim） 与 数据划分 2.1 字段维度获取（field_dim） 2.2 数据划分 "},{"title":"软著申请流程","date":"2022-03-27T02:32:53.849Z","url":"/2022/03/27/%E8%BD%AF%E8%91%97%E7%94%B3%E8%AF%B7/","categories":[["undefined",""]],"content":"感谢   "},{"title":"python中axis=0和axis=1的理解","date":"2022-03-25T01:45:41.345Z","url":"/2022/03/25/axis%E7%90%86%E8%A7%A3/","categories":[["undefined",""]],"content":"原文链接： axis的重点在于方向，而不是行和列。1表示横轴，方向从左到右；0表示纵轴，方向从上到下。 即axis&#x3D;1为横向，axis&#x3D;0为纵向，而不是行和列，具体到各种用法而言也是如此。当axis&#x3D;1时，如果是求平均，那么是从左到右横向求平均；如果是拼接，那么也是左右横向拼接；如果是drop，那么也是横向发生变化，体现为列的减少 axis &#x3D; 0 纵向处理 axis &#x3D; 1 横向处理 "},{"title":"信用卡交易数据解读与探索（数据合并）","date":"2022-03-21T07:01:44.023Z","url":"/2022/03/21/%E4%BF%A1%E7%94%A8%E5%8D%A1%E4%BA%A4%E6%98%93%E6%95%B0%E6%8D%AE%E8%A7%A3%E8%AF%BB/","categories":[["undefined",""]],"content":"数据分析首先还是对数据集进行解释，以及简单验证数据集的正确性。信用卡交易记录包括了两个数据集，分别是historical_transactions和new_merchant_transactions。两个数据集字段类似，只是记录了不同时间区间的信用卡消费情况： 这里的数据存在两个 一个18以前的数据集 一个18以后的 数据解读 首先简单查看有哪些字段一致： 并且我们进一步发现，交易记录中的merhcant_id信息并不唯一： 造成该现象的原因可能是商铺在逐渐经营过程动态变化，而基于此，在后续的建模过程中，我们将优先使用交易记录中表中的相应记录。 数据预处理 连续&#x2F;离散字段标注 首先也是一样，需要对其连续&#x2F;离散变量进行标注。当然该数据集中比较特殊的一点，是存在一个时间列，我们将其单独归为一类： 字段类型转换&#x2F;缺失值填补 "},{"title":"商户数据解读与探索(包含较为复杂的数据处理)","date":"2022-03-21T01:54:47.085Z","url":"/2022/03/21/%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E6%B8%85%E6%B4%97/","categories":[["undefined",""]],"content":"复杂的数据处理过程（含清洗）1. 数据解读 2. 数据探索 正确性检验 查看id出现次数是否唯一 缺失值分析 能够发现，第二个匿名分类变量存在较多缺失值，而avg_sales_lag3&#x2F;6&#x2F;12缺失值数量一致，则很有可能是存在13个商户同时确实了这三方面信息。其他数据没有缺失，数据整体来看较为完整。 3. 数据预处理3.1 离散&#x2F;连续字段标注由于商户数据集中特征同时存在分类变量和离散变量，因此我们首先可以根据字段的说明对不同属性特征进行统一的划分： 3.2 离散数据处理 离散变量数据情况 离散变量字典编码 接下来对离散变量进行字典编码，即将object对象类型按照sort顺序进行数值化（整数）编码。例如原始category_1取值为Y&#x2F;N，通过sort排序后N在Y之前，因此在重新编码时N取值会重编码为0、Y取值会重编码为1。以此类推。 需要注意的是，从严格角度来说，变量类型应该是有三类，分别是连续性变量、名义型变量以及有序变量。连续变量较好理解，所谓名义变量，指的是没有数值大小意义的分类变量，例如用1表示女、0表示男，0、1只是作为性别的指代，而没有1&gt;0的含义。而所有有序变量，其也是离散型变量，但却有数值大小含义，如上述most_recent_purchases_range字段，销售等级中A&gt;B&gt;C&gt;D&gt;E，该离散变量的5个取值水平是有严格大小意义的，该变量就被称为有序变量。 在实际建模过程中，如果不需要提取有序变量的数值大小信息的话，可以考虑将其和名义变量一样进行独热编码。但本阶段初级预处理时暂时不考虑这些问题，先统一将object类型转化为数值型。&#x3D;&#x3D;（object类型转换类型）&#x3D;&#x3D; 测试 3.3 连续变量数据探索 据此我们发现连续型变量中存在部分缺失值，并且部分连续变量还存在无穷值inf，需要对其进行简单处理。 无穷值处理 缺失值处理 不同于无穷值的处理，缺失值处理方法有很多。但该数据集缺失数据较少，33万条数据中只有13条连续特征缺失值，此处我们先简单采用均值进行填补处理，后续若有需要再进行优化处理。 "},{"title":"kaggle入门","date":"2022-03-20T11:02:48.709Z","url":"/2022/03/20/kaggle%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"环境安装和准备anaconda + jupyter 获取kaggle.json &amp;emsp;&amp;emsp;在安装完成kaggle之后，进入Kaggle的个人主页（点击右上角头像），点击Create New API Token，则可创建一个kaggle.json文件，并自动开始下载 ​ - 将kaggle.json文件移动到.kaggle文件夹内 安装内核使用anaconda虚拟环境作为jupyter notebook内核 删除内核 感谢： "},{"title":"深度推荐系统 下","date":"2022-03-19T07:31:56.636Z","url":"/2022/03/19/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(2)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（2）2.4 FM与深度学习模型2.4.1 FNN - 用FM的隐向量完成embedding层初始化 FNN相较于deep crossing模型的区别 对于embedding进行了改进，在模型初始化时引入了有价值的先验信息 在训练时特征被划分了不同的特征域，每个特征域有对应的embedding层 2.4.2 deepFM（啊哈哈 大怨种来啦）究竟是怎样的大佬 看得懂我下面这句话 反正我是看不懂的 强调 红色为 权重1连接 就是是啥就是啥 用FM代替wide部分 FM部分 这个FM layer 在图上面 直观的看啊 有 一个+ 和 好多个 * 这个+ 是 FM层 的线性部分 这个*呢 就是 FM层的 特征组合部分（这里应该是 vi 点积 vj） 其输出公式为 deep 部分 这个deep部分啊 就是多层感知机嘛 没啥难的 ​ ​ FM与深度模型的组合有两种，一种是二者并行，另一种是二者串行。DeepFM就是并行的一种结构。并行就是FM将输入部分计算完之后单独拿出来，得到一组特征表示，然后再利用深度模型（多层全连接）对输入部分进行高阶的特征组合。最后把二者的特征进行concact，得到一组特征，最后对这组特征进行分类或者回归。其实这只是特征的一种组合方式，目的就是为了得到特征的高阶表示。 输出公式 细节（权重共用） 下面解释好好的看 这里的第二点如何理解呢，假设我们的k&#x3D;5，首先，对于输入的一条记录，同一个field 只有一个位置是1，那么在由输入得到dense vector的过程中，输入层只有一个神经元起作用，得到的dense vector其实就是输入层到embedding层该神经元相连的五条线的权重，即vi1，vi2，vi3，vi4，vi5。这五个值组合起来就是我们在FM中所提到的Vi。在FM部分和DNN部分，这一块是共享权重的，对同一个特征来说，得到的Vi是相同的。 2.4.3 总结特征工程在这条路上已经穷尽了可能性的尝试，模型的提升空间会非常小。但是很重要 2.5 注意力机制的应用 Attention机制的本质 attention机制的本质是从人类视觉注意力机制中获得灵感(可以说很‘以人为本’了)。大致是我们视觉在感知东西的时候，一般不会是一个场景从到头看到尾每次全部都看，而往往是根据需求观察注意特定的一部分。而且当我们发现一个场景经常在某部分出现自己想观察的东西时，我们就会进行学习在将来再出现类似场景时把注意力放到该部分上。这可以说就是注意力机制的本质内容了。至于它本身包含的‘自上而下’和‘自下而上’方式就不在过多的讨论。 Attention机制的理解 Attention机制其实就是一系列注意力分配系数，也就是一系列权重参数罢了。 2.5.1 AFM - 引入注意力机制的FM 注意力网络的作用是为每一个交叉特征提供权重 2.5.2 DIN basemodel DIN通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给定的广告）。 注意力在其上面的形式是激活单元来生成注意力得分"},{"title":"训练集和数据集数据探索","date":"2022-03-18T13:03:49.043Z","url":"/2022/03/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","categories":[["undefined",""]],"content":"数据分析 1. 数据解读 上面的操作就很easy 2. 数据质量分析​ 接下来简单数据探索。在实际建模过程中，首先我们会先校验数据的正确性，并检验缺失值、异常值等情况。 数据正确性校验 所谓数据正确性，指的是数据本身是否符合基本逻辑，例如此处信用卡id作为建模分析对象独一无二的标识，我们需要验证其是否确实独一无二，并且训练集和测试集信用卡id无重复。 判断缺失值情况 3. 异常值分析 describe()方法 异常值检验。由于我们尚未对数据集特征进行预处理，因此我们先查看标签列的异常值情况。首先我们可以用describe()方法查看这一列的基本统计信息： 通过直方图观察 由于是连续变量可以借助概率密度直方图进行分布的观察： $3\\delta$原则进行异常值识别 ​ 能够发现，大部分用户忠诚度评分都集中在[-10,10]之间，并且基本符合正态分布，唯一需要注意的是有个别异常值取值在-30以下，该数据在后续分析中需要额外注意。我们可以简单查看有多少用户的标签数值是小于30的： 当然，对于连续变量，一般可以采用$3\\delta$原则进行异常值识别，此处我们也可以简单计算下异常值范围： &amp;emsp;&amp;emsp;需要注意的是，此处我们是围绕标签进行的异常值检测，而本案例中标签并不是自然数值测量或统计的结果（如消费金额、身高体重等），而是通过某种公式人工计算得出（详见赛题分析）。出现如此离群点极有可能是某类特殊用户的标记。因此不宜进行异常值处理，而应该将其单独视作特殊的一类，在后续建模分析时候单独对此类用户进行特征提取与建模分析。 4. 规律一致性分析&amp;emsp;&amp;emsp;接下来，进行训练集和测试集的规律一致性分析。 &amp;emsp;&amp;emsp;所谓规律一致性，指的是需要对训练集和测试集特征数据的分布进行简单比对，以“确定”两组数据是否诞生于同一个总体，即两组数据是否都遵循着背后总体的规律，即两组数据是否存在着规律一致性。 &amp;emsp;&amp;emsp;我们知道，尽管机器学习并不强调样本-总体的概念，但在训练集上挖掘到的规律要在测试集上起到预测效果，就必须要求这两部分数据受到相同规律的影响。一般来说，对于标签未知的测试集，我们可以通过特征的分布规律来判断两组数据是否取自同一总体 单变量分析 当然，我们需要同时对比训练集和测试集的四个特征，可以通过如下代码实现： 多级联合分布 ​ 接下来，我们进一步查看联合变量分布。所谓联合概率分布，指的是将离散变量两两组合，然后查看这个新变量的相对占比分布。例如特征1有0&#x2F;1两个取值水平，特征2有A&#x2F;B两个取值水平，则联合分布中就将存在0A、0B、1A、1B四种不同取值水平，然后进一步查看这四种不同取值水平出现的分布情况。 ​ 实际建模过程中，规律一致性分析是非常重要但又经常容易被忽视的一个环节。通过规律一致性分析，我们可以得出非常多的可用于后续指导后续建模的关键性意见。通常我们可以根据规律一致性分析得出以下基本结论 ​ 作用： 如果分布非常一致，则说明所有特征均取自同一整体，训练集和测试集规律拥有较高一致性，模型效果上限较高，建模过程中应该更加依靠特征工程方法和模型建模技巧提高最终预测效果 如果分布不太一致，则说明训练集和测试集规律不太一致，此时模型预测效果上限会受此影响而被限制，并且模型大概率容易过拟合，在实际建模过程中可以多考虑使用交叉验证等方式防止过拟合，并且需要注重除了通用特征工程和建模方法外的trick的使用； "},{"title":"深度推荐系统 上","date":"2022-03-15T13:20:16.514Z","url":"/2022/03/15/%E6%B7%B1%E5%BA%A6%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(1)/","categories":[["undefined",""]],"content":"1. 深度学习推荐模型（1）演化方式： 改变神经网络的复杂程度 改变特征交叉方式 wide&amp;deep模型 FM深度版本 注意力机制与推荐系统结合 序列模型与推荐模型结合 强化学习与深度学习结合 2.1 神经网络复杂程度2.1.1 Auto-rec–单层神经网络推荐模型（easy） 通过自编码器原理，还原输入的结果。 重建函数： 目标函数： 目标函数l2正则： 参考： 2.1.2 Deep Crossing模型–经典深度学习架构 应用场景 网络结构 embedding层，stacking层，multiple residual units层，scoring层 ​ 反思 embedding+多层神经网络，相较于传统的二阶特征交叉能力，deep crossing拥有深度交叉的能力 2.1.2 NeuralCF - CF与深度学习的结合先回忆一下传统的矩阵分解怎么做 物品-用户共现矩阵分解成用户向量和物品向量 向量embedding化 embedding后向量取内积（重要） 得到分数 这个模型 复杂的位置就是在第三步操作上 使用多层神经网络去替换这个卷积操作 &#x3D;&#x3D;优势&#x3D;&#x3D; 利用神经网络来拟合任意函数，灵活地组成不同的特征，按需增加或减少模型的复杂度 &#x3D;&#x3D;劣势&#x3D;&#x3D; 基于协同过滤构造,没有引入更多其他类型的特征 在实践中，防止过拟合的风险 2.2 加强特征交叉能力2.2.1 PNN模型感谢：  ​ ​ 相较于 deep crossing模型中的stacking层，PNN模型替换成了乘积层。其他的在模型的输入，embeding层，多层神经网络以及最终的输出层上没有结构上的不同。 ​ product layer层，左边为线性部分，认为 特征之间的关系是and“且”的一种关系，而非add”加”的关系。 ​ z&#x3D;concat([emb1,emb2..,embn],axis&#x3D;1) 其右边操作为乘积操作，有内积和外积的区别。外积在操作上会将问题的复杂度从原来的m到 $m^2$,在选择上更应该慎重。 优势 ​ 定义了外积和内积操作更有针对性地强调不同特征之间的交互 局限 ​ 在外积操作上，为了效率经行大量的简化操作，对所有特征进行无差别的交叉，在一定程度上忽略了原始特征中包含的价值信息。 2.2.2 product layer 内积 PNN中p的计算方式如下，即使用内积来代表pij： 2.2.3 product layer 外积 OPNN中p的计算方式如下： 此时pij为MM的矩阵，计算一个pij的时间复杂度为MM，而p是NNMM的矩阵，因此计算p的事件复杂度为NNMM。从而计算lp的时间复杂度变为D1 * NNM*M。这个显然代价很高的。为了减少负责度，论文使用了叠加的思想，它重新定义了p矩阵： 2.3 记忆能力与泛化能力的综合2.3.1 wide&amp;deep模型​ wide部分是让模型具有较强的“记忆能力”，deep部分是让模型具有泛化能力。这样的结构使模型兼具了逻辑回归和深度神经网络的优点–能快速处理并且记忆大量的历史行为特征，并且具有强大的表达能力。 在提出W&amp;D模型，平衡Wide模型和Deep模型的记忆能力和泛化能力。实际上是lr+dnn。记忆（memorization） 通过特征叉乘对原始特征做非线性变换，输入为高维度的稀疏向量。通过大量的特征叉乘产生特征相互作用的“记忆（Memorization）”，高效且可解释，但要泛化需要更多的特征工程。 泛化（generalization）只需要少量的特征工程，深度神经网络通过embedding的方法，使用低维稠密特征输入，可以更好地泛化训练样本中未出现过的特征组合。但当user-item交互矩阵稀疏且高阶时，容易出现“过泛化（over-generalize）”导致推荐的item相关性差 工程应用 2.3.2 wide&amp;deep进化 deep&amp;cross模型 Cross Network ​ 交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式： ​ xl和xl+1 分别是第l层和第l+1层cross layer的输出，wl和bl是这两层之间的连接参数。注意上式中所有的变量均是列向量，W也是列向量，并不是矩阵。xl+1 &#x3D; f(xl, wl, bl) + xl. 每一层的输出，都是上一层的输出加上feature crossing f。而f就是在拟合该层输出和上一层输出的残差。 ​ Cross Layer 设计的巧妙之处全部体现在上面的计算公式中，我们先看一些明显的细节：1) 每层的神经元个数都相同，都等于输入 的维度 DCN能够有效地捕获有限度的有效特征的相互作用，学会高度非线性的相互作用，不需要人工特征工程或遍历搜索，并具有较低的计算成本。1）提出了一种新的交叉网络，在每个层上明确地应用特征交叉，有效地学习有界度的预测交叉特征，并且不需要手工特征工程或穷举搜索。2）跨网络简单而有效。通过设计，各层的多项式级数最高，并由层深度决定。网络由所有的交叉项组成，它们的系数各不相同。3）跨网络内存高效，易于实现。4）实验结果表明，交叉网络（DCN）在LogLoss上与DNN相比少了近一个量级的参数量"},{"title":"传统推荐系统","date":"2022-03-14T13:48:48.181Z","url":"/2022/03/14/%E4%BC%A0%E7%BB%9F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","categories":[["undefined",""]],"content":"1. 推荐系统技术架构数据+模型 1.1 数据部分通过特征工程，将客户端或服务端采集到的数据进行特征处理 1.2 模型部分主题 一般由召回层，排序层，补充数据与算法层组成 2. 传统推荐模型（粗略整理）2.1 协同过滤分为 用户过滤和物品过滤 2.1.1 用户过滤（没人用）公式一 余弦相似度 公式二 皮尔逊相关系数（减少了用户评分的影响） 2.1.2 物品过滤 基于历史数据，构建用户-物品共现矩阵（m*n） 计算共现矩阵两两向量间的相似性 获得用户历史行为数据的正反馈物品列表 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的topK物品 对于相似度进行排序，生成最终的推荐列表 2.2 矩阵分解算法–狗都不用通过分解协同过滤生成的共现矩阵 得到用户和物品的隐向量 2.2.1 矩阵分解的求解方法梯度下降 目标函数 2.3 特征交叉单一特征的表达性 没有特征组合起来的表达性好 且 单一特征会损失一定量的信息 2.3.1 POLY2模型暴力将特征n个 变成了$ n^2 $ 会将数据更加稀疏 增加训练复杂度 2.3.2 FM模型-隐向量特征交叉（嗯嗯嗯！）感谢： 个人的愚蠢回忆说实话 有些东西一定要好好折磨一下 才能有新的收获 ！ 比如这个 本人大白话讲一遍，啊啊啊 上面的式子太暴力，脑子思考一下就发现权重项太多，我在上面的这篇文献中发现这么一句：**对于任何正定实矩阵 只要k足够大，都存在k维向量组成的矩阵 使得 ** 在这个公式下 这个V啊 就是辅助向量。 这个可爱的V呢 满足 （我想到了2077） 隐向量 就是为每个特征 学习一个隐权重向量（latent vector） 交互使用两个向量取内积就好 优势 1. 权重参数减少到nk ​ 2. 训练复杂度降低到nk级别 2.3.3 FFM模型 特征域感知概念（可笑 咱把东西想简单了）数据还是上一次的数据 FFM模型中引入了类别的概念，即field 在上面的广告点击案例中，“Day&#x3D;26&#x2F;11&#x2F;15”、“Day&#x3D;1&#x2F;7&#x2F;14”、“Day&#x3D;19&#x2F;2&#x2F;15”这三个特征都是代表日期的，可以放到同一个field中。同理，Country也可以放到一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户国籍，广告类型，日期等等 在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量 v_i,fj。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day&#x3D;26&#x2F;11&#x2F;15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来 训练过程中，需要学习n个特征在f个域上的k维隐向量，参数 n * k * f 复杂度为 k$ n^2 $ 远多于FM模型的 nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 O(kn^2)。 数据的处理像Country，day 这样的categorical特征 可以通过 one-hot进行生成 如果是 像price这样的数值特征，则并不需要对其进行单独编码，但需要为其生成一个field。 个人的纯真理解（一眼顶真） 敲散说明白假如说 我这里有4个field 一共有10个feature。 动动小脑就晓得 假如说 field1 对应 feat1到feat4 则对应 feat1到feat4 要生成对应的 field2到field4的权重（这个就是场感知啊！！！） 脑瓜子随便一想 哇靠 真复杂 2.4 GBDT+LR 特征工程模型化 原始特征向量x，通过树分裂 将转化的特征类似于one-hot的向量来表示原始的特征，特征组合能力特别强 但是容易产生过拟合，以及这样的过程丢失了大量特征数值信息。 2.5 MLR 深度学习开始的曙光2.5.1 MLR与LR的区别 普通的LR模型 无法拟合我们所需的曲线 但是MLR模型正常拟合出来了 2.5.2 目标公式 如果m为1 则为普通的LR模型 当m越大 模型的拟合能力越强 而同样 需要的训练样本也变得更大 （阿里巴巴 经验12） 2.5.3 优点 端到端的非线性学习能力 模型稀疏性强 "},{"title":"centos安装","date":"2022-03-13T06:47:18.013Z","url":"/2022/03/13/1.%20centos%E5%AE%89%E8%A3%85/","categories":[["undefined",""]],"content":"个人主页  1. centos安装1.1 centos安装注意 1.2 ssh的连接 "}]